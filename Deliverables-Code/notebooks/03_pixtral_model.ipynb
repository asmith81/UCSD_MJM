{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41314937",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Pixtral Model Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the Pixtral-12B model's performance on invoice data extraction.\n",
    "It follows the project's notebook handling rules and functional programming approach.\n",
    "Results are saved as pure data collection artifacts with analysis performed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d828e7c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup and Configuration\n",
    "### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67291e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from PIL import Image\n",
    "from typing import Union, Dict, Any, List, Literal\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c60725",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Define a Function and Global Variable to Decide and Hold the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d12657",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Global variable to store selected prompt\n",
    "SELECTED_PROMPT = None\n",
    "\n",
    "def load_prompt_files() -> Dict[str, Dict]:\n",
    "    \"\"\"Load all prompt YAML files from the config/prompts directory.\"\"\"\n",
    "    prompts_dir = ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"prompts\"\n",
    "    prompt_files = {\n",
    "        \"basic_extraction\": prompts_dir / \"basic_extraction.yaml\",\n",
    "        \"detailed\": prompts_dir / \"detailed.yaml\",\n",
    "        \"few_shot\": prompts_dir / \"few_shot.yaml\",\n",
    "        \"locational\": prompts_dir / \"locational.yaml\",\n",
    "        \"step_by_step\": prompts_dir / \"step_by_step.yaml\"\n",
    "    }\n",
    "    \n",
    "    loaded_prompts = {}\n",
    "    for name, file_path in prompt_files.items():\n",
    "        with open(file_path, 'r') as f:\n",
    "            loaded_prompts[name] = yaml.safe_load(f)\n",
    "    return loaded_prompts\n",
    "\n",
    "def select_prompt() -> str:\n",
    "    \"\"\"Allow user to select a prompt type and return the prompt text.\"\"\"\n",
    "    global SELECTED_PROMPT\n",
    "    \n",
    "    prompts = load_prompt_files()\n",
    "    print(\"\\nAvailable prompt types:\")\n",
    "    for i, name in enumerate(prompts.keys(), 1):\n",
    "        print(f\"{i}. {name.replace('_', ' ').title()}\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nSelect a prompt type (1-5): \"))\n",
    "            if 1 <= choice <= len(prompts):\n",
    "                selected_name = list(prompts.keys())[choice - 1]\n",
    "                SELECTED_PROMPT = prompts[selected_name]\n",
    "                print(f\"\\nSelected prompt type: {selected_name.replace('_', ' ').title()}\")\n",
    "                print(\"\\nPrompt text:\")\n",
    "                print(\"-\" * 50)\n",
    "                print(SELECTED_PROMPT['prompts'][0]['text'])\n",
    "                print(\"-\" * 50)\n",
    "                return selected_name\n",
    "            else:\n",
    "                print(\"Invalid choice. Please select a number between 1 and 5.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dbaaf",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0be2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef7fcf",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Root Directory Determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine root directory by finding .gitignore and .gitattributes\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find project root by locating directory containing .gitignore and .gitattributes\"\"\"\n",
    "    try:\n",
    "        # When running as a script, start from script location\n",
    "        start_path = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # When running in a notebook, start from current working directory\n",
    "        start_path = Path.cwd()\n",
    "    \n",
    "    # Walk up the directory tree to find git markers\n",
    "    current_path = start_path\n",
    "    while current_path != current_path.parent:  # Stop at filesystem root\n",
    "        if (current_path / \".gitignore\").exists() and (current_path / \".gitattributes\").exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    \n",
    "    raise RuntimeError(\"Could not find project root (directory containing .gitignore and .gitattributes)\")\n",
    "\n",
    "ROOT_DIR = find_project_root()\n",
    "logger.info(f\"Found project root: {ROOT_DIR}\")\n",
    "\n",
    "# Verify expected files exist in the Deliverables-Code directory\n",
    "deliverables_dir = ROOT_DIR / \"Deliverables-Code\"\n",
    "if not deliverables_dir.exists():\n",
    "    raise RuntimeError(\"Could not find Deliverables-Code directory in project root\")\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Create results directory\n",
    "results_dir = ROOT_DIR / \"Deliverables-Code\" / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493c764",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install required dependencies with progress tracking.\"\"\"\n",
    "    # Check if PyTorch is already installed with correct version\n",
    "    try:\n",
    "        import torch\n",
    "        torch_version = torch.__version__\n",
    "        if torch_version.startswith(\"2.1.0\") and \"cu118\" in torch_version:\n",
    "            logger.info(f\"PyTorch {torch_version} already installed, skipping PyTorch installation\")\n",
    "            pytorch_step = None\n",
    "        else:\n",
    "            pytorch_step = (\"PyTorch\", [\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"torch==2.1.0\",\n",
    "                \"torchvision==0.16.0\",\n",
    "                \"torchaudio==2.1.0\",\n",
    "                \"--index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
    "            ])\n",
    "    except ImportError:\n",
    "        pytorch_step = (\"PyTorch\", [\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "            \"torch==2.1.0\",\n",
    "            \"torchvision==0.16.0\",\n",
    "            \"torchaudio==2.1.0\",\n",
    "            \"--index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
    "        ])\n",
    "    \n",
    "    steps = [\n",
    "        (\"Base requirements\", [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(ROOT_DIR / \"Deliverables-Code\" / \"requirements\" / \"requirements_pixtral.txt\")])\n",
    "    ]\n",
    "    \n",
    "    if pytorch_step:\n",
    "        steps.append(pytorch_step)\n",
    "    \n",
    "    for step_name, command in tqdm(steps, desc=\"Installing dependencies\"):\n",
    "        try:\n",
    "            subprocess.check_call(command)\n",
    "            logger.info(f\"Successfully installed {step_name}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.error(f\"Error installing {step_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "# Install dependencies\n",
    "install_dependencies()\n",
    "\n",
    "# Clear CUDA cache to prevent conflicts\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8b772",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Memory Resource Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a40125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory_resources():\n",
    "    \"\"\"\n",
    "    Check available GPU memory, system RAM, and compare with Pixtral model requirements.\n",
    "    Returns a dictionary with memory information and recommendations.\n",
    "    \"\"\"\n",
    "    memory_info = {\n",
    "        \"gpu_available\": False,\n",
    "        \"gpu_memory\": None,\n",
    "        \"system_ram\": None,\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    # Check GPU availability and memory\n",
    "    if torch.cuda.is_available():\n",
    "        memory_info[\"gpu_available\"] = True\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "        memory_info[\"gpu_memory\"] = round(gpu_mem, 2)\n",
    "        logger.info(f\"GPU Memory Available: {memory_info['gpu_memory']} GB\")\n",
    "    else:\n",
    "        logger.warning(\"No GPU available. This will significantly impact model performance.\")\n",
    "        memory_info[\"recommendations\"].append(\"No GPU detected. Consider using a GPU-enabled environment.\")\n",
    "    \n",
    "    # Check system RAM\n",
    "    import psutil\n",
    "    system_ram = psutil.virtual_memory().total / (1024**3)  # Convert to GB\n",
    "    memory_info[\"system_ram\"] = round(system_ram, 2)\n",
    "    logger.info(f\"System RAM Available: {memory_info['system_ram']} GB\")\n",
    "    \n",
    "    # Model requirements and recommendations\n",
    "    model_requirements = {\n",
    "        \"no_quantization\": 93.0,  # GB\n",
    "        \"8bit_quantization\": 46.0,  # GB\n",
    "        \"4bit_quantization\": 23.0   # GB\n",
    "    }\n",
    "    \n",
    "    # Add recommendations based on available resources\n",
    "    if memory_info[\"gpu_available\"]:\n",
    "        if memory_info[\"gpu_memory\"] >= model_requirements[\"no_quantization\"]:\n",
    "            memory_info[\"recommendations\"].append(\"Sufficient GPU memory for full precision model\")\n",
    "        elif memory_info[\"gpu_memory\"] >= model_requirements[\"8bit_quantization\"]:\n",
    "            memory_info[\"recommendations\"].append(\"Consider using 8-bit quantization\")\n",
    "        elif memory_info[\"gpu_memory\"] >= model_requirements[\"4bit_quantization\"]:\n",
    "            memory_info[\"recommendations\"].append(\"Consider using 4-bit quantization\")\n",
    "        else:\n",
    "            memory_info[\"recommendations\"].append(\"Insufficient GPU memory. Consider using CPU offloading or a different model.\")\n",
    "    \n",
    "    # Check if system RAM is sufficient for CPU offloading if needed\n",
    "    if memory_info[\"system_ram\"] < model_requirements[\"4bit_quantization\"]:\n",
    "        memory_info[\"recommendations\"].append(\"Warning: System RAM may be insufficient for CPU offloading\")\n",
    "    \n",
    "    return memory_info\n",
    "\n",
    "# Check memory resources\n",
    "memory_status = check_memory_resources()\n",
    "logger.info(\"Memory Status:\")\n",
    "for key, value in memory_status.items():\n",
    "    if key != \"recommendations\":\n",
    "        logger.info(f\"{key}: {value}\")\n",
    "logger.info(\"Recommendations:\")\n",
    "for rec in memory_status[\"recommendations\"]:\n",
    "    logger.info(f\"- {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad13d61",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Quantization Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb05144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_quantization() -> Literal[\"bfloat16\", \"int8\", \"int4\"]:\n",
    "    \"\"\"\n",
    "    Select quantization level for the model.\n",
    "    Returns one of: \"bfloat16\", \"int8\", \"int4\"\n",
    "    \"\"\"\n",
    "    print(\"\\nAvailable quantization options:\")\n",
    "    print(\"1. bfloat16 (full precision, 93GB VRAM)\")\n",
    "    print(\"2. int8 (8-bit, 46GB VRAM)\")\n",
    "    print(\"3. int4 (4-bit, 23GB VRAM)\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nSelect quantization (1-3): \"))\n",
    "            if choice == 1:\n",
    "                return \"bfloat16\"\n",
    "            elif choice == 2:\n",
    "                return \"int8\"\n",
    "            elif choice == 3:\n",
    "                return \"int4\"\n",
    "            else:\n",
    "                print(\"Invalid choice. Please select 1, 2, or 3.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a number between 1 and 3.\")\n",
    "\n",
    "# Select quantization\n",
    "quantization = select_quantization()\n",
    "logger.info(f\"Selected quantization: {quantization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652d231",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Flash Attention Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fce1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_flash_attention() -> bool:\n",
    "    \"\"\"\n",
    "    Check if Flash Attention is available and configure it.\n",
    "    Returns True if Flash Attention is enabled, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import flash_attn\n",
    "        \n",
    "        # Check if GPU supports Flash Attention\n",
    "        if not torch.cuda.is_available():\n",
    "            logger.warning(\"Flash Attention requires CUDA GPU. Disabling Flash Attention.\")\n",
    "            return False\n",
    "            \n",
    "        # Get GPU compute capability\n",
    "        major, minor = torch.cuda.get_device_capability()\n",
    "        compute_capability = float(f\"{major}.{minor}\")\n",
    "        \n",
    "        # Flash Attention 2 requires compute capability >= 8.0\n",
    "        if compute_capability >= 8.0:\n",
    "            logger.info(\"Flash Attention 2 enabled - GPU supports compute capability 8.0+\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"GPU compute capability {compute_capability} does not support Flash Attention 2\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        logger.warning(\"Flash Attention not installed. Please install with: pip install flash-attn\")\n",
    "        return False\n",
    "\n",
    "# Configure Flash Attention\n",
    "use_flash_attention = configure_flash_attention()\n",
    "logger.info(f\"Flash Attention Status: {'Enabled' if use_flash_attention else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa97e38",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Device Mapping Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_device_mapping() -> dict:\n",
    "    \"\"\"\n",
    "    Configure device mapping for GPU.\n",
    "    Returns device map configuration for model loading.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"No GPU available. This notebook requires a GPU to run.\")\n",
    "    \n",
    "    # Use single GPU setup\n",
    "    device_map = {\"\": 0}\n",
    "    logger.info(\"Using single GPU setup\")\n",
    "    return device_map\n",
    "\n",
    "# Configure device mapping\n",
    "device_map = configure_device_mapping()\n",
    "logger.info(f\"Device Map: {device_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbf206",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model Download and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pixtral_model(model_id: str = \"mistral-community/pixtral-12b\", \n",
    "                         max_retries: int = 2,\n",
    "                         retry_delay: int = 5) -> tuple:\n",
    "    \"\"\"\n",
    "    Download the Pixtral model with retry logic and memory monitoring.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        max_retries: Maximum number of download attempts\n",
    "        retry_delay: Delay between retries in seconds\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, processor) if successful\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If download fails after max retries\n",
    "    \"\"\"\n",
    "    from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "    import time\n",
    "    import psutil\n",
    "    \n",
    "    def log_memory_usage(stage: str):\n",
    "        \"\"\"Log current memory usage\"\"\"\n",
    "        gpu_mem = torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0\n",
    "        ram = psutil.virtual_memory().used / (1024**3)\n",
    "        logger.info(f\"Memory usage at {stage}: GPU={gpu_mem:.2f}GB, RAM={ram:.2f}GB\")\n",
    "    \n",
    "    # Log initial memory usage\n",
    "    log_memory_usage(\"start\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Download attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            # Configure model loading based on selected quantization\n",
    "            model_kwargs = {\n",
    "                \"device_map\": device_map,\n",
    "                \"trust_remote_code\": True\n",
    "            }\n",
    "            \n",
    "            if quantization == \"bfloat16\":\n",
    "                model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
    "            elif quantization == \"int8\":\n",
    "                # Simplified 8-bit config that works more reliably\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_threshold=6.0,\n",
    "                    llm_int8_enable_fp32_cpu_offload=False\n",
    "                )\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "            elif quantization == \"int4\":\n",
    "                # Simplified 4-bit config\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\n",
    "                    bnb_4bit_use_double_quant=False,\n",
    "                    bnb_4bit_quant_type=\"nf4\"\n",
    "                )\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "            \n",
    "            # Download model and processor\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(model_id, **model_kwargs)\n",
    "            processor = AutoProcessor.from_pretrained(model_id)\n",
    "            \n",
    "            # Log final memory usage\n",
    "            log_memory_usage(\"complete\")\n",
    "            \n",
    "            logger.info(\"Model and processor downloaded successfully\")\n",
    "            return model, processor\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Download attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Failed to download model after {max_retries} attempts: {str(e)}\")\n",
    "\n",
    "model, processor = download_pixtral_model()\n",
    "\n",
    "# Clear CUDA cache after model loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "logger.info(\"Model and processor ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36474be2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Prompt Selection\n",
    "Select a prompt type for the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prompt selection\n",
    "selected_prompt_type = select_prompt()\n",
    "logger.info(f\"Selected prompt type: {selected_prompt_type}\")\n",
    "\n",
    "# The selected prompt is now stored in the global variable SELECTED_PROMPT\n",
    "# This can be accessed in subsequent cells for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39fc40",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Configuration and Metadata Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72275dd4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_results_filename(model_name: str, quantization_level: str, results_dir: Path) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Generate a results filename with auto-incrementing counter.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model (e.g., \"pixtral\", \"llama\", \"doctr\")\n",
    "        quantization_level: Quantization level (e.g., \"bfloat16\", \"int8\", \"int4\", \"none\")\n",
    "        results_dir: Directory where results are stored\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (filename_without_extension, full_filepath)\n",
    "    \"\"\"\n",
    "    # Find existing files with the same model and quantization pattern\n",
    "    pattern = f\"results-{model_name}-{quantization_level}-*.json\"\n",
    "    existing_files = list(results_dir.glob(pattern))\n",
    "    \n",
    "    # Extract counter numbers from existing files\n",
    "    counter_numbers = []\n",
    "    for file in existing_files:\n",
    "        try:\n",
    "            # Extract number from filename like \"results-pixtral-bfloat16-3.json\"\n",
    "            parts = file.stem.split('-')\n",
    "            if len(parts) >= 4:\n",
    "                counter_numbers.append(int(parts[-1]))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Get next counter number\n",
    "    next_counter = max(counter_numbers, default=0) + 1\n",
    "    \n",
    "    # Generate filename\n",
    "    filename_base = f\"results-{model_name}-{quantization_level}-{next_counter}\"\n",
    "    full_filepath = results_dir / f\"{filename_base}.json\"\n",
    "    \n",
    "    return filename_base, str(full_filepath)\n",
    "\n",
    "def collect_test_metadata(test_id: str) -> dict:\n",
    "    \"\"\"Collect metadata about the current test configuration.\"\"\"\n",
    "    config = yaml.safe_load(open(ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"pixtral.yaml\", 'r'))\n",
    "    \n",
    "    # Get GPU information\n",
    "    gpu_props = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None\n",
    "    \n",
    "    return {\n",
    "        \"test_id\": test_id,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_info\": {\n",
    "            \"name\": \"Pixtral-12B\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"model_id\": \"mistral-community/pixtral-12b\",\n",
    "            \"model_type\": \"vision_language_model\",\n",
    "            \"quantization\": {\n",
    "                \"type\": quantization,\n",
    "                \"config\": {\n",
    "                    \"load_in_4bit\": quantization == \"int4\",\n",
    "                    \"load_in_8bit\": quantization == \"int8\",\n",
    "                    \"torch_dtype\": quantization if quantization == \"bfloat16\" else None,\n",
    "                    \"bnb_4bit_compute_dtype\": \"torch.float16\" if quantization == \"int4\" else None,\n",
    "                    \"bnb_4bit_quant_type\": \"nf4\" if quantization == \"int4\" else None\n",
    "                }\n",
    "            },\n",
    "            \"device_info\": {\n",
    "                \"device_map\": device_map,\n",
    "                \"use_flash_attention\": use_flash_attention,\n",
    "                \"gpu_memory_gb\": round(gpu_props.total_memory / (1024**3), 2) if gpu_props else None,\n",
    "                \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\" if gpu_props else None\n",
    "            }\n",
    "        },\n",
    "        \"prompt_info\": {\n",
    "            \"prompt_type\": selected_prompt_type,\n",
    "            \"raw_text\": SELECTED_PROMPT['prompts'][0]['text'],\n",
    "            \"formatted_text\": format_prompt(SELECTED_PROMPT['prompts'][0]['text']),\n",
    "            \"special_tokens\": config['model_params']['special_tokens']\n",
    "        },\n",
    "        \"processing_config\": {\n",
    "            \"inference_params\": config['inference'],\n",
    "            \"image_preprocessing\": {\n",
    "                \"max_size\": config['model_params']['max_image_size'],\n",
    "                \"format\": config['model_params']['image_format'],\n",
    "                \"resize_strategy\": \"maintain_aspect_ratio\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def format_prompt(prompt_text: str) -> str:\n",
    "    \"\"\"Format the prompt using the Pixtral template.\"\"\"\n",
    "    config = yaml.safe_load(open(ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"pixtral.yaml\", 'r'))\n",
    "    special_tokens = config['model_params']['special_tokens']\n",
    "    \n",
    "    # Format the prompt with special tokens and image token\n",
    "    formatted_prompt = f\"{special_tokens[2]}\\n{prompt_text}\\n{special_tokens[0]}\\n{special_tokens[1]}\\n{special_tokens[3]}\"\n",
    "    return formatted_prompt\n",
    "\n",
    "def load_and_process_image(image_path: str) -> Image.Image:\n",
    "    \"\"\"Load and process the image according to Pixtral specifications.\"\"\"\n",
    "    config = yaml.safe_load(open(ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"pixtral.yaml\", 'r'))\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert to RGB if needed\n",
    "    if image.mode != config['model_params']['image_format']:\n",
    "        image = image.convert(config['model_params']['image_format'])\n",
    "    \n",
    "    # Resize if needed\n",
    "    max_size = config['model_params']['max_image_size']\n",
    "    if image.size[0] > max_size[0] or image.size[1] > max_size[1]:\n",
    "        image.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de866ad",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Single Image Test\n",
    "Run the model on a single image using the selected prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f078ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_image_test():\n",
    "    \"\"\"Run the model on a single image with the selected prompt.\"\"\"\n",
    "    # Get the first .jpg file from data/images\n",
    "    image_dir = ROOT_DIR / \"Deliverables-Code\" / \"data\" / \"images\" / \"1_curated\"\n",
    "    image_files = list(image_dir.glob(\"*.jpg\"))\n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(\"No .jpg files found in data/images/1_curated directory\")\n",
    "    \n",
    "    image_path = str(image_files[0])\n",
    "    \n",
    "    # Load and process image\n",
    "    image = load_and_process_image(image_path)\n",
    "    \n",
    "    # Create a display version of the image with a max size of 800x800\n",
    "    display_image = image.copy()\n",
    "    max_display_size = (800, 800)\n",
    "    display_image.thumbnail(max_display_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Format the prompt\n",
    "    prompt_text = SELECTED_PROMPT['prompts'][0]['text']\n",
    "    formatted_prompt = format_prompt(prompt_text)\n",
    "    \n",
    "    # Display the image\n",
    "    print(\"\\nInput Image (resized for display):\")\n",
    "    display(display_image)\n",
    "    \n",
    "    # Display the prompt\n",
    "    print(\"\\nFormatted Prompt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(formatted_prompt)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Prepare model inputs using the original image\n",
    "    inputs = processor(\n",
    "        text=formatted_prompt,\n",
    "        images=[image],  # Pass image as a list\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the correct device - simplified dtype handling\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get inference parameters from config\n",
    "    config = yaml.safe_load(open(ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"pixtral.yaml\", 'r'))\n",
    "    inference_params = config['inference']\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=inference_params['max_new_tokens'],\n",
    "            do_sample=inference_params['do_sample'],\n",
    "            temperature=inference_params['temperature'],\n",
    "            top_k=inference_params['top_k'],\n",
    "            top_p=inference_params['top_p']\n",
    "        )\n",
    "    \n",
    "    # Decode and display response\n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\nModel Response:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run the single image test\n",
    "try:\n",
    "    test_response = run_single_image_test()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during single image test: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c2ffc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Batch Processing - Data Collection Only\n",
    "Run the model on all images and save raw results only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4cf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_incremental_results(results_file: Path, results: list, metadata: dict):\n",
    "    \"\"\"Save results incrementally to avoid losing progress.\"\"\"\n",
    "    complete_results = {\n",
    "        \"metadata\": metadata,\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(complete_results, f, indent=2)\n",
    "\n",
    "def process_all_images(results_file: Path, metadata: dict) -> list:\n",
    "    \"\"\"Process all images in the data/images directory and collect raw responses only.\"\"\"\n",
    "    results = []\n",
    "    image_dir = ROOT_DIR / \"Deliverables-Code\" / \"data\" / \"images\" / \"1_curated\"\n",
    "    image_files = list(image_dir.glob(\"*.jpg\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(\"No .jpg files found in data/images/1_curated directory\")\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        try:\n",
    "            # Load and process image\n",
    "            image = load_and_process_image(str(image_path))\n",
    "            \n",
    "            # Format the prompt\n",
    "            prompt_text = SELECTED_PROMPT['prompts'][0]['text']\n",
    "            formatted_prompt = format_prompt(prompt_text)\n",
    "            \n",
    "            # Prepare model inputs\n",
    "            inputs = processor(\n",
    "                text=formatted_prompt,\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move inputs to the correct device - simplified dtype handling\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get inference parameters from config\n",
    "            config = yaml.safe_load(open(ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"pixtral.yaml\", 'r'))\n",
    "            inference_params = config['inference']\n",
    "            \n",
    "            # Time the inference\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=inference_params['max_new_tokens'],\n",
    "                    do_sample=inference_params['do_sample'],\n",
    "                    temperature=inference_params['temperature'],\n",
    "                    top_k=inference_params['top_k'],\n",
    "                    top_p=inference_params['top_p']\n",
    "                )\n",
    "            \n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # Decode response\n",
    "            response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Create result entry with raw output only\n",
    "            result = {\n",
    "                \"image_name\": image_path.name,\n",
    "                \"status\": \"completed\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": round(processing_time, 2),\n",
    "                \"raw_output\": {\n",
    "                    \"model_response\": response,\n",
    "                    \"model_tokens_used\": len(outputs[0]),\n",
    "                    \"generation_parameters_used\": {\n",
    "                        \"max_new_tokens\": inference_params['max_new_tokens'],\n",
    "                        \"temperature\": inference_params['temperature'],\n",
    "                        \"top_k\": inference_params['top_k'],\n",
    "                        \"top_p\": inference_params['top_p']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add to results\n",
    "            results.append(result)\n",
    "            \n",
    "            # Save incremental results\n",
    "            save_incremental_results(results_file, results, metadata)\n",
    "            \n",
    "            logger.info(f\"Processed image: {image_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing image {image_path.name}: {str(e)}\")\n",
    "            result = {\n",
    "                \"image_name\": image_path.name,\n",
    "                \"status\": \"error\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": {\n",
    "                    \"type\": \"processing_error\",\n",
    "                    \"message\": str(e),\n",
    "                    \"stage\": \"inference\"\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "            # Save incremental results even on error\n",
    "            save_incremental_results(results_file, results, metadata)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_batch_test():\n",
    "    \"\"\"Run the model on all images and save raw results only.\"\"\"\n",
    "    try:\n",
    "        # Generate filename with new naming convention\n",
    "        test_id, results_file_path = generate_results_filename(\"pixtral\", quantization, results_dir)\n",
    "        results_file = Path(results_file_path)\n",
    "        \n",
    "        # Collect metadata with the test_id\n",
    "        metadata = collect_test_metadata(test_id)\n",
    "        \n",
    "        logger.info(f\"Starting Pixtral batch test with {quantization} quantization\")\n",
    "        logger.info(f\"Results will be saved to: {results_file}\")\n",
    "        \n",
    "        # Process all images with incremental saving\n",
    "        results = process_all_images(results_file, metadata)\n",
    "        \n",
    "        logger.info(f\"Batch test completed. Raw results saved to: {results_file}\")\n",
    "        return str(results_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during batch test: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run the batch test\n",
    "batch_results_file = run_batch_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2799795",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Analysis Functions - Data Processing Phase\n",
    "Functions for analyzing raw model outputs and generating structured analysis reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4896d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_json_from_response(response: str) -> tuple[dict, str]:\n",
    "    \"\"\"\n",
    "    Extract and parse JSON from raw model response.\n",
    "    Returns tuple of (parsed_json, error_message).\n",
    "    If successful, error_message will be empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the second occurrence of {\n",
    "        first_brace = response.find('{')\n",
    "        if first_brace == -1:\n",
    "            return None, \"No JSON object found in response\"\n",
    "            \n",
    "        second_brace = response.find('{', first_brace + 1)\n",
    "        if second_brace == -1:\n",
    "            return None, \"No second JSON object found in response\"\n",
    "        \n",
    "        # Find the matching closing brace\n",
    "        brace_count = 1\n",
    "        end_idx = second_brace + 1\n",
    "        while brace_count > 0 and end_idx < len(response):\n",
    "            if response[end_idx] == '{':\n",
    "                brace_count += 1\n",
    "            elif response[end_idx] == '}':\n",
    "                brace_count -= 1\n",
    "            end_idx += 1\n",
    "            \n",
    "        if brace_count != 0:\n",
    "            return None, \"Unmatched braces in JSON object\"\n",
    "            \n",
    "        # Extract and parse JSON\n",
    "        json_str = response[second_brace:end_idx].strip()\n",
    "        parsed_json = json.loads(json_str)\n",
    "        \n",
    "        # Validate structure\n",
    "        if not isinstance(parsed_json, dict):\n",
    "            return None, \"Response is not a JSON object\"\n",
    "        \n",
    "        # Validate required fields\n",
    "        required_fields = {\"work_order_number\", \"total_cost\"}\n",
    "        missing_fields = required_fields - set(parsed_json.keys())\n",
    "        if missing_fields:\n",
    "            return None, f\"Missing required fields: {missing_fields}\"\n",
    "        \n",
    "        return parsed_json, \"\"\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, f\"JSON parsing error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "def normalize_total_cost(cost_str: str) -> float:\n",
    "    \"\"\"Convert a cost string to a float by removing currency symbols and commas.\"\"\"\n",
    "    if not cost_str:\n",
    "        return None\n",
    "    # If already a float, return as is\n",
    "    if isinstance(cost_str, (int, float)):\n",
    "        return float(cost_str)\n",
    "    # Remove $ and commas, then convert to float\n",
    "    try:\n",
    "        return float(cost_str.replace('$', '').replace(',', '').strip())\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def categorize_work_order_error(predicted: str, ground_truth: str) -> str:\n",
    "    \"\"\"Categorize the type of error in work order number prediction.\"\"\"\n",
    "    if not predicted or not ground_truth:\n",
    "        return \"No Extraction\"\n",
    "    if predicted == ground_truth:\n",
    "        return \"Exact Match\"\n",
    "    # Check if prediction looks like a date (contains - or /)\n",
    "    if '-' in predicted or '/' in predicted:\n",
    "        return \"Date Confusion\"\n",
    "    # Check for partial match (some digits match)\n",
    "    if any(digit in ground_truth for digit in predicted):\n",
    "        return \"Partial Match\"\n",
    "    return \"Completely Wrong\"\n",
    "\n",
    "def categorize_total_cost_error(predicted: float, ground_truth: float) -> str:\n",
    "    \"\"\"Categorize the type of error in total cost prediction.\"\"\"\n",
    "    if predicted is None or ground_truth is None:\n",
    "        return \"No Extraction\"\n",
    "    if predicted == ground_truth:\n",
    "        return \"Numeric Match\"\n",
    "    \n",
    "    # Convert to strings for digit comparison\n",
    "    pred_str = str(int(predicted))\n",
    "    truth_str = str(int(ground_truth))\n",
    "    \n",
    "    # Check for digit reversal\n",
    "    if pred_str[::-1] == truth_str:\n",
    "        return \"Digit Reversal\"\n",
    "    \n",
    "    # Check for missing digit\n",
    "    if len(pred_str) == len(truth_str) - 1 and all(d in truth_str for d in pred_str):\n",
    "        return \"Missing Digit\"\n",
    "    \n",
    "    # Check for extra digit\n",
    "    if len(pred_str) == len(truth_str) + 1 and all(d in pred_str for d in truth_str):\n",
    "        return \"Extra Digit\"\n",
    "    \n",
    "    return \"Completely Wrong\"\n",
    "\n",
    "def calculate_cer(str1: str, str2: str) -> float:\n",
    "    \"\"\"Calculate Character Error Rate between two strings.\"\"\"\n",
    "    if not str1 or not str2:\n",
    "        return 1.0  # Return maximum error if either string is empty\n",
    "    \n",
    "    # Convert to strings and remove whitespace\n",
    "    str1 = str(str1).strip()\n",
    "    str2 = str(str2).strip()\n",
    "    \n",
    "    # Calculate Levenshtein distance\n",
    "    if len(str1) < len(str2):\n",
    "        str1, str2 = str2, str1\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    previous_row = range(len(str2) + 1)\n",
    "    for i, c1 in enumerate(str1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(str2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    # Return CER as distance divided by length of longer string\n",
    "    return previous_row[-1] / len(str1)\n",
    "\n",
    "def analyze_raw_results(results_file: str, ground_truth_file: str = None) -> dict:\n",
    "    \"\"\"Analyze raw model results and generate analysis report.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Set default ground truth file path\n",
    "    if ground_truth_file is None:\n",
    "        ground_truth_file = str(ROOT_DIR / \"Deliverables-Code\" / \"data\" / \"images\" / \"metadata\" / \"ground_truth.csv\")\n",
    "    \n",
    "    # Load results and ground truth\n",
    "    with open(results_file, 'r') as f:\n",
    "        raw_results = json.load(f)\n",
    "    \n",
    "    # Read ground truth with explicit string type for filename column\n",
    "    ground_truth = pd.read_csv(ground_truth_file, dtype={'filename': str})\n",
    "    \n",
    "    # Initialize analysis structure\n",
    "    analysis = {\n",
    "        \"source_results\": results_file,\n",
    "        \"extraction_method\": \"json_parsing_v2\",\n",
    "        \"ground_truth_file\": ground_truth_file,\n",
    "        \"metadata\": raw_results[\"metadata\"],\n",
    "        \"summary\": {\n",
    "            \"total_images\": len(raw_results[\"results\"]),\n",
    "            \"completed\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"json_extraction_successful\": 0,\n",
    "            \"work_order_accuracy\": 0,\n",
    "            \"total_cost_accuracy\": 0,\n",
    "            \"average_cer\": 0\n",
    "        },\n",
    "        \"error_categories\": {\n",
    "            \"work_order\": {},\n",
    "            \"total_cost\": {}\n",
    "        },\n",
    "        \"extracted_data\": [],\n",
    "        \"performance_metrics\": {}\n",
    "    }\n",
    "    \n",
    "    # Process each result\n",
    "    total_cer = 0\n",
    "    work_order_matches = 0\n",
    "    total_cost_matches = 0\n",
    "    json_successful = 0\n",
    "    \n",
    "    for result in raw_results[\"results\"]:\n",
    "        # Get ground truth for this image - use filename directly for matching\n",
    "        image_filename = result[\"image_name\"]\n",
    "        \n",
    "        gt_row = ground_truth[ground_truth[\"filename\"] == image_filename]\n",
    "        \n",
    "        if gt_row.empty:\n",
    "            logger.warning(f\"No ground truth found for image {image_filename}\")\n",
    "            continue\n",
    "            \n",
    "        gt_work_order = str(gt_row[\"work_order_number\"].iloc[0]).strip()\n",
    "        gt_total_cost = normalize_total_cost(str(gt_row[\"total\"].iloc[0]))\n",
    "        \n",
    "        # Initialize extraction entry\n",
    "        extraction_entry = {\n",
    "            \"image_name\": result[\"image_name\"],\n",
    "            \"status\": result[\"status\"],\n",
    "            \"raw_response\": result.get(\"raw_output\", {}).get(\"model_response\", \"\"),\n",
    "            \"ground_truth\": {\n",
    "                \"work_order_number\": gt_work_order,\n",
    "                \"total_cost\": gt_total_cost\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if result[\"status\"] == \"completed\":\n",
    "            analysis[\"summary\"][\"completed\"] += 1\n",
    "            \n",
    "            # Extract JSON data from raw response\n",
    "            raw_response = result[\"raw_output\"][\"model_response\"]\n",
    "            parsed_json, error_message = extract_json_from_response(raw_response)\n",
    "            \n",
    "            if parsed_json:\n",
    "                json_successful += 1\n",
    "                \n",
    "                # Analyze work order\n",
    "                pred_work_order = parsed_json.get(\"work_order_number\", \"\")\n",
    "                work_order_error = categorize_work_order_error(pred_work_order, gt_work_order)\n",
    "                work_order_cer = calculate_cer(pred_work_order, gt_work_order)\n",
    "                \n",
    "                if work_order_error == \"Exact Match\":\n",
    "                    work_order_matches += 1\n",
    "                \n",
    "                # Analyze total cost\n",
    "                pred_total_cost = normalize_total_cost(parsed_json.get(\"total_cost\", \"\"))\n",
    "                total_cost_error = categorize_total_cost_error(pred_total_cost, gt_total_cost)\n",
    "                \n",
    "                if total_cost_error == \"Numeric Match\":\n",
    "                    total_cost_matches += 1\n",
    "                \n",
    "                # Update extraction entry\n",
    "                extraction_entry.update({\n",
    "                    \"extracted_data\": {\n",
    "                        \"work_order_number\": pred_work_order,\n",
    "                        \"total_cost\": pred_total_cost\n",
    "                    },\n",
    "                    \"extraction_confidence\": {\n",
    "                        \"json_extraction_successful\": True,\n",
    "                        \"parsing_method\": \"json_extraction\",\n",
    "                        \"work_order_found\": bool(pred_work_order),\n",
    "                        \"total_cost_found\": bool(pred_total_cost),\n",
    "                        \"overall_confidence\": 1.0 - work_order_cer\n",
    "                    },\n",
    "                    \"performance\": {\n",
    "                        \"work_order_error_category\": work_order_error,\n",
    "                        \"total_cost_error_category\": total_cost_error,\n",
    "                        \"work_order_cer\": work_order_cer,\n",
    "                        \"work_order_correct\": work_order_error == \"Exact Match\",\n",
    "                        \"total_cost_correct\": total_cost_error == \"Numeric Match\"\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                # Update error categories\n",
    "                analysis[\"error_categories\"][\"work_order\"][work_order_error] = \\\n",
    "                    analysis[\"error_categories\"][\"work_order\"].get(work_order_error, 0) + 1\n",
    "                analysis[\"error_categories\"][\"total_cost\"][total_cost_error] = \\\n",
    "                    analysis[\"error_categories\"][\"total_cost\"].get(total_cost_error, 0) + 1\n",
    "                \n",
    "                total_cer += work_order_cer\n",
    "            else:\n",
    "                extraction_entry.update({\n",
    "                    \"extraction_error\": error_message,\n",
    "                    \"extraction_confidence\": {\n",
    "                        \"json_extraction_successful\": False,\n",
    "                        \"parsing_method\": \"json_extraction\",\n",
    "                        \"work_order_found\": False,\n",
    "                        \"total_cost_found\": False,\n",
    "                        \"overall_confidence\": 0.0\n",
    "                    }\n",
    "                })\n",
    "        else:\n",
    "            analysis[\"summary\"][\"errors\"] += 1\n",
    "            extraction_entry[\"processing_error\"] = result.get(\"error\", {})\n",
    "        \n",
    "        analysis[\"extracted_data\"].append(extraction_entry)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    completed = analysis[\"summary\"][\"completed\"]\n",
    "    if completed > 0:\n",
    "        analysis[\"summary\"][\"json_extraction_successful\"] = json_successful\n",
    "        analysis[\"summary\"][\"work_order_accuracy\"] = work_order_matches / completed\n",
    "        analysis[\"summary\"][\"total_cost_accuracy\"] = total_cost_matches / completed\n",
    "        analysis[\"summary\"][\"average_cer\"] = total_cer / completed\n",
    "        \n",
    "        # Performance metrics\n",
    "        analysis[\"performance_metrics\"] = {\n",
    "            \"json_extraction_rate\": json_successful / completed,\n",
    "            \"work_order_extraction_rate\": work_order_matches / completed,\n",
    "            \"total_cost_extraction_rate\": total_cost_matches / completed,\n",
    "            \"average_processing_time\": sum(\n",
    "                r.get(\"processing_time_seconds\", 0) \n",
    "                for r in raw_results[\"results\"] \n",
    "                if r[\"status\"] == \"completed\"\n",
    "            ) / completed\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def select_test_results_file() -> Path:\n",
    "    \"\"\"Allow user to select a test results file for analysis.\"\"\"\n",
    "    # Get all test result files\n",
    "    results_dir_path = ROOT_DIR / \"Deliverables-Code\" / \"results\"\n",
    "    result_files = list(results_dir_path.glob(\"results-*.json\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        raise FileNotFoundError(\"No test result files found in results directory\")\n",
    "    \n",
    "    # Sort files by modification time (newest first)\n",
    "    result_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(\"\\nAvailable test result files:\")\n",
    "    for i, file in enumerate(result_files, 1):\n",
    "        # Get file modification time\n",
    "        mod_time = datetime.fromtimestamp(file.stat().st_mtime)\n",
    "        print(f\"{i}. {file.name} (Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nSelect a test result file (1-{}): \".format(len(result_files))))\n",
    "            if 1 <= choice <= len(result_files):\n",
    "                selected_file = result_files[choice - 1]\n",
    "                print(f\"\\nSelected file: {selected_file.name}\")\n",
    "                return selected_file\n",
    "            else:\n",
    "                print(f\"Invalid choice. Please select a number between 1 and {len(result_files)}.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "def run_analysis():\n",
    "    \"\"\"Run analysis on raw results and generate comprehensive performance report.\"\"\"\n",
    "    try:\n",
    "        # Get test results file\n",
    "        results_file = select_test_results_file()\n",
    "        \n",
    "        # Generate analysis\n",
    "        analysis = analyze_raw_results(str(results_file))\n",
    "        \n",
    "        # Create analysis directory if it doesn't exist\n",
    "        analysis_dir = ROOT_DIR / \"Deliverables-Code\" / \"analysis\"\n",
    "        analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Generate analysis filename with same convention as results\n",
    "        model_name = \"pixtral\"\n",
    "        quantization_level = analysis['metadata']['model_info']['quantization']['type']\n",
    "        \n",
    "        # Find existing analysis files with the same model and quantization pattern\n",
    "        pattern = f\"analysis-{model_name}-{quantization_level}-*.json\"\n",
    "        existing_files = list(analysis_dir.glob(pattern))\n",
    "        \n",
    "        # Extract counter numbers from existing files\n",
    "        counter_numbers = []\n",
    "        for file in existing_files:\n",
    "            try:\n",
    "                # Extract number from filename like \"analysis-pixtral-bfloat16-3.json\"\n",
    "                parts = file.stem.split('-')\n",
    "                if len(parts) >= 4:\n",
    "                    counter_numbers.append(int(parts[-1]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Get next counter number\n",
    "        next_counter = max(counter_numbers, default=0) + 1\n",
    "        \n",
    "        # Generate analysis filename\n",
    "        analysis_filename = f\"analysis-{model_name}-{quantization_level}-{next_counter}.json\"\n",
    "        analysis_file = analysis_dir / analysis_filename\n",
    "        \n",
    "        with open(analysis_file, 'w') as f:\n",
    "            json.dump(analysis, f, indent=2)\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\nPixtral Model Analysis Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total Images: {analysis['summary']['total_images']}\")\n",
    "        print(f\"Completed: {analysis['summary']['completed']}\")\n",
    "        print(f\"Errors: {analysis['summary']['errors']}\")\n",
    "        print(f\"JSON Extraction Successful: {analysis['summary']['json_extraction_successful']}\")\n",
    "        print(f\"Work Order Accuracy: {analysis['summary']['work_order_accuracy']:.2%}\")\n",
    "        print(f\"Total Cost Accuracy: {analysis['summary']['total_cost_accuracy']:.2%}\")\n",
    "        print(f\"Average CER: {analysis['summary']['average_cer']:.3f}\")\n",
    "        \n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        for metric, value in analysis['performance_metrics'].items():\n",
    "            if 'rate' in metric:\n",
    "                print(f\"- {metric.replace('_', ' ').title()}: {value:.2%}\")\n",
    "            else:\n",
    "                print(f\"- {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "        \n",
    "        print(\"\\nWork Order Error Categories:\")\n",
    "        for category, count in analysis['error_categories']['work_order'].items():\n",
    "            print(f\"- {category}: {count}\")\n",
    "        \n",
    "        print(\"\\nTotal Cost Error Categories:\")\n",
    "        for category, count in analysis['error_categories']['total_cost'].items():\n",
    "            print(f\"- {category}: {count}\")\n",
    "        \n",
    "        print(f\"\\nDetailed analysis saved to: {analysis_file}\")\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during analysis: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c958fec",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Run Analysis\n",
    "Generate and display analysis of raw model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7c24c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "analysis_results = run_analysis()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

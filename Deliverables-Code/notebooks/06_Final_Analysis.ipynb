{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Final Analysis Framework v2.0 - Focused Results Analysis\n",
        "\n",
        "This analysis framework focuses on understanding the experimental results from the construction invoice processing study, incorporating controlled experimental design considerations and practical system improvement insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Requirements Installation and Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üöÄ Analysis Requirements Installation & Verification\n",
            "======================================================================\n",
            "üîç Checking current package status...\n",
            "--------------------------------------------------\n",
            "‚úÖ pandas          - Already installed\n",
            "‚úÖ numpy           - Already installed\n",
            "‚úÖ matplotlib      - Already installed\n",
            "‚úÖ seaborn         - Already installed\n",
            "‚úÖ scipy           - Already installed\n",
            "‚úÖ scikit-learn    - Already installed\n",
            "‚úÖ statsmodels     - Already installed\n",
            "‚úÖ tqdm            - Already installed\n",
            "‚úÖ pyyaml          - Already installed\n",
            "\n",
            "üìä Status Summary:\n",
            "   Already installed: 9\n",
            "   Need installation: 0\n",
            "\n",
            "‚è±Ô∏è  Starting installation process at 13:37:10\n",
            "‚úì Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "\n",
            "üì¶ Installing 13 packages from requirements file...\n",
            "============================================================\n",
            "\n",
            "[1/13] Installing pandas...\n",
            "   Full requirement: pandas>=1.5.0\n",
            "   ‚úÖ pandas installed successfully (1.5s)\n",
            "\n",
            "[2/13] Installing numpy...\n",
            "   Full requirement: numpy>=1.21.0\n",
            "   ‚úÖ numpy installed successfully (1.6s)\n",
            "\n",
            "[3/13] Installing matplotlib...\n",
            "   Full requirement: matplotlib>=3.5.0\n",
            "   ‚úÖ matplotlib installed successfully (1.7s)\n",
            "\n",
            "[4/13] Installing seaborn...\n",
            "   Full requirement: seaborn>=0.11.0\n",
            "   ‚úÖ seaborn installed successfully (1.6s)\n",
            "\n",
            "[5/13] Installing scipy...\n",
            "   Full requirement: scipy>=1.8.0\n",
            "   ‚úÖ scipy installed successfully (1.7s)\n",
            "\n",
            "[6/13] Installing scikit-learn...\n",
            "   Full requirement: scikit-learn>=1.1.0\n",
            "   ‚úÖ scikit-learn installed successfully (1.6s)\n",
            "\n",
            "[7/13] Installing statsmodels...\n",
            "   Full requirement: statsmodels>=0.13.0\n",
            "   ‚úÖ statsmodels installed successfully (1.5s)\n",
            "\n",
            "[8/13] Installing pathlib2...\n",
            "   Full requirement: pathlib2>=2.3.0\n",
            "   ‚úÖ pathlib2 installed successfully (1.6s)\n",
            "\n",
            "[9/13] Installing tqdm...\n",
            "   Full requirement: tqdm>=4.64.0\n",
            "   ‚úÖ tqdm installed successfully (1.4s)\n",
            "\n",
            "[10/13] Installing pyyaml...\n",
            "   Full requirement: pyyaml>=6.0\n",
            "   ‚úÖ pyyaml installed successfully (1.6s)\n",
            "\n",
            "[11/13] Installing ipywidgets...\n",
            "   Full requirement: ipywidgets>=7.6.0\n",
            "   ‚úÖ ipywidgets installed successfully (1.5s)\n",
            "\n",
            "[12/13] Installing jupyter...\n",
            "   Full requirement: jupyter>=1.0.0\n",
            "   ‚úÖ jupyter installed successfully (1.8s)\n",
            "\n",
            "[13/13] Installing psutil...\n",
            "   Full requirement: psutil>=5.8.0\n",
            "   ‚úÖ psutil installed successfully (1.5s)\n",
            "\n",
            "============================================================\n",
            "üìä Installation Summary:\n",
            "   ‚úÖ Successful: 13\n",
            "   ‚ùå Failed: 0\n",
            "\n",
            "   Successfully installed: pandas, numpy, matplotlib, seaborn, scipy\n",
            "   ... and 8 more\n",
            "\n",
            "‚è±Ô∏è  Total installation time: 20.6 seconds\n",
            "\n",
            "üîç Verifying library imports...\n",
            "----------------------------------------\n",
            "‚úÖ pandas               - OK\n",
            "‚úÖ numpy                - OK\n",
            "‚úÖ matplotlib.pyplot    - OK\n",
            "‚úÖ seaborn              - OK\n",
            "‚úÖ scipy                - OK\n",
            "‚úÖ pathlib              - OK\n",
            "‚úÖ json                 - OK\n",
            "‚úÖ yaml                 - OK\n",
            "‚úÖ sklearn              - OK\n",
            "‚úÖ statsmodels.api      - OK\n",
            "\n",
            "‚úÖ All required libraries verified successfully\n",
            "\n",
            "======================================================================\n",
            "üéâ Setup complete! Ready to proceed with analysis.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "def find_requirements_file():\n",
        "    \"\"\"Find the requirements_analysis.txt file in expected locations.\"\"\"\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            return path\n",
        "    \n",
        "    return None\n",
        "\n",
        "def read_requirements_file(requirements_file):\n",
        "    \"\"\"Read and parse the requirements file.\"\"\"\n",
        "    try:\n",
        "        with open(requirements_file, 'r') as f:\n",
        "            requirements_content = f.read().strip().split('\\n')\n",
        "        \n",
        "        # Filter out comments and empty lines\n",
        "        requirements_list = [\n",
        "            req.strip() for req in requirements_content \n",
        "            if req.strip() and not req.strip().startswith('#')\n",
        "        ]\n",
        "        \n",
        "        return requirements_list\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading requirements file: {e}\")\n",
        "        return []\n",
        "\n",
        "def install_package(requirement, index, total):\n",
        "    \"\"\"Install a single package with progress reporting.\"\"\"\n",
        "    package_name = requirement.split('>=')[0].split('==')[0].split('[')[0]\n",
        "    print(f\"\\n[{index}/{total}] Installing {package_name}...\")\n",
        "    print(f\"   Full requirement: {requirement}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'install', requirement, '--timeout', '120'\n",
        "        ], capture_output=True, text=True, timeout=180)  # 3 minute timeout per package\n",
        "        \n",
        "        elapsed_time = time.time() - start_time\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(f\"   ‚úÖ {package_name} installed successfully ({elapsed_time:.1f}s)\")\n",
        "            return True, package_name\n",
        "        else:\n",
        "            print(f\"   ‚ùå Failed to install {package_name}\")\n",
        "            if result.stderr:\n",
        "                print(f\"   Error: {result.stderr[:200]}...\")\n",
        "            return False, package_name\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"   ‚è∞ Timeout installing {package_name} (>3 minutes)\")\n",
        "        return False, package_name\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Exception installing {package_name}: {e}\")\n",
        "        return False, package_name\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install all requirements from the requirements file.\"\"\"\n",
        "    # Find requirements file\n",
        "    requirements_file = find_requirements_file()\n",
        "    if not requirements_file:\n",
        "        print(\"‚ùå requirements_analysis.txt not found in any expected location\")\n",
        "        print(\"Expected locations:\")\n",
        "        for path in [Path('./requirements/requirements_analysis.txt'),\n",
        "                    Path('../requirements/requirements_analysis.txt'),\n",
        "                    Path('../../requirements/requirements_analysis.txt'),\n",
        "                    Path('./Deliverables-Code/requirements/requirements_analysis.txt')]:\n",
        "            print(f\"   - {path}\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úì Found requirements file: {requirements_file}\")\n",
        "    \n",
        "    # Read requirements\n",
        "    requirements_list = read_requirements_file(requirements_file)\n",
        "    if not requirements_list:\n",
        "        print(\"‚ùå No valid requirements found in file\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"\\nüì¶ Installing {len(requirements_list)} packages from requirements file...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Install each requirement\n",
        "    failed_packages = []\n",
        "    successful_packages = []\n",
        "    \n",
        "    for i, requirement in enumerate(requirements_list, 1):\n",
        "        success, package_name = install_package(requirement, i, len(requirements_list))\n",
        "        if success:\n",
        "            successful_packages.append(package_name)\n",
        "        else:\n",
        "            failed_packages.append(package_name)\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üìä Installation Summary:\")\n",
        "    print(f\"   ‚úÖ Successful: {len(successful_packages)}\")\n",
        "    print(f\"   ‚ùå Failed: {len(failed_packages)}\")\n",
        "    \n",
        "    if successful_packages:\n",
        "        print(f\"\\n   Successfully installed: {', '.join(successful_packages[:5])}\")\n",
        "        if len(successful_packages) > 5:\n",
        "            print(f\"   ... and {len(successful_packages) - 5} more\")\n",
        "    \n",
        "    if failed_packages:\n",
        "        print(f\"\\n   ‚ö†Ô∏è  Failed packages: {', '.join(failed_packages)}\")\n",
        "        print(\"   You may need to install these manually or check for dependency conflicts.\")\n",
        "    \n",
        "    return len(failed_packages) == 0\n",
        "\n",
        "def check_package_installed(package_name):\n",
        "    \"\"\"Check if a package is already installed.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'show', package_name\n",
        "        ], capture_output=True, text=True)\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def show_pre_installation_status():\n",
        "    \"\"\"Show which key packages are already installed.\"\"\"\n",
        "    check_packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scipy', \n",
        "                     'scikit-learn', 'statsmodels', 'tqdm', 'pyyaml']\n",
        "    \n",
        "    print(\"üîç Checking current package status...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    already_installed = []\n",
        "    need_installation = []\n",
        "    \n",
        "    for package in check_packages:\n",
        "        if check_package_installed(package):\n",
        "            print(f\"‚úÖ {package:<15} - Already installed\")\n",
        "            already_installed.append(package)\n",
        "        else:\n",
        "            print(f\"‚ùå {package:<15} - Needs installation\")\n",
        "            need_installation.append(package)\n",
        "    \n",
        "    print(f\"\\nüìä Status Summary:\")\n",
        "    print(f\"   Already installed: {len(already_installed)}\")\n",
        "    print(f\"   Need installation: {len(need_installation)}\")\n",
        "    \n",
        "    if need_installation:\n",
        "        print(f\"   Packages to install: {', '.join(need_installation)}\")\n",
        "    \n",
        "    return already_installed, need_installation\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml',\n",
        "        'sklearn': 'sklearn',\n",
        "        'statsmodels.api': 'sm'\n",
        "    }\n",
        "    \n",
        "    print(\"\\nüîç Verifying library imports...\")\n",
        "    print(\"-\" * 40)\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"‚úÖ {lib:<20} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"‚ùå {lib:<20} - FAILED: {str(e)[:50]}...\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"   Failed libraries:\", ', '.join(failed_imports))\n",
        "        print(\"   You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "# Run installation and verification with progress tracking\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Analysis Requirements Installation & Verification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show pre-installation status\n",
        "already_installed, need_installation = show_pre_installation_status()\n",
        "\n",
        "# Proceed with installation\n",
        "print(f\"\\n‚è±Ô∏è  Starting installation process at {time.strftime('%H:%M:%S')}\")\n",
        "start_total = time.time()\n",
        "\n",
        "installation_success = install_requirements()\n",
        "\n",
        "total_time = time.time() - start_total\n",
        "print(f\"\\n‚è±Ô∏è  Total installation time: {total_time:.1f} seconds\")\n",
        "\n",
        "# Verify installation\n",
        "verification_success = verify_imports()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if installation_success and verification_success:\n",
        "    print(\"üéâ Setup complete! Ready to proceed with analysis.\")\n",
        "elif installation_success:\n",
        "    print(\"‚ö†Ô∏è  Installation complete but some imports failed. You may need to restart the kernel.\")\n",
        "else:\n",
        "    print(\"‚ùå Installation had issues. Please check the error messages above.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Root Directory Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Root Directory Detection & Path Setup ===\n",
            "‚úì Found project root: d:\\UCSD_MJM\n",
            "‚úì Found Deliverables-Code directory: d:\\UCSD_MJM\\Deliverables-Code\n",
            "‚úì Found data directory: d:\\UCSD_MJM\\Deliverables-Code\\data\n",
            "‚úì Found results directory: d:\\UCSD_MJM\\Deliverables-Code\\results\n",
            "‚úì Found analysis directory: d:\\UCSD_MJM\\Deliverables-Code\\analysis\n",
            "‚úì Found config directory: d:\\UCSD_MJM\\Deliverables-Code\\config\n",
            "\n",
            "‚úì All project directories located successfully\n",
            "‚úì Added project root to Python path\n",
            "\n",
            "=== Project Structure (Key Directories) ===\n",
            "ROOT_DIR:         d:\\UCSD_MJM\n",
            "DELIVERABLES_DIR: d:\\UCSD_MJM\\Deliverables-Code\n",
            "DATA_DIR:         d:\\UCSD_MJM\\Deliverables-Code\\data\n",
            "RESULTS_DIR:      d:\\UCSD_MJM\\Deliverables-Code\\results\n",
            "ANALYSIS_DIR:     d:\\UCSD_MJM\\Deliverables-Code\\analysis\n",
            "CONFIG_DIR:       d:\\UCSD_MJM\\Deliverables-Code\\config\n",
            "\n",
            "Result files found: 15\n",
            "Analysis files found: 15\n",
            "Metadata files found: 2\n",
            "\n",
            "üéØ Ready to proceed with analysis from: UCSD_MJM\n"
          ]
        }
      ],
      "source": [
        "def find_project_root():\n",
        "    \"\"\"\n",
        "    Find project root by locating directory containing .gitignore and .gitattributes.\n",
        "    Similar to implementation in 03_pixtral_model.py\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import sys\n",
        "    \n",
        "    try:\n",
        "        # When running as a script, start from script location\n",
        "        start_path = Path(__file__).parent\n",
        "    except NameError:\n",
        "        # When running in a notebook, start from current working directory\n",
        "        start_path = Path.cwd()\n",
        "    \n",
        "    # Walk up the directory tree to find git markers\n",
        "    current_path = start_path\n",
        "    while current_path != current_path.parent:  # Stop at filesystem root\n",
        "        if (current_path / \".gitignore\").exists() and (current_path / \".gitattributes\").exists():\n",
        "            return current_path\n",
        "        current_path = current_path.parent\n",
        "    \n",
        "    raise RuntimeError(\"Could not find project root (directory containing .gitignore and .gitattributes)\")\n",
        "\n",
        "def setup_project_paths():\n",
        "    \"\"\"Set up all project directory paths and verify they exist.\"\"\"\n",
        "    global ROOT_DIR, DELIVERABLES_DIR, DATA_DIR, RESULTS_DIR, ANALYSIS_DIR, CONFIG_DIR\n",
        "    \n",
        "    # Find and set root directory\n",
        "    ROOT_DIR = find_project_root()\n",
        "    print(f\"‚úì Found project root: {ROOT_DIR}\")\n",
        "    \n",
        "    # Set up key directories\n",
        "    DELIVERABLES_DIR = ROOT_DIR / \"Deliverables-Code\"\n",
        "    DATA_DIR = DELIVERABLES_DIR / \"data\"\n",
        "    RESULTS_DIR = DELIVERABLES_DIR / \"results\"\n",
        "    ANALYSIS_DIR = DELIVERABLES_DIR / \"analysis\"\n",
        "    CONFIG_DIR = DELIVERABLES_DIR / \"config\"\n",
        "    \n",
        "    # Verify expected directories exist\n",
        "    required_dirs = {\n",
        "        \"Deliverables-Code\": DELIVERABLES_DIR,\n",
        "        \"data\": DATA_DIR,\n",
        "        \"results\": RESULTS_DIR,\n",
        "        \"analysis\": ANALYSIS_DIR,\n",
        "        \"config\": CONFIG_DIR\n",
        "    }\n",
        "    \n",
        "    missing_dirs = []\n",
        "    for name, path in required_dirs.items():\n",
        "        if path.exists():\n",
        "            print(f\"‚úì Found {name} directory: {path}\")\n",
        "        else:\n",
        "            print(f\"‚ö† Missing {name} directory: {path}\")\n",
        "            missing_dirs.append(name)\n",
        "    \n",
        "    if missing_dirs:\n",
        "        print(f\"\\n‚ö† Warning: {len(missing_dirs)} required directories not found\")\n",
        "        print(\"This may indicate the notebook is being run from an unexpected location\")\n",
        "    else:\n",
        "        print(\"\\n‚úì All project directories located successfully\")\n",
        "    \n",
        "    # Create analysis directory if it doesn't exist\n",
        "    ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Add project root to Python path for imports\n",
        "    import sys\n",
        "    if str(ROOT_DIR) not in sys.path:\n",
        "        sys.path.append(str(ROOT_DIR))\n",
        "        print(f\"‚úì Added project root to Python path\")\n",
        "    \n",
        "    return ROOT_DIR\n",
        "\n",
        "def display_project_structure():\n",
        "    \"\"\"Display relevant project structure for reference.\"\"\"\n",
        "    print(\"\\n=== Project Structure (Key Directories) ===\")\n",
        "    print(f\"ROOT_DIR:         {ROOT_DIR}\")\n",
        "    print(f\"DELIVERABLES_DIR: {DELIVERABLES_DIR}\")\n",
        "    print(f\"DATA_DIR:         {DATA_DIR}\")\n",
        "    print(f\"RESULTS_DIR:      {RESULTS_DIR}\")\n",
        "    print(f\"ANALYSIS_DIR:     {ANALYSIS_DIR}\")\n",
        "    print(f\"CONFIG_DIR:       {CONFIG_DIR}\")\n",
        "    \n",
        "    # Show counts of files in key directories\n",
        "    if RESULTS_DIR.exists():\n",
        "        result_files = list(RESULTS_DIR.glob(\"*.json\"))\n",
        "        print(f\"\\nResult files found: {len(result_files)}\")\n",
        "        \n",
        "    if ANALYSIS_DIR.exists():\n",
        "        analysis_files = list(ANALYSIS_DIR.glob(\"*.json\"))\n",
        "        print(f\"Analysis files found: {len(analysis_files)}\")\n",
        "        \n",
        "    if (DATA_DIR / \"images\" / \"metadata\").exists():\n",
        "        metadata_files = list((DATA_DIR / \"images\" / \"metadata\").glob(\"*.csv\"))\n",
        "        print(f\"Metadata files found: {len(metadata_files)}\")\n",
        "\n",
        "# Run root directory detection and path setup\n",
        "print(\"=== Root Directory Detection & Path Setup ===\")\n",
        "project_root = setup_project_paths()\n",
        "display_project_structure()\n",
        "\n",
        "print(f\"\\nüéØ Ready to proceed with analysis from: {ROOT_DIR.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì All libraries imported successfully\n",
            "‚úì Plotting parameters configured\n",
            "‚úì Custom color palette defined\n",
            "‚úì Analysis environment ready\n",
            "\n",
            "üìä Available analysis colors: ['LMM', 'OCR', 'Pixtral', 'Llama', 'DocTR', 'accuracy', 'cer', 'work_order', 'total_cost', 'baseline', 'improvement']\n",
            "üé® Visualization settings optimized for analysis reports\n"
          ]
        }
      ],
      "source": [
        "# Import standard libraries for data analysis and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind, mannwhitneyu, kruskal\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import itertools\n",
        "\n",
        "# Statistical and machine learning utilities\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure plotting parameters and styles\n",
        "plt.style.use('default')  # Start with clean default style\n",
        "\n",
        "# Set up matplotlib and seaborn styling\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': (12, 8),\n",
        "    'figure.dpi': 100,\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'legend.fontsize': 10,\n",
        "    'legend.title_fontsize': 11,\n",
        "    'axes.grid': True,\n",
        "    'grid.alpha': 0.3,\n",
        "    'lines.linewidth': 2,\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'figure.facecolor': 'white',\n",
        "    'axes.facecolor': 'white'\n",
        "})\n",
        "\n",
        "# Set seaborn style and palette\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Custom color palette for consistent visualization\n",
        "ANALYSIS_COLORS = {\n",
        "    'LMM': '#2E86AB',        # Blue for LMM models\n",
        "    'OCR': '#A23B72',        # Purple for OCR models\n",
        "    'Pixtral': '#2E86AB',    # Blue for Pixtral\n",
        "    'Llama': '#00A6D6',      # Light blue for Llama\n",
        "    'DocTR': '#A23B72',      # Purple for DocTR\n",
        "    'accuracy': '#28A745',    # Green for accuracy metrics\n",
        "    'cer': '#DC3545',        # Red for error metrics\n",
        "    'work_order': '#FD7E14',  # Orange for work order\n",
        "    'total_cost': '#6F42C1',  # Purple for total cost\n",
        "    'baseline': '#6C757D',    # Gray for baseline/reference\n",
        "    'improvement': '#20C997'   # Teal for improvements\n",
        "}\n",
        "\n",
        "# Configure warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Display configuration\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "print(\"‚úì All libraries imported successfully\")\n",
        "print(\"‚úì Plotting parameters configured\")\n",
        "print(\"‚úì Custom color palette defined\")\n",
        "print(\"‚úì Analysis environment ready\")\n",
        "\n",
        "# Show available color palette\n",
        "print(f\"\\nüìä Available analysis colors: {list(ANALYSIS_COLORS.keys())}\")\n",
        "print(\"üé® Visualization settings optimized for analysis reports\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è  Initializing data loading functions\n",
            "INFO: Discovering results files\n",
            "INFO: Found 15 total results files\n",
            "INFO:   pixtral: 4 files\n",
            "INFO:   llama: 4 files\n",
            "INFO:   doctr: 7 files\n",
            "INFO: Discovering analysis files\n",
            "INFO: Found 15 total analysis files\n",
            "INFO:   pixtral: 4 files\n",
            "INFO:   llama: 4 files\n",
            "INFO:   doctr: 7 files\n",
            "INFO: Loaded ground truth data: 549 records\n",
            "‚úÖ Ground truth loaded: 549 records\n",
            "INFO: Creating comprehensive dataset\n",
            "INFO: Loaded ground truth data: 549 records\n",
            "INFO: Loading all analysis files\n",
            "INFO: Discovering analysis files\n",
            "INFO: Found 15 total analysis files\n",
            "INFO:   pixtral: 4 files\n",
            "INFO:   llama: 4 files\n",
            "INFO:   doctr: 7 files\n",
            "INFO: Loaded 15 analysis files across 3 model types\n",
            "INFO: Added 4 experiments for pixtral\n",
            "INFO: Added 4 experiments for llama\n",
            "INFO: Added 7 experiments for doctr\n",
            "INFO: Comprehensive dataset created with 3 models and 15 experiments\n",
            "‚úÖ Comprehensive dataset created successfully\n",
            "\n",
            "üìä Data Loading Summary:\n",
            "   ‚Ä¢ Ground truth records: 549\n",
            "   ‚Ä¢ Results files found: 15\n",
            "   ‚Ä¢ Analysis files found: 15\n",
            "\n",
            "   Results by model type:\n",
            "     - Pixtral: 4 files\n",
            "     - Llama: 4 files\n",
            "     - Doctr: 7 files\n",
            "\n",
            "   Analysis by model type:\n",
            "     - Pixtral: 4 files\n",
            "     - Llama: 4 files\n",
            "     - Doctr: 7 files\n",
            "\n",
            "‚úÖ Data loading functions ready for analysis\n"
          ]
        }
      ],
      "source": [
        "def load_ground_truth_data(ground_truth_file: str = None) -> pd.DataFrame:\n",
        "    \"\"\"Load and validate ground truth CSV data.\"\"\"\n",
        "    # Set default ground truth file path using ROOT_DIR\n",
        "    if ground_truth_file is None:\n",
        "        ground_truth_file = DATA_DIR / \"images\" / \"metadata\" / \"ground_truth.csv\"\n",
        "    else:\n",
        "        ground_truth_file = Path(ground_truth_file)\n",
        "    \n",
        "    if not ground_truth_file.exists():\n",
        "        raise FileNotFoundError(f\"Ground truth file not found: {ground_truth_file}\")\n",
        "    \n",
        "    try:\n",
        "        # Load with explicit string type for filename column to ensure consistent matching\n",
        "        ground_truth = pd.read_csv(ground_truth_file, dtype={'filename': str})\n",
        "        \n",
        "        # Validate required columns\n",
        "        required_columns = {'filename', 'work_order_number', 'total'}\n",
        "        missing_columns = required_columns - set(ground_truth.columns)\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing required columns in ground truth: {missing_columns}\")\n",
        "        \n",
        "        # Clean and validate data\n",
        "        ground_truth['filename'] = ground_truth['filename'].str.strip()\n",
        "        ground_truth['work_order_number'] = ground_truth['work_order_number'].astype(str).str.strip()\n",
        "        \n",
        "        print(f\"INFO: Loaded ground truth data: {len(ground_truth)} records\")\n",
        "        return ground_truth\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error loading ground truth data: {e}\")\n",
        "        raise\n",
        "\n",
        "def discover_results_files() -> Dict[str, List[Path]]:\n",
        "    \"\"\"Discover all results files organized by model type.\"\"\"\n",
        "    print(\"INFO: Discovering results files\")\n",
        "    \n",
        "    results_files = {\n",
        "        'pixtral': [],\n",
        "        'llama': [],\n",
        "        'doctr': [],\n",
        "        'all': []\n",
        "    }\n",
        "    \n",
        "    # Get all results JSON files\n",
        "    all_files = list(RESULTS_DIR.glob(\"results-*.json\"))\n",
        "    \n",
        "    for file in all_files:\n",
        "        results_files['all'].append(file)\n",
        "        \n",
        "        # Categorize by model type based on filename pattern\n",
        "        if 'pixtral' in file.name:\n",
        "            results_files['pixtral'].append(file)\n",
        "        elif 'llama' in file.name:\n",
        "            results_files['llama'].append(file)\n",
        "        elif 'doctr' in file.name:\n",
        "            results_files['doctr'].append(file)\n",
        "    \n",
        "    # Sort files by modification time (newest first)\n",
        "    for model_type in results_files:\n",
        "        results_files[model_type].sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    \n",
        "    print(f\"INFO: Found {len(results_files['all'])} total results files\")\n",
        "    for model_type, files in results_files.items():\n",
        "        if model_type != 'all' and files:\n",
        "            print(f\"INFO:   {model_type}: {len(files)} files\")\n",
        "    \n",
        "    return results_files\n",
        "\n",
        "def discover_analysis_files() -> Dict[str, List[Path]]:\n",
        "    \"\"\"Discover all analysis files organized by model type.\"\"\"\n",
        "    print(\"INFO: Discovering analysis files\")\n",
        "    \n",
        "    analysis_files = {\n",
        "        'pixtral': [],\n",
        "        'llama': [],\n",
        "        'doctr': [],\n",
        "        'all': []\n",
        "    }\n",
        "    \n",
        "    # Get all analysis JSON files\n",
        "    all_files = list(ANALYSIS_DIR.glob(\"analysis-*.json\"))\n",
        "    \n",
        "    for file in all_files:\n",
        "        analysis_files['all'].append(file)\n",
        "        \n",
        "        # Categorize by model type based on filename pattern\n",
        "        if 'pixtral' in file.name:\n",
        "            analysis_files['pixtral'].append(file)\n",
        "        elif 'llama' in file.name:\n",
        "            analysis_files['llama'].append(file)\n",
        "        elif 'doctr' in file.name:\n",
        "            analysis_files['doctr'].append(file)\n",
        "    \n",
        "    # Sort files by modification time (newest first)\n",
        "    for model_type in analysis_files:\n",
        "        analysis_files[model_type].sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    \n",
        "    print(f\"INFO: Found {len(analysis_files['all'])} total analysis files\")\n",
        "    for model_type, files in analysis_files.items():\n",
        "        if model_type != 'all' and files:\n",
        "            print(f\"INFO:   {model_type}: {len(files)} files\")\n",
        "    \n",
        "    return analysis_files\n",
        "\n",
        "def load_results_file(file_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Load and validate a results JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Validate structure\n",
        "        required_keys = {'metadata', 'results'}\n",
        "        missing_keys = required_keys - set(data.keys())\n",
        "        if missing_keys:\n",
        "            raise ValueError(f\"Missing required keys in results file: {missing_keys}\")\n",
        "        \n",
        "        # Add file metadata\n",
        "        data['file_info'] = {\n",
        "            'filename': file_path.name,\n",
        "            'file_path': str(file_path),\n",
        "            'file_size_mb': round(file_path.stat().st_size / (1024*1024), 2),\n",
        "            'modification_time': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()\n",
        "        }\n",
        "        \n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error loading results file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_analysis_file(file_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Load and validate an analysis JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Validate structure\n",
        "        required_keys = {'metadata', 'summary', 'extracted_data'}\n",
        "        missing_keys = required_keys - set(data.keys())\n",
        "        if missing_keys:\n",
        "            raise ValueError(f\"Missing required keys in analysis file: {missing_keys}\")\n",
        "        \n",
        "        # Add file metadata\n",
        "        data['file_info'] = {\n",
        "            'filename': file_path.name,\n",
        "            'file_path': str(file_path),\n",
        "            'file_size_mb': round(file_path.stat().st_size / (1024*1024), 2),\n",
        "            'modification_time': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()\n",
        "        }\n",
        "        \n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error loading analysis file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_all_results(model_types: List[str] = None) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"Load all results files for specified model types.\"\"\"\n",
        "    print(\"INFO: Loading all results files\")\n",
        "    \n",
        "    if model_types is None:\n",
        "        model_types = ['pixtral', 'llama', 'doctr']\n",
        "    \n",
        "    results_files = discover_results_files()\n",
        "    all_results = {}\n",
        "    \n",
        "    for model_type in model_types:\n",
        "        if model_type in results_files:\n",
        "            all_results[model_type] = []\n",
        "            for file_path in results_files[model_type]:\n",
        "                try:\n",
        "                    result_data = load_results_file(file_path)\n",
        "                    all_results[model_type].append(result_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Skipping corrupted results file {file_path}: {e}\")\n",
        "    \n",
        "    total_loaded = sum(len(results) for results in all_results.values())\n",
        "    print(f\"INFO: Loaded {total_loaded} results files across {len(all_results)} model types\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def load_all_analysis(model_types: List[str] = None) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"Load all analysis files for specified model types.\"\"\"\n",
        "    print(\"INFO: Loading all analysis files\")\n",
        "    \n",
        "    if model_types is None:\n",
        "        model_types = ['pixtral', 'llama', 'doctr']\n",
        "    \n",
        "    analysis_files = discover_analysis_files()\n",
        "    all_analysis = {}\n",
        "    \n",
        "    for model_type in model_types:\n",
        "        if model_type in analysis_files:\n",
        "            all_analysis[model_type] = []\n",
        "            for file_path in analysis_files[model_type]:\n",
        "                try:\n",
        "                    analysis_data = load_analysis_file(file_path)\n",
        "                    all_analysis[model_type].append(analysis_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Skipping corrupted analysis file {file_path}: {e}\")\n",
        "    \n",
        "    total_loaded = sum(len(analyses) for analyses in all_analysis.values())\n",
        "    print(f\"INFO: Loaded {total_loaded} analysis files across {len(all_analysis)} model types\")\n",
        "    \n",
        "    return all_analysis\n",
        "\n",
        "def select_files_interactive(file_type: str = \"results\") -> List[Path]:\n",
        "    \"\"\"Interactive file selection for analysis.\"\"\"\n",
        "    if file_type == \"results\":\n",
        "        files_dict = discover_results_files()\n",
        "        title = \"Available Results Files\"\n",
        "    elif file_type == \"analysis\":\n",
        "        files_dict = discover_analysis_files()\n",
        "        title = \"Available Analysis Files\"\n",
        "    else:\n",
        "        raise ValueError(\"file_type must be 'results' or 'analysis'\")\n",
        "    \n",
        "    all_files = files_dict['all']\n",
        "    if not all_files:\n",
        "        print(f\"No {file_type} files found.\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"\\n{title}:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, file_path in enumerate(all_files, 1):\n",
        "        # Extract model info from filename\n",
        "        model_info = \"\"\n",
        "        if 'pixtral' in file_path.name:\n",
        "            model_info = \" [Pixtral]\"\n",
        "        elif 'llama' in file_path.name:\n",
        "            model_info = \" [Llama]\"\n",
        "        elif 'doctr' in file_path.name:\n",
        "            model_info = \" [DocTR]\"\n",
        "        \n",
        "        # Get file modification time\n",
        "        mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
        "        print(f\"{i:2d}. {file_path.name}{model_info}\")\n",
        "        print(f\"     Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    print(f\"\\n{len(all_files) + 1}. Load all files\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(f\"\\nSelect files (comma-separated numbers, or {len(all_files) + 1} for all): \")\n",
        "            \n",
        "            if choice.strip() == str(len(all_files) + 1):\n",
        "                return all_files\n",
        "            \n",
        "            # Parse comma-separated choices\n",
        "            choices = [int(x.strip()) for x in choice.split(',')]\n",
        "            selected_files = []\n",
        "            \n",
        "            for choice_num in choices:\n",
        "                if 1 <= choice_num <= len(all_files):\n",
        "                    selected_files.append(all_files[choice_num - 1])\n",
        "                else:\n",
        "                    print(f\"Invalid choice: {choice_num}\")\n",
        "                    continue\n",
        "            \n",
        "            if selected_files:\n",
        "                print(f\"\\nSelected {len(selected_files)} file(s):\")\n",
        "                for file_path in selected_files:\n",
        "                    print(f\"  - {file_path.name}\")\n",
        "                return selected_files\n",
        "            else:\n",
        "                print(\"No valid files selected.\")\n",
        "                \n",
        "        except ValueError:\n",
        "            print(\"Please enter valid numbers separated by commas.\")\n",
        "\n",
        "def create_comprehensive_dataset() -> Dict[str, Any]:\n",
        "    \"\"\"Create a comprehensive dataset combining all available data.\"\"\"\n",
        "    print(\"INFO: Creating comprehensive dataset\")\n",
        "    \n",
        "    # Load ground truth\n",
        "    ground_truth = load_ground_truth_data()\n",
        "    \n",
        "    # Load all analysis files (which contain the processed results)\n",
        "    all_analysis = load_all_analysis()\n",
        "    \n",
        "    # Create comprehensive dataset structure\n",
        "    dataset = {\n",
        "        'ground_truth': ground_truth,\n",
        "        'model_data': {},\n",
        "        'metadata': {\n",
        "            'created_timestamp': datetime.now().isoformat(),\n",
        "            'total_models': 0,\n",
        "            'total_experiments': 0,\n",
        "            'data_sources': {\n",
        "                'ground_truth_file': str(DATA_DIR / \"images\" / \"metadata\" / \"ground_truth.csv\"),\n",
        "                'results_directory': str(RESULTS_DIR),\n",
        "                'analysis_directory': str(ANALYSIS_DIR)\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    total_experiments = 0\n",
        "    for model_type, analyses in all_analysis.items():\n",
        "        if analyses:\n",
        "            dataset['model_data'][model_type] = analyses\n",
        "            total_experiments += len(analyses)\n",
        "            print(f\"INFO: Added {len(analyses)} experiments for {model_type}\")\n",
        "    \n",
        "    dataset['metadata']['total_models'] = len(dataset['model_data'])\n",
        "    dataset['metadata']['total_experiments'] = total_experiments\n",
        "    \n",
        "    print(f\"INFO: Comprehensive dataset created with {dataset['metadata']['total_models']} models and {total_experiments} experiments\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Initialize data loading and create comprehensive dataset\n",
        "print(\"‚ÑπÔ∏è  Initializing data loading functions\")\n",
        "\n",
        "# Verify data directories exist\n",
        "required_dirs = [RESULTS_DIR, ANALYSIS_DIR, DATA_DIR / \"images\" / \"metadata\"]\n",
        "for dir_path in required_dirs:\n",
        "    if not dir_path.exists():\n",
        "        print(f\"WARNING: Creating missing directory: {dir_path}\")\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Discover available data files\n",
        "available_results = discover_results_files()\n",
        "available_analysis = discover_analysis_files()\n",
        "\n",
        "# Load ground truth data\n",
        "try:\n",
        "    GROUND_TRUTH_DATA = load_ground_truth_data()\n",
        "    print(f\"‚úÖ Ground truth loaded: {len(GROUND_TRUTH_DATA)} records\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Could not load ground truth data: {e}\")\n",
        "    GROUND_TRUTH_DATA = None\n",
        "\n",
        "# Create comprehensive dataset for analysis\n",
        "try:\n",
        "    COMPREHENSIVE_DATASET = create_comprehensive_dataset()\n",
        "    print(\"‚úÖ Comprehensive dataset created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Could not create comprehensive dataset: {e}\")\n",
        "    COMPREHENSIVE_DATASET = None\n",
        "\n",
        "# Display summary of available data\n",
        "print(\"\\nüìä Data Loading Summary:\")\n",
        "print(f\"   ‚Ä¢ Ground truth records: {len(GROUND_TRUTH_DATA) if GROUND_TRUTH_DATA is not None else 'Not available'}\")\n",
        "print(f\"   ‚Ä¢ Results files found: {len(available_results['all'])}\")\n",
        "print(f\"   ‚Ä¢ Analysis files found: {len(available_analysis['all'])}\")\n",
        "\n",
        "if available_results['all']:\n",
        "    print(\"\\n   Results by model type:\")\n",
        "    for model_type, files in available_results.items():\n",
        "        if model_type != 'all' and files:\n",
        "            print(f\"     - {model_type.title()}: {len(files)} files\")\n",
        "\n",
        "if available_analysis['all']:\n",
        "    print(\"\\n   Analysis by model type:\")\n",
        "    for model_type, files in available_analysis.items():\n",
        "        if model_type != 'all' and files:\n",
        "            print(f\"     - {model_type.title()}: {len(files)} files\")\n",
        "\n",
        "print(\"\\n‚úÖ Data loading functions ready for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 1: Executive Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Experimental Design & Controlled Variables\n",
        "\n",
        "*Placeholder for discussion of controlled experimental design, image quality control, content standardization, and design rationale.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 1.1: Project Context & Key Findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Primary Performance Comparison Bar Chart\n",
        "# Side-by-side comparison of total accuracy for all LMM trials vs all OCR trials\n",
        "# Roll up across all prompts and queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model Type Breakdown Bar Chart\n",
        "# Break down into model types within each category\n",
        "# (LMM-Pixtral, LMM-Llama, OCR with all 7 recognition models)\n",
        "# Group by category and order by performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Complete Model Performance Bar Chart\n",
        "# All models organized by performance, color coded by category (LMM vs OCR only)\n",
        "# Include 85% accuracy reference line for industry automation standards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for key findings discussion and business case establishment.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 2: Cross-Model Performance Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 2.1: Comprehensive Model Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Models vs Prompts Heatmap (Accuracy)\n",
        "# Pixtral/Llama (rows) √ó Prompt types (columns) with accuracy values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Models vs Prompts Heatmap (CER)\n",
        "# Pixtral/Llama (rows) √ó Prompt types (columns) with CER values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Prompts vs Query Heatmap (Accuracy)\n",
        "# Prompt types (rows) √ó Query types (Work Order/Total Cost) with accuracy values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Prompts vs Query Heatmap (CER)\n",
        "# Prompt types (rows) √ó Query types (Work Order/Total Cost) with CER values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create All Models vs Query Heatmap (Accuracy)\n",
        "# All models including OCR (rows) √ó Query types (columns) with accuracy values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create All Models vs Query Heatmap (CER)\n",
        "# All models including OCR (rows) √ó Query types (columns) with CER values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for analysis of LMM model responses to different prompt strategies, optimal prompt-model combinations, and CER pattern relationships.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 2.2: Model Consistency Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Coefficient of Variation Bar Chart\n",
        "# Performance stability across prompts for each model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Min-Max Range Visualization\n",
        "# Performance ranges to identify most/least consistent models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for evaluation of performance stability across different conditions.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 3: Error Pattern Taxonomy & System Improvement Insights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 3.1: Systematic Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Error Pattern Examples visualization\n",
        "# Visual examples of each error category with actual vs. expected results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Post-Processing Opportunity Assessment\n",
        "# Estimate potential accuracy improvements for each error type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for identification of patterns that could be addressed through post-processing.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 3.2: Error Classification System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Error Type Distribution Pie Charts\n",
        "# Separate charts for Work Order vs. Total Cost errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Error Frequency Heatmap\n",
        "# Error types (rows) √ó Models (columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for categorization and quantification of different types of failures.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 3.3: Failure Mode Deep Dive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Failure Severity Distribution\n",
        "# Histogram of error magnitudes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model Robustness Comparison\n",
        "# How models handle edge cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for understanding catastrophic vs. graceful degradation patterns.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 4: Prompt Engineering Effectiveness Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 4.1: Prompt Strategy Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Prompt Performance Matrix\n",
        "# Accuracy gains/losses by prompt type across models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Prompt-Model Interaction Effects\n",
        "# Line graphs showing how each model responds to different prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for quantifying effectiveness of different prompting approaches.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 5: Field-Specific Performance Deep Dive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 5.1: Work Order vs. Total Cost Performance Differential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Field Performance Comparison\n",
        "# Side-by-side accuracy for each field across all models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Performance Gap Analysis\n",
        "# Difference between Total Cost and Work Order accuracy by model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for understanding why models excel at one field but struggle with another.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 6: Character Error Rate (CER) Deep Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 6.1: CER Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create CER Distribution Histograms\n",
        "# Separate for Work Order and Total Cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model CER Comparison Box Plots\n",
        "# Show ranges and outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for understanding the spread and clustering of character-level errors.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 7: Computational Efficiency Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 7.1: Performance per Resource Unit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Efficiency Frontier Plot\n",
        "# Accuracy vs. computational cost scatter plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Cost-Benefit Analysis\n",
        "# ROI calculations for different model choices\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for comparing accuracy gains vs. computational cost increases.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 8: Statistical Overview & Significance Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 8.1: Statistical Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Performance Distribution Box Plots\n",
        "# Accuracy ranges across all model/prompt combinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Statistical Significance Matrix\n",
        "# P-values for key comparisons\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for high-level statistical summary of all results.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 9: Synthesis & Key Insights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 9.1: Model Selection Decision Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Multi-Criteria Decision Matrix\n",
        "# Weighted scoring across accuracy, speed, cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Use Case Recommendations\n",
        "# Different models for different deployment scenarios\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for providing clear guidance for model choice based on different criteria.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 9.2: System Improvement Roadmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Improvement Opportunity Matrix\n",
        "# Effort vs. Impact for different enhancement areas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Implementation Timeline\n",
        "# Suggested sequence for system improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for prioritizing enhancement opportunities based on analysis findings.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 9.3: Unexpected Findings & Future Research\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Create Unexpected Findings Highlight* \n",
        "*Key discoveries and their implications*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Create Future Research Opportunities\n",
        "*Areas identified for continued investigation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for highlighting discoveries not anticipated in initial research design.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Final Analysis Framework v2.0 - Focused Results Analysis\n",
        "\n",
        "This analysis framework focuses on understanding the experimental results from the construction invoice processing study, incorporating controlled experimental design considerations and practical system improvement insights.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Requirements Installation and Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "🚀 Analysis Requirements Installation & Verification\n",
            "======================================================================\n",
            "🔍 Checking current package status...\n",
            "--------------------------------------------------\n",
            "✅ pandas          - Already installed\n",
            "✅ numpy           - Already installed\n",
            "❌ matplotlib      - Needs installation\n",
            "❌ seaborn         - Needs installation\n",
            "✅ scipy           - Already installed\n",
            "✅ scikit-learn    - Already installed\n",
            "❌ statsmodels     - Needs installation\n",
            "✅ tqdm            - Already installed\n",
            "✅ pyyaml          - Already installed\n",
            "\n",
            "📊 Status Summary:\n",
            "   Already installed: 6\n",
            "   Need installation: 3\n",
            "   Packages to install: matplotlib, seaborn, statsmodels\n",
            "\n",
            "⏱️  Starting installation process at 12:46:51\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "\n",
            "📦 Installing 14 packages from requirements file...\n",
            "============================================================\n",
            "\n",
            "[1/14] Installing pandas...\n",
            "   Full requirement: pandas>=1.5.0\n",
            "   ✅ pandas installed successfully (1.7s)\n",
            "\n",
            "[2/14] Installing numpy...\n",
            "   Full requirement: numpy>=1.21.0\n",
            "   ✅ numpy installed successfully (1.5s)\n",
            "\n",
            "[3/14] Installing matplotlib...\n",
            "   Full requirement: matplotlib>=3.5.0\n",
            "   ✅ matplotlib installed successfully (33.6s)\n",
            "\n",
            "[4/14] Installing seaborn...\n",
            "   Full requirement: seaborn>=0.11.0\n",
            "   ✅ seaborn installed successfully (6.9s)\n",
            "\n",
            "[5/14] Installing plotly...\n",
            "   Full requirement: plotly>=5.0.0\n",
            "   ⏰ Timeout installing plotly (>3 minutes)\n",
            "\n",
            "[6/14] Installing scipy...\n",
            "   Full requirement: scipy>=1.8.0\n",
            "   ✅ scipy installed successfully (2.1s)\n",
            "\n",
            "[7/14] Installing scikit-learn...\n",
            "   Full requirement: scikit-learn>=1.1.0\n",
            "   ✅ scikit-learn installed successfully (2.1s)\n",
            "\n",
            "[8/14] Installing statsmodels...\n",
            "   Full requirement: statsmodels>=0.13.0\n",
            "   ✅ statsmodels installed successfully (137.4s)\n",
            "\n",
            "[9/14] Installing pathlib2...\n",
            "   Full requirement: pathlib2>=2.3.0\n",
            "   ✅ pathlib2 installed successfully (2.2s)\n",
            "\n",
            "[10/14] Installing tqdm...\n",
            "   Full requirement: tqdm>=4.64.0\n",
            "   ✅ tqdm installed successfully (2.0s)\n",
            "\n",
            "[11/14] Installing pyyaml...\n",
            "   Full requirement: pyyaml>=6.0\n",
            "   ✅ pyyaml installed successfully (1.9s)\n",
            "\n",
            "[12/14] Installing ipywidgets...\n",
            "   Full requirement: ipywidgets>=7.6.0\n",
            "   ✅ ipywidgets installed successfully (2.0s)\n",
            "\n",
            "[13/14] Installing jupyter...\n",
            "   Full requirement: jupyter>=1.0.0\n",
            "   ✅ jupyter installed successfully (149.8s)\n",
            "\n",
            "[14/14] Installing psutil...\n",
            "   Full requirement: psutil>=5.8.0\n",
            "   ✅ psutil installed successfully (2.6s)\n",
            "\n",
            "============================================================\n",
            "📊 Installation Summary:\n",
            "   ✅ Successful: 13\n",
            "   ❌ Failed: 1\n",
            "\n",
            "   Successfully installed: pandas, numpy, matplotlib, seaborn, scipy\n",
            "   ... and 8 more\n",
            "\n",
            "   ⚠️  Failed packages: plotly\n",
            "   These will be retried with basic fallback...\n",
            "\n",
            "🔄 Installing 1 basic requirements...\n",
            "--------------------------------------------------\n",
            "[1/1] Installing plotly... ⏰ Timeout (>3 min)\n",
            "\n",
            "⏱️  Total installation time: 706.1 seconds\n",
            "\n",
            "🔍 Verifying library imports...\n",
            "----------------------------------------\n",
            "✅ pandas               - OK\n",
            "✅ numpy                - OK\n",
            "❌ matplotlib.pyplot    - FAILED: No module named 'matplotlib'...\n",
            "❌ seaborn              - FAILED: No module named 'seaborn'...\n",
            "✅ scipy                - OK\n",
            "✅ pathlib              - OK\n",
            "✅ json                 - OK\n",
            "✅ yaml                 - OK\n",
            "✅ sklearn              - OK\n",
            "❌ statsmodels.api      - FAILED: No module named 'statsmodels'...\n",
            "\n",
            "⚠️  Warning: 3 libraries failed to import\n",
            "   Failed libraries: matplotlib.pyplot, seaborn, statsmodels.api\n",
            "   You may need to restart the kernel after installation\n",
            "\n",
            "======================================================================\n",
            "⚠️  Some issues detected. You may need to restart the kernel.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback and detailed progress.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0',\n",
        "        'scikit-learn>=1.1.0',\n",
        "        'statsmodels>=0.13.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Read requirements file to show what will be installed\n",
        "            with open(requirements_file, 'r') as f:\n",
        "                requirements_content = f.read().strip().split('\\n')\n",
        "            \n",
        "            # Filter out comments and empty lines\n",
        "            requirements_list = [\n",
        "                req.strip() for req in requirements_content \n",
        "                if req.strip() and not req.strip().startswith('#')\n",
        "            ]\n",
        "            \n",
        "            print(f\"\\n📦 Installing {len(requirements_list)} packages from requirements file...\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Install each requirement individually with progress\n",
        "            failed_packages = []\n",
        "            successful_packages = []\n",
        "            \n",
        "            for i, requirement in enumerate(requirements_list, 1):\n",
        "                package_name = requirement.split('>=')[0].split('==')[0].split('[')[0]\n",
        "                print(f\"\\n[{i}/{len(requirements_list)}] Installing {package_name}...\")\n",
        "                print(f\"   Full requirement: {requirement}\")\n",
        "                \n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    result = subprocess.run([\n",
        "                        sys.executable, '-m', 'pip', 'install', requirement, '--timeout', '120'\n",
        "                    ], capture_output=True, text=True, timeout=180)  # 3 minute timeout per package\n",
        "                    \n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    \n",
        "                    if result.returncode == 0:\n",
        "                        print(f\"   ✅ {package_name} installed successfully ({elapsed_time:.1f}s)\")\n",
        "                        successful_packages.append(package_name)\n",
        "                    else:\n",
        "                        print(f\"   ❌ Failed to install {package_name}\")\n",
        "                        print(f\"   Error: {result.stderr[:200]}...\")\n",
        "                        failed_packages.append(package_name)\n",
        "                        \n",
        "                except subprocess.TimeoutExpired:\n",
        "                    print(f\"   ⏰ Timeout installing {package_name} (>3 minutes)\")\n",
        "                    failed_packages.append(package_name)\n",
        "                except Exception as e:\n",
        "                    print(f\"   ❌ Exception installing {package_name}: {e}\")\n",
        "                    failed_packages.append(package_name)\n",
        "            \n",
        "            # Summary\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(f\"📊 Installation Summary:\")\n",
        "            print(f\"   ✅ Successful: {len(successful_packages)}\")\n",
        "            print(f\"   ❌ Failed: {len(failed_packages)}\")\n",
        "            \n",
        "            if successful_packages:\n",
        "                print(f\"\\n   Successfully installed: {', '.join(successful_packages[:5])}\")\n",
        "                if len(successful_packages) > 5:\n",
        "                    print(f\"   ... and {len(successful_packages) - 5} more\")\n",
        "            \n",
        "            if failed_packages:\n",
        "                print(f\"\\n   ⚠️  Failed packages: {', '.join(failed_packages)}\")\n",
        "                print(\"   These will be retried with basic fallback...\")\n",
        "                install_basic_requirements([f\"{pkg}>=0.0.0\" for pkg in failed_packages])\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error reading requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually with progress feedback.\"\"\"\n",
        "    print(f\"\\n🔄 Installing {len(requirements_list)} basic requirements...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for i, req in enumerate(requirements_list, 1):\n",
        "        package_name = req.split('>=')[0].split('==')[0].split('[')[0]\n",
        "        print(f\"[{i}/{len(requirements_list)}] Installing {package_name}...\", end=\" \")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req, '--timeout', '120'\n",
        "            ], capture_output=True, text=True, timeout=180)\n",
        "            \n",
        "            elapsed_time = time.time() - start_time\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                print(f\"✅ ({elapsed_time:.1f}s)\")\n",
        "            else:\n",
        "                print(f\"❌ Failed\")\n",
        "                print(f\"    Error: {result.stderr[:100]}...\")\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"⏰ Timeout (>3 min)\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Exception: {str(e)[:50]}...\")\n",
        "\n",
        "def check_package_installed(package_name):\n",
        "    \"\"\"Check if a package is already installed.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'show', package_name\n",
        "        ], capture_output=True, text=True)\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml',\n",
        "        'sklearn': 'sklearn',\n",
        "        'statsmodels.api': 'sm'\n",
        "    }\n",
        "    \n",
        "    print(\"\\n🔍 Verifying library imports...\")\n",
        "    print(\"-\" * 40)\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✅ {lib:<20} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"❌ {lib:<20} - FAILED: {str(e)[:50]}...\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠️  Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"   Failed libraries:\", ', '.join(failed_imports))\n",
        "        print(\"   You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✅ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "def show_pre_installation_status():\n",
        "    \"\"\"Show which packages are already installed.\"\"\"\n",
        "    check_packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scipy', \n",
        "                     'scikit-learn', 'statsmodels', 'tqdm', 'pyyaml']\n",
        "    \n",
        "    print(\"🔍 Checking current package status...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    already_installed = []\n",
        "    need_installation = []\n",
        "    \n",
        "    for package in check_packages:\n",
        "        if check_package_installed(package):\n",
        "            print(f\"✅ {package:<15} - Already installed\")\n",
        "            already_installed.append(package)\n",
        "        else:\n",
        "            print(f\"❌ {package:<15} - Needs installation\")\n",
        "            need_installation.append(package)\n",
        "    \n",
        "    print(f\"\\n📊 Status Summary:\")\n",
        "    print(f\"   Already installed: {len(already_installed)}\")\n",
        "    print(f\"   Need installation: {len(need_installation)}\")\n",
        "    \n",
        "    if need_installation:\n",
        "        print(f\"   Packages to install: {', '.join(need_installation)}\")\n",
        "    \n",
        "    return already_installed, need_installation\n",
        "\n",
        "# Run installation and verification with progress tracking\n",
        "print(\"=\" * 70)\n",
        "print(\"🚀 Analysis Requirements Installation & Verification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show pre-installation status\n",
        "already_installed, need_installation = show_pre_installation_status()\n",
        "\n",
        "# Proceed with installation\n",
        "print(f\"\\n⏱️  Starting installation process at {time.strftime('%H:%M:%S')}\")\n",
        "start_total = time.time()\n",
        "\n",
        "install_requirements()\n",
        "\n",
        "total_time = time.time() - start_total\n",
        "print(f\"\\n⏱️  Total installation time: {total_time:.1f} seconds\")\n",
        "\n",
        "# Verify installation\n",
        "verification_success = verify_imports()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if verification_success:\n",
        "    print(\"🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"⚠️  Some issues detected. You may need to restart the kernel.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "🚀 Analysis Requirements Installation & Verification\n",
            "======================================================================\n",
            "🔍 Checking current package status...\n",
            "--------------------------------------------------\n",
            "✅ pandas          - Already installed\n",
            "✅ numpy           - Already installed\n",
            "❌ matplotlib      - Needs installation\n",
            "❌ seaborn         - Needs installation\n",
            "✅ scipy           - Already installed\n",
            "✅ scikit-learn    - Already installed\n",
            "❌ statsmodels     - Needs installation\n",
            "✅ tqdm            - Already installed\n",
            "✅ pyyaml          - Already installed\n",
            "\n",
            "📊 Status Summary:\n",
            "   Already installed: 6\n",
            "   Need installation: 3\n",
            "   Packages to install: matplotlib, seaborn, statsmodels\n",
            "\n",
            "⏱️  Starting installation process at 12:46:51\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "\n",
            "📦 Installing 14 packages from requirements file...\n",
            "============================================================\n",
            "\n",
            "[1/14] Installing pandas...\n",
            "   Full requirement: pandas>=1.5.0\n",
            "   ✅ pandas installed successfully (1.7s)\n",
            "\n",
            "[2/14] Installing numpy...\n",
            "   Full requirement: numpy>=1.21.0\n",
            "   ✅ numpy installed successfully (1.5s)\n",
            "\n",
            "[3/14] Installing matplotlib...\n",
            "   Full requirement: matplotlib>=3.5.0\n",
            "   ✅ matplotlib installed successfully (33.6s)\n",
            "\n",
            "[4/14] Installing seaborn...\n",
            "   Full requirement: seaborn>=0.11.0\n",
            "   ✅ seaborn installed successfully (6.9s)\n",
            "\n",
            "[5/14] Installing plotly...\n",
            "   Full requirement: plotly>=5.0.0\n",
            "   ⏰ Timeout installing plotly (>3 minutes)\n",
            "\n",
            "[6/14] Installing scipy...\n",
            "   Full requirement: scipy>=1.8.0\n",
            "   ✅ scipy installed successfully (2.1s)\n",
            "\n",
            "[7/14] Installing scikit-learn...\n",
            "   Full requirement: scikit-learn>=1.1.0\n",
            "   ✅ scikit-learn installed successfully (2.1s)\n",
            "\n",
            "[8/14] Installing statsmodels...\n",
            "   Full requirement: statsmodels>=0.13.0\n",
            "   ✅ statsmodels installed successfully (137.4s)\n",
            "\n",
            "[9/14] Installing pathlib2...\n",
            "   Full requirement: pathlib2>=2.3.0\n",
            "   ✅ pathlib2 installed successfully (2.2s)\n",
            "\n",
            "[10/14] Installing tqdm...\n",
            "   Full requirement: tqdm>=4.64.0\n",
            "   ✅ tqdm installed successfully (2.0s)\n",
            "\n",
            "[11/14] Installing pyyaml...\n",
            "   Full requirement: pyyaml>=6.0\n",
            "   ✅ pyyaml installed successfully (1.9s)\n",
            "\n",
            "[12/14] Installing ipywidgets...\n",
            "   Full requirement: ipywidgets>=7.6.0\n",
            "   ✅ ipywidgets installed successfully (2.0s)\n",
            "\n",
            "[13/14] Installing jupyter...\n",
            "   Full requirement: jupyter>=1.0.0\n",
            "   ✅ jupyter installed successfully (149.8s)\n",
            "\n",
            "[14/14] Installing psutil...\n",
            "   Full requirement: psutil>=5.8.0\n",
            "   ✅ psutil installed successfully (2.6s)\n",
            "\n",
            "============================================================\n",
            "📊 Installation Summary:\n",
            "   ✅ Successful: 13\n",
            "   ❌ Failed: 1\n",
            "\n",
            "   Successfully installed: pandas, numpy, matplotlib, seaborn, scipy\n",
            "   ... and 8 more\n",
            "\n",
            "   ⚠️  Failed packages: plotly\n",
            "   These will be retried with basic fallback...\n",
            "\n",
            "🔄 Installing 1 basic requirements...\n",
            "--------------------------------------------------\n",
            "[1/1] Installing plotly... ⏰ Timeout (>3 min)\n",
            "\n",
            "⏱️  Total installation time: 706.1 seconds\n",
            "\n",
            "🔍 Verifying library imports...\n",
            "----------------------------------------\n",
            "✅ pandas               - OK\n",
            "✅ numpy                - OK\n",
            "❌ matplotlib.pyplot    - FAILED: No module named 'matplotlib'...\n",
            "❌ seaborn              - FAILED: No module named 'seaborn'...\n",
            "✅ scipy                - OK\n",
            "✅ pathlib              - OK\n",
            "✅ json                 - OK\n",
            "✅ yaml                 - OK\n",
            "✅ sklearn              - OK\n",
            "❌ statsmodels.api      - FAILED: No module named 'statsmodels'...\n",
            "\n",
            "⚠️  Warning: 3 libraries failed to import\n",
            "   Failed libraries: matplotlib.pyplot, seaborn, statsmodels.api\n",
            "   You may need to restart the kernel after installation\n",
            "\n",
            "======================================================================\n",
            "⚠️  Some issues detected. You may need to restart the kernel.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback and detailed progress.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0',\n",
        "        'scikit-learn>=1.1.0',\n",
        "        'statsmodels>=0.13.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Read requirements file to show what will be installed\n",
        "            with open(requirements_file, 'r') as f:\n",
        "                requirements_content = f.read().strip().split('\\n')\n",
        "            \n",
        "            # Filter out comments and empty lines\n",
        "            requirements_list = [\n",
        "                req.strip() for req in requirements_content \n",
        "                if req.strip() and not req.strip().startswith('#')\n",
        "            ]\n",
        "            \n",
        "            print(f\"\\n📦 Installing {len(requirements_list)} packages from requirements file...\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Install each requirement individually with progress\n",
        "            failed_packages = []\n",
        "            successful_packages = []\n",
        "            \n",
        "            for i, requirement in enumerate(requirements_list, 1):\n",
        "                package_name = requirement.split('>=')[0].split('==')[0].split('[')[0]\n",
        "                print(f\"\\n[{i}/{len(requirements_list)}] Installing {package_name}...\")\n",
        "                print(f\"   Full requirement: {requirement}\")\n",
        "                \n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    result = subprocess.run([\n",
        "                        sys.executable, '-m', 'pip', 'install', requirement, '--timeout', '120'\n",
        "                    ], capture_output=True, text=True, timeout=180)  # 3 minute timeout per package\n",
        "                    \n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    \n",
        "                    if result.returncode == 0:\n",
        "                        print(f\"   ✅ {package_name} installed successfully ({elapsed_time:.1f}s)\")\n",
        "                        successful_packages.append(package_name)\n",
        "                    else:\n",
        "                        print(f\"   ❌ Failed to install {package_name}\")\n",
        "                        print(f\"   Error: {result.stderr[:200]}...\")\n",
        "                        failed_packages.append(package_name)\n",
        "                        \n",
        "                except subprocess.TimeoutExpired:\n",
        "                    print(f\"   ⏰ Timeout installing {package_name} (>3 minutes)\")\n",
        "                    failed_packages.append(package_name)\n",
        "                except Exception as e:\n",
        "                    print(f\"   ❌ Exception installing {package_name}: {e}\")\n",
        "                    failed_packages.append(package_name)\n",
        "            \n",
        "            # Summary\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(f\"📊 Installation Summary:\")\n",
        "            print(f\"   ✅ Successful: {len(successful_packages)}\")\n",
        "            print(f\"   ❌ Failed: {len(failed_packages)}\")\n",
        "            \n",
        "            if successful_packages:\n",
        "                print(f\"\\n   Successfully installed: {', '.join(successful_packages[:5])}\")\n",
        "                if len(successful_packages) > 5:\n",
        "                    print(f\"   ... and {len(successful_packages) - 5} more\")\n",
        "            \n",
        "            if failed_packages:\n",
        "                print(f\"\\n   ⚠️  Failed packages: {', '.join(failed_packages)}\")\n",
        "                print(\"   These will be retried with basic fallback...\")\n",
        "                install_basic_requirements([f\"{pkg}>=0.0.0\" for pkg in failed_packages])\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error reading requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually with progress feedback.\"\"\"\n",
        "    print(f\"\\n🔄 Installing {len(requirements_list)} basic requirements...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for i, req in enumerate(requirements_list, 1):\n",
        "        package_name = req.split('>=')[0].split('==')[0].split('[')[0]\n",
        "        print(f\"[{i}/{len(requirements_list)}] Installing {package_name}...\", end=\" \")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req, '--timeout', '120'\n",
        "            ], capture_output=True, text=True, timeout=180)\n",
        "            \n",
        "            elapsed_time = time.time() - start_time\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                print(f\"✅ ({elapsed_time:.1f}s)\")\n",
        "            else:\n",
        "                print(f\"❌ Failed\")\n",
        "                print(f\"    Error: {result.stderr[:100]}...\")\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"⏰ Timeout (>3 min)\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Exception: {str(e)[:50]}...\")\n",
        "\n",
        "def check_package_installed(package_name):\n",
        "    \"\"\"Check if a package is already installed.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'show', package_name\n",
        "        ], capture_output=True, text=True)\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml',\n",
        "        'sklearn': 'sklearn',\n",
        "        'statsmodels.api': 'sm'\n",
        "    }\n",
        "    \n",
        "    print(\"\\n🔍 Verifying library imports...\")\n",
        "    print(\"-\" * 40)\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✅ {lib:<20} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"❌ {lib:<20} - FAILED: {str(e)[:50]}...\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠️  Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"   Failed libraries:\", ', '.join(failed_imports))\n",
        "        print(\"   You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✅ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "def show_pre_installation_status():\n",
        "    \"\"\"Show which packages are already installed.\"\"\"\n",
        "    check_packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scipy', \n",
        "                     'scikit-learn', 'statsmodels', 'tqdm', 'pyyaml']\n",
        "    \n",
        "    print(\"🔍 Checking current package status...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    already_installed = []\n",
        "    need_installation = []\n",
        "    \n",
        "    for package in check_packages:\n",
        "        if check_package_installed(package):\n",
        "            print(f\"✅ {package:<15} - Already installed\")\n",
        "            already_installed.append(package)\n",
        "        else:\n",
        "            print(f\"❌ {package:<15} - Needs installation\")\n",
        "            need_installation.append(package)\n",
        "    \n",
        "    print(f\"\\n📊 Status Summary:\")\n",
        "    print(f\"   Already installed: {len(already_installed)}\")\n",
        "    print(f\"   Need installation: {len(need_installation)}\")\n",
        "    \n",
        "    if need_installation:\n",
        "        print(f\"   Packages to install: {', '.join(need_installation)}\")\n",
        "    \n",
        "    return already_installed, need_installation\n",
        "\n",
        "# Run installation and verification with progress tracking\n",
        "print(\"=\" * 70)\n",
        "print(\"🚀 Analysis Requirements Installation & Verification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show pre-installation status\n",
        "already_installed, need_installation = show_pre_installation_status()\n",
        "\n",
        "# Proceed with installation\n",
        "print(f\"\\n⏱️  Starting installation process at {time.strftime('%H:%M:%S')}\")\n",
        "start_total = time.time()\n",
        "\n",
        "install_requirements()\n",
        "\n",
        "total_time = time.time() - start_total\n",
        "print(f\"\\n⏱️  Total installation time: {total_time:.1f} seconds\")\n",
        "\n",
        "# Verify installation\n",
        "verification_success = verify_imports()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if verification_success:\n",
        "    print(\"🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"⚠️  Some issues detected. You may need to restart the kernel.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "🚀 Analysis Requirements Installation & Verification\n",
            "======================================================================\n",
            "🔍 Checking current package status...\n",
            "--------------------------------------------------\n",
            "✅ pandas          - Already installed\n",
            "✅ numpy           - Already installed\n",
            "❌ matplotlib      - Needs installation\n",
            "❌ seaborn         - Needs installation\n",
            "✅ scipy           - Already installed\n",
            "✅ scikit-learn    - Already installed\n",
            "❌ statsmodels     - Needs installation\n",
            "✅ tqdm            - Already installed\n",
            "✅ pyyaml          - Already installed\n",
            "\n",
            "📊 Status Summary:\n",
            "   Already installed: 6\n",
            "   Need installation: 3\n",
            "   Packages to install: matplotlib, seaborn, statsmodels\n",
            "\n",
            "⏱️  Starting installation process at 12:46:51\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "\n",
            "📦 Installing 14 packages from requirements file...\n",
            "============================================================\n",
            "\n",
            "[1/14] Installing pandas...\n",
            "   Full requirement: pandas>=1.5.0\n",
            "   ✅ pandas installed successfully (1.7s)\n",
            "\n",
            "[2/14] Installing numpy...\n",
            "   Full requirement: numpy>=1.21.0\n",
            "   ✅ numpy installed successfully (1.5s)\n",
            "\n",
            "[3/14] Installing matplotlib...\n",
            "   Full requirement: matplotlib>=3.5.0\n",
            "   ✅ matplotlib installed successfully (33.6s)\n",
            "\n",
            "[4/14] Installing seaborn...\n",
            "   Full requirement: seaborn>=0.11.0\n",
            "   ✅ seaborn installed successfully (6.9s)\n",
            "\n",
            "[5/14] Installing plotly...\n",
            "   Full requirement: plotly>=5.0.0\n",
            "   ⏰ Timeout installing plotly (>3 minutes)\n",
            "\n",
            "[6/14] Installing scipy...\n",
            "   Full requirement: scipy>=1.8.0\n",
            "   ✅ scipy installed successfully (2.1s)\n",
            "\n",
            "[7/14] Installing scikit-learn...\n",
            "   Full requirement: scikit-learn>=1.1.0\n",
            "   ✅ scikit-learn installed successfully (2.1s)\n",
            "\n",
            "[8/14] Installing statsmodels...\n",
            "   Full requirement: statsmodels>=0.13.0\n",
            "   ✅ statsmodels installed successfully (137.4s)\n",
            "\n",
            "[9/14] Installing pathlib2...\n",
            "   Full requirement: pathlib2>=2.3.0\n",
            "   ✅ pathlib2 installed successfully (2.2s)\n",
            "\n",
            "[10/14] Installing tqdm...\n",
            "   Full requirement: tqdm>=4.64.0\n",
            "   ✅ tqdm installed successfully (2.0s)\n",
            "\n",
            "[11/14] Installing pyyaml...\n",
            "   Full requirement: pyyaml>=6.0\n",
            "   ✅ pyyaml installed successfully (1.9s)\n",
            "\n",
            "[12/14] Installing ipywidgets...\n",
            "   Full requirement: ipywidgets>=7.6.0\n",
            "   ✅ ipywidgets installed successfully (2.0s)\n",
            "\n",
            "[13/14] Installing jupyter...\n",
            "   Full requirement: jupyter>=1.0.0\n",
            "   ✅ jupyter installed successfully (149.8s)\n",
            "\n",
            "[14/14] Installing psutil...\n",
            "   Full requirement: psutil>=5.8.0\n",
            "   ✅ psutil installed successfully (2.6s)\n",
            "\n",
            "============================================================\n",
            "📊 Installation Summary:\n",
            "   ✅ Successful: 13\n",
            "   ❌ Failed: 1\n",
            "\n",
            "   Successfully installed: pandas, numpy, matplotlib, seaborn, scipy\n",
            "   ... and 8 more\n",
            "\n",
            "   ⚠️  Failed packages: plotly\n",
            "   These will be retried with basic fallback...\n",
            "\n",
            "🔄 Installing 1 basic requirements...\n",
            "--------------------------------------------------\n",
            "[1/1] Installing plotly... ⏰ Timeout (>3 min)\n",
            "\n",
            "⏱️  Total installation time: 706.1 seconds\n",
            "\n",
            "🔍 Verifying library imports...\n",
            "----------------------------------------\n",
            "✅ pandas               - OK\n",
            "✅ numpy                - OK\n",
            "❌ matplotlib.pyplot    - FAILED: No module named 'matplotlib'...\n",
            "❌ seaborn              - FAILED: No module named 'seaborn'...\n",
            "✅ scipy                - OK\n",
            "✅ pathlib              - OK\n",
            "✅ json                 - OK\n",
            "✅ yaml                 - OK\n",
            "✅ sklearn              - OK\n",
            "❌ statsmodels.api      - FAILED: No module named 'statsmodels'...\n",
            "\n",
            "⚠️  Warning: 3 libraries failed to import\n",
            "   Failed libraries: matplotlib.pyplot, seaborn, statsmodels.api\n",
            "   You may need to restart the kernel after installation\n",
            "\n",
            "======================================================================\n",
            "⚠️  Some issues detected. You may need to restart the kernel.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback and detailed progress.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0',\n",
        "        'scikit-learn>=1.1.0',\n",
        "        'statsmodels>=0.13.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Read requirements file to show what will be installed\n",
        "            with open(requirements_file, 'r') as f:\n",
        "                requirements_content = f.read().strip().split('\\n')\n",
        "            \n",
        "            # Filter out comments and empty lines\n",
        "            requirements_list = [\n",
        "                req.strip() for req in requirements_content \n",
        "                if req.strip() and not req.strip().startswith('#')\n",
        "            ]\n",
        "            \n",
        "            print(f\"\\n📦 Installing {len(requirements_list)} packages from requirements file...\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Install each requirement individually with progress\n",
        "            failed_packages = []\n",
        "            successful_packages = []\n",
        "            \n",
        "            for i, requirement in enumerate(requirements_list, 1):\n",
        "                package_name = requirement.split('>=')[0].split('==')[0].split('[')[0]\n",
        "                print(f\"\\n[{i}/{len(requirements_list)}] Installing {package_name}...\")\n",
        "                print(f\"   Full requirement: {requirement}\")\n",
        "                \n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    result = subprocess.run([\n",
        "                        sys.executable, '-m', 'pip', 'install', requirement, '--timeout', '120'\n",
        "                    ], capture_output=True, text=True, timeout=180)  # 3 minute timeout per package\n",
        "                    \n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    \n",
        "                    if result.returncode == 0:\n",
        "                        print(f\"   ✅ {package_name} installed successfully ({elapsed_time:.1f}s)\")\n",
        "                        successful_packages.append(package_name)\n",
        "                    else:\n",
        "                        print(f\"   ❌ Failed to install {package_name}\")\n",
        "                        print(f\"   Error: {result.stderr[:200]}...\")\n",
        "                        failed_packages.append(package_name)\n",
        "                        \n",
        "                except subprocess.TimeoutExpired:\n",
        "                    print(f\"   ⏰ Timeout installing {package_name} (>3 minutes)\")\n",
        "                    failed_packages.append(package_name)\n",
        "                except Exception as e:\n",
        "                    print(f\"   ❌ Exception installing {package_name}: {e}\")\n",
        "                    failed_packages.append(package_name)\n",
        "            \n",
        "            # Summary\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(f\"📊 Installation Summary:\")\n",
        "            print(f\"   ✅ Successful: {len(successful_packages)}\")\n",
        "            print(f\"   ❌ Failed: {len(failed_packages)}\")\n",
        "            \n",
        "            if successful_packages:\n",
        "                print(f\"\\n   Successfully installed: {', '.join(successful_packages[:5])}\")\n",
        "                if len(successful_packages) > 5:\n",
        "                    print(f\"   ... and {len(successful_packages) - 5} more\")\n",
        "            \n",
        "            if failed_packages:\n",
        "                print(f\"\\n   ⚠️  Failed packages: {', '.join(failed_packages)}\")\n",
        "                print(\"   These will be retried with basic fallback...\")\n",
        "                install_basic_requirements([f\"{pkg}>=0.0.0\" for pkg in failed_packages])\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error reading requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually with progress feedback.\"\"\"\n",
        "    print(f\"\\n🔄 Installing {len(requirements_list)} basic requirements...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for i, req in enumerate(requirements_list, 1):\n",
        "        package_name = req.split('>=')[0].split('==')[0].split('[')[0]\n",
        "        print(f\"[{i}/{len(requirements_list)}] Installing {package_name}...\", end=\" \")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req, '--timeout', '120'\n",
        "            ], capture_output=True, text=True, timeout=180)\n",
        "            \n",
        "            elapsed_time = time.time() - start_time\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                print(f\"✅ ({elapsed_time:.1f}s)\")\n",
        "            else:\n",
        "                print(f\"❌ Failed\")\n",
        "                print(f\"    Error: {result.stderr[:100]}...\")\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"⏰ Timeout (>3 min)\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Exception: {str(e)[:50]}...\")\n",
        "\n",
        "def check_package_installed(package_name):\n",
        "    \"\"\"Check if a package is already installed.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'show', package_name\n",
        "        ], capture_output=True, text=True)\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml',\n",
        "        'sklearn': 'sklearn',\n",
        "        'statsmodels.api': 'sm'\n",
        "    }\n",
        "    \n",
        "    print(\"\\n🔍 Verifying library imports...\")\n",
        "    print(\"-\" * 40)\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✅ {lib:<20} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"❌ {lib:<20} - FAILED: {str(e)[:50]}...\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠️  Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"   Failed libraries:\", ', '.join(failed_imports))\n",
        "        print(\"   You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✅ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "def show_pre_installation_status():\n",
        "    \"\"\"Show which packages are already installed.\"\"\"\n",
        "    check_packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scipy', \n",
        "                     'scikit-learn', 'statsmodels', 'tqdm', 'pyyaml']\n",
        "    \n",
        "    print(\"🔍 Checking current package status...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    already_installed = []\n",
        "    need_installation = []\n",
        "    \n",
        "    for package in check_packages:\n",
        "        if check_package_installed(package):\n",
        "            print(f\"✅ {package:<15} - Already installed\")\n",
        "            already_installed.append(package)\n",
        "        else:\n",
        "            print(f\"❌ {package:<15} - Needs installation\")\n",
        "            need_installation.append(package)\n",
        "    \n",
        "    print(f\"\\n📊 Status Summary:\")\n",
        "    print(f\"   Already installed: {len(already_installed)}\")\n",
        "    print(f\"   Need installation: {len(need_installation)}\")\n",
        "    \n",
        "    if need_installation:\n",
        "        print(f\"   Packages to install: {', '.join(need_installation)}\")\n",
        "    \n",
        "    return already_installed, need_installation\n",
        "\n",
        "# Run installation and verification with progress tracking\n",
        "print(\"=\" * 70)\n",
        "print(\"🚀 Analysis Requirements Installation & Verification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show pre-installation status\n",
        "already_installed, need_installation = show_pre_installation_status()\n",
        "\n",
        "# Proceed with installation\n",
        "print(f\"\\n⏱️  Starting installation process at {time.strftime('%H:%M:%S')}\")\n",
        "start_total = time.time()\n",
        "\n",
        "install_requirements()\n",
        "\n",
        "total_time = time.time() - start_total\n",
        "print(f\"\\n⏱️  Total installation time: {total_time:.1f} seconds\")\n",
        "\n",
        "# Verify installation\n",
        "verification_success = verify_imports()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if verification_success:\n",
        "    print(\"🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"⚠️  Some issues detected. You may need to restart the kernel.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "🚀 Analysis Requirements Installation & Verification\n",
            "======================================================================\n",
            "🔍 Checking current package status...\n",
            "--------------------------------------------------\n",
            "✅ pandas          - Already installed\n",
            "✅ numpy           - Already installed\n",
            "❌ matplotlib      - Needs installation\n",
            "❌ seaborn         - Needs installation\n",
            "✅ scipy           - Already installed\n",
            "✅ scikit-learn    - Already installed\n",
            "❌ statsmodels     - Needs installation\n",
            "✅ tqdm            - Already installed\n",
            "✅ pyyaml          - Already installed\n",
            "\n",
            "📊 Status Summary:\n",
            "   Already installed: 6\n",
            "   Need installation: 3\n",
            "   Packages to install: matplotlib, seaborn, statsmodels\n",
            "\n",
            "⏱️  Starting installation process at 12:46:51\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "\n",
            "📦 Installing 14 packages from requirements file...\n",
            "============================================================\n",
            "\n",
            "[1/14] Installing pandas...\n",
            "   Full requirement: pandas>=1.5.0\n",
            "   ✅ pandas installed successfully (1.7s)\n",
            "\n",
            "[2/14] Installing numpy...\n",
            "   Full requirement: numpy>=1.21.0\n",
            "   ✅ numpy installed successfully (1.5s)\n",
            "\n",
            "[3/14] Installing matplotlib...\n",
            "   Full requirement: matplotlib>=3.5.0\n",
            "   ✅ matplotlib installed successfully (33.6s)\n",
            "\n",
            "[4/14] Installing seaborn...\n",
            "   Full requirement: seaborn>=0.11.0\n",
            "   ✅ seaborn installed successfully (6.9s)\n",
            "\n",
            "[5/14] Installing plotly...\n",
            "   Full requirement: plotly>=5.0.0\n",
            "   ⏰ Timeout installing plotly (>3 minutes)\n",
            "\n",
            "[6/14] Installing scipy...\n",
            "   Full requirement: scipy>=1.8.0\n",
            "   ✅ scipy installed successfully (2.1s)\n",
            "\n",
            "[7/14] Installing scikit-learn...\n",
            "   Full requirement: scikit-learn>=1.1.0\n",
            "   ✅ scikit-learn installed successfully (2.1s)\n",
            "\n",
            "[8/14] Installing statsmodels...\n",
            "   Full requirement: statsmodels>=0.13.0\n",
            "   ✅ statsmodels installed successfully (137.4s)\n",
            "\n",
            "[9/14] Installing pathlib2...\n",
            "   Full requirement: pathlib2>=2.3.0\n",
            "   ✅ pathlib2 installed successfully (2.2s)\n",
            "\n",
            "[10/14] Installing tqdm...\n",
            "   Full requirement: tqdm>=4.64.0\n",
            "   ✅ tqdm installed successfully (2.0s)\n",
            "\n",
            "[11/14] Installing pyyaml...\n",
            "   Full requirement: pyyaml>=6.0\n",
            "   ✅ pyyaml installed successfully (1.9s)\n",
            "\n",
            "[12/14] Installing ipywidgets...\n",
            "   Full requirement: ipywidgets>=7.6.0\n",
            "   ✅ ipywidgets installed successfully (2.0s)\n",
            "\n",
            "[13/14] Installing jupyter...\n",
            "   Full requirement: jupyter>=1.0.0\n",
            "   ✅ jupyter installed successfully (149.8s)\n",
            "\n",
            "[14/14] Installing psutil...\n",
            "   Full requirement: psutil>=5.8.0\n",
            "   ✅ psutil installed successfully (2.6s)\n",
            "\n",
            "============================================================\n",
            "📊 Installation Summary:\n",
            "   ✅ Successful: 13\n",
            "   ❌ Failed: 1\n",
            "\n",
            "   Successfully installed: pandas, numpy, matplotlib, seaborn, scipy\n",
            "   ... and 8 more\n",
            "\n",
            "   ⚠️  Failed packages: plotly\n",
            "   These will be retried with basic fallback...\n",
            "\n",
            "🔄 Installing 1 basic requirements...\n",
            "--------------------------------------------------\n",
            "[1/1] Installing plotly... ⏰ Timeout (>3 min)\n",
            "\n",
            "⏱️  Total installation time: 706.1 seconds\n",
            "\n",
            "🔍 Verifying library imports...\n",
            "----------------------------------------\n",
            "✅ pandas               - OK\n",
            "✅ numpy                - OK\n",
            "❌ matplotlib.pyplot    - FAILED: No module named 'matplotlib'...\n",
            "❌ seaborn              - FAILED: No module named 'seaborn'...\n",
            "✅ scipy                - OK\n",
            "✅ pathlib              - OK\n",
            "✅ json                 - OK\n",
            "✅ yaml                 - OK\n",
            "✅ sklearn              - OK\n",
            "❌ statsmodels.api      - FAILED: No module named 'statsmodels'...\n",
            "\n",
            "⚠️  Warning: 3 libraries failed to import\n",
            "   Failed libraries: matplotlib.pyplot, seaborn, statsmodels.api\n",
            "   You may need to restart the kernel after installation\n",
            "\n",
            "======================================================================\n",
            "⚠️  Some issues detected. You may need to restart the kernel.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback and detailed progress.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0',\n",
        "        'scikit-learn>=1.1.0',\n",
        "        'statsmodels>=0.13.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Read requirements file to show what will be installed\n",
        "            with open(requirements_file, 'r') as f:\n",
        "                requirements_content = f.read().strip().split('\\n')\n",
        "            \n",
        "            # Filter out comments and empty lines\n",
        "            requirements_list = [\n",
        "                req.strip() for req in requirements_content \n",
        "                if req.strip() and not req.strip().startswith('#')\n",
        "            ]\n",
        "            \n",
        "            print(f\"\\n📦 Installing {len(requirements_list)} packages from requirements file...\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Install each requirement individually with progress\n",
        "            failed_packages = []\n",
        "            successful_packages = []\n",
        "            \n",
        "            for i, requirement in enumerate(requirements_list, 1):\n",
        "                package_name = requirement.split('>=')[0].split('==')[0].split('[')[0]\n",
        "                print(f\"\\n[{i}/{len(requirements_list)}] Installing {package_name}...\")\n",
        "                print(f\"   Full requirement: {requirement}\")\n",
        "                \n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    result = subprocess.run([\n",
        "                        sys.executable, '-m', 'pip', 'install', requirement, '--timeout', '120'\n",
        "                    ], capture_output=True, text=True, timeout=180)  # 3 minute timeout per package\n",
        "                    \n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    \n",
        "                    if result.returncode == 0:\n",
        "                        print(f\"   ✅ {package_name} installed successfully ({elapsed_time:.1f}s)\")\n",
        "                        successful_packages.append(package_name)\n",
        "                    else:\n",
        "                        print(f\"   ❌ Failed to install {package_name}\")\n",
        "                        print(f\"   Error: {result.stderr[:200]}...\")\n",
        "                        failed_packages.append(package_name)\n",
        "                        \n",
        "                except subprocess.TimeoutExpired:\n",
        "                    print(f\"   ⏰ Timeout installing {package_name} (>3 minutes)\")\n",
        "                    failed_packages.append(package_name)\n",
        "                except Exception as e:\n",
        "                    print(f\"   ❌ Exception installing {package_name}: {e}\")\n",
        "                    failed_packages.append(package_name)\n",
        "            \n",
        "            # Summary\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(f\"📊 Installation Summary:\")\n",
        "            print(f\"   ✅ Successful: {len(successful_packages)}\")\n",
        "            print(f\"   ❌ Failed: {len(failed_packages)}\")\n",
        "            \n",
        "            if successful_packages:\n",
        "                print(f\"\\n   Successfully installed: {', '.join(successful_packages[:5])}\")\n",
        "                if len(successful_packages) > 5:\n",
        "                    print(f\"   ... and {len(successful_packages) - 5} more\")\n",
        "            \n",
        "            if failed_packages:\n",
        "                print(f\"\\n   ⚠️  Failed packages: {', '.join(failed_packages)}\")\n",
        "                print(\"   These will be retried with basic fallback...\")\n",
        "                install_basic_requirements([f\"{pkg}>=0.0.0\" for pkg in failed_packages])\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error reading requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually with progress feedback.\"\"\"\n",
        "    print(f\"\\n🔄 Installing {len(requirements_list)} basic requirements...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for i, req in enumerate(requirements_list, 1):\n",
        "        package_name = req.split('>=')[0].split('==')[0].split('[')[0]\n",
        "        print(f\"[{i}/{len(requirements_list)}] Installing {package_name}...\", end=\" \")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req, '--timeout', '120'\n",
        "            ], capture_output=True, text=True, timeout=180)\n",
        "            \n",
        "            elapsed_time = time.time() - start_time\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                print(f\"✅ ({elapsed_time:.1f}s)\")\n",
        "            else:\n",
        "                print(f\"❌ Failed\")\n",
        "                print(f\"    Error: {result.stderr[:100]}...\")\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"⏰ Timeout (>3 min)\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Exception: {str(e)[:50]}...\")\n",
        "\n",
        "def check_package_installed(package_name):\n",
        "    \"\"\"Check if a package is already installed.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'show', package_name\n",
        "        ], capture_output=True, text=True)\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml',\n",
        "        'sklearn': 'sklearn',\n",
        "        'statsmodels.api': 'sm'\n",
        "    }\n",
        "    \n",
        "    print(\"\\n🔍 Verifying library imports...\")\n",
        "    print(\"-\" * 40)\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✅ {lib:<20} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"❌ {lib:<20} - FAILED: {str(e)[:50]}...\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠️  Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"   Failed libraries:\", ', '.join(failed_imports))\n",
        "        print(\"   You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✅ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "def show_pre_installation_status():\n",
        "    \"\"\"Show which packages are already installed.\"\"\"\n",
        "    check_packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scipy', \n",
        "                     'scikit-learn', 'statsmodels', 'tqdm', 'pyyaml']\n",
        "    \n",
        "    print(\"🔍 Checking current package status...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    already_installed = []\n",
        "    need_installation = []\n",
        "    \n",
        "    for package in check_packages:\n",
        "        if check_package_installed(package):\n",
        "            print(f\"✅ {package:<15} - Already installed\")\n",
        "            already_installed.append(package)\n",
        "        else:\n",
        "            print(f\"❌ {package:<15} - Needs installation\")\n",
        "            need_installation.append(package)\n",
        "    \n",
        "    print(f\"\\n📊 Status Summary:\")\n",
        "    print(f\"   Already installed: {len(already_installed)}\")\n",
        "    print(f\"   Need installation: {len(need_installation)}\")\n",
        "    \n",
        "    if need_installation:\n",
        "        print(f\"   Packages to install: {', '.join(need_installation)}\")\n",
        "    \n",
        "    return already_installed, need_installation\n",
        "\n",
        "# Run installation and verification with progress tracking\n",
        "print(\"=\" * 70)\n",
        "print(\"🚀 Analysis Requirements Installation & Verification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show pre-installation status\n",
        "already_installed, need_installation = show_pre_installation_status()\n",
        "\n",
        "# Proceed with installation\n",
        "print(f\"\\n⏱️  Starting installation process at {time.strftime('%H:%M:%S')}\")\n",
        "start_total = time.time()\n",
        "\n",
        "install_requirements()\n",
        "\n",
        "total_time = time.time() - start_total\n",
        "print(f\"\\n⏱️  Total installation time: {total_time:.1f} seconds\")\n",
        "\n",
        "# Verify installation\n",
        "verification_success = verify_imports()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if verification_success:\n",
        "    print(\"🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"⚠️  Some issues detected. You may need to restart the kernel.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Analysis Requirements Installation & Verification ===\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "Installing requirements from file...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'pathlib2>=2.3.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Install from requirements file\n",
        "            print(\"Installing requirements from file...\")\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', '-r', str(requirements_file)\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(\"✓ Requirements installed successfully from file\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Error installing from requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually.\"\"\"\n",
        "    for req in requirements_list:\n",
        "        try:\n",
        "            print(f\"Installing {req}...\")\n",
        "            subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(f\"✓ {req} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Failed to install {req}: {e}\")\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml'\n",
        "    }\n",
        "    \n",
        "    print(\"\\nVerifying library imports...\")\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✓ {lib} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"✗ {lib} - FAILED: {e}\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠ Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✓ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "# Run installation and verification\n",
        "print(\"=== Analysis Requirements Installation & Verification ===\")\n",
        "install_requirements()\n",
        "verification_success = verify_imports()\n",
        "\n",
        "if verification_success:\n",
        "    print(\"\\n🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"\\n⚠ Some issues detected. You may need to restart the kernel.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Analysis Requirements Installation & Verification ===\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "Installing requirements from file...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'pathlib2>=2.3.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Install from requirements file\n",
        "            print(\"Installing requirements from file...\")\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', '-r', str(requirements_file)\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(\"✓ Requirements installed successfully from file\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Error installing from requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually.\"\"\"\n",
        "    for req in requirements_list:\n",
        "        try:\n",
        "            print(f\"Installing {req}...\")\n",
        "            subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(f\"✓ {req} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Failed to install {req}: {e}\")\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml'\n",
        "    }\n",
        "    \n",
        "    print(\"\\nVerifying library imports...\")\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✓ {lib} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"✗ {lib} - FAILED: {e}\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠ Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✓ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "# Run installation and verification\n",
        "print(\"=== Analysis Requirements Installation & Verification ===\")\n",
        "install_requirements()\n",
        "verification_success = verify_imports()\n",
        "\n",
        "if verification_success:\n",
        "    print(\"\\n🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"\\n⚠ Some issues detected. You may need to restart the kernel.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Analysis Requirements Installation & Verification ===\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "Installing requirements from file...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'pathlib2>=2.3.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Install from requirements file\n",
        "            print(\"Installing requirements from file...\")\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', '-r', str(requirements_file)\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(\"✓ Requirements installed successfully from file\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Error installing from requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually.\"\"\"\n",
        "    for req in requirements_list:\n",
        "        try:\n",
        "            print(f\"Installing {req}...\")\n",
        "            subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(f\"✓ {req} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Failed to install {req}: {e}\")\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml'\n",
        "    }\n",
        "    \n",
        "    print(\"\\nVerifying library imports...\")\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✓ {lib} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"✗ {lib} - FAILED: {e}\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠ Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✓ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "# Run installation and verification\n",
        "print(\"=== Analysis Requirements Installation & Verification ===\")\n",
        "install_requirements()\n",
        "verification_success = verify_imports()\n",
        "\n",
        "if verification_success:\n",
        "    print(\"\\n🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"\\n⚠ Some issues detected. You may need to restart the kernel.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Analysis Requirements Installation & Verification ===\n",
            "✓ Found requirements file: ..\\requirements\\requirements_analysis.txt\n",
            "Installing requirements from file...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install or verify analysis requirements with graceful fallback.\"\"\"\n",
        "    \n",
        "    # Basic requirements fallback\n",
        "    basic_requirements = [\n",
        "        'pandas>=1.5.0',\n",
        "        'numpy>=1.21.0', \n",
        "        'matplotlib>=3.5.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'scipy>=1.8.0',\n",
        "        'pathlib2>=2.3.0',\n",
        "        'tqdm>=4.64.0',\n",
        "        'pyyaml>=6.0'\n",
        "    ]\n",
        "    \n",
        "    # Try to find requirements_analysis.txt\n",
        "    requirements_paths = [\n",
        "        Path('./requirements/requirements_analysis.txt'),\n",
        "        Path('../requirements/requirements_analysis.txt'),\n",
        "        Path('../../requirements/requirements_analysis.txt'),\n",
        "        Path('./Deliverables-Code/requirements/requirements_analysis.txt')\n",
        "    ]\n",
        "    \n",
        "    requirements_file = None\n",
        "    for path in requirements_paths:\n",
        "        if path.exists():\n",
        "            requirements_file = path\n",
        "            print(f\"✓ Found requirements file: {path}\")\n",
        "            break\n",
        "    \n",
        "    if requirements_file:\n",
        "        try:\n",
        "            # Install from requirements file\n",
        "            print(\"Installing requirements from file...\")\n",
        "            result = subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', '-r', str(requirements_file)\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(\"✓ Requirements installed successfully from file\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Error installing from requirements file: {e}\")\n",
        "            print(\"Falling back to basic requirements...\")\n",
        "            install_basic_requirements(basic_requirements)\n",
        "            \n",
        "    else:\n",
        "        print(\"⚠ requirements_analysis.txt not found\")\n",
        "        print(\"Installing basic requirements for analysis...\")\n",
        "        install_basic_requirements(basic_requirements)\n",
        "\n",
        "def install_basic_requirements(requirements_list):\n",
        "    \"\"\"Install basic requirements individually.\"\"\"\n",
        "    for req in requirements_list:\n",
        "        try:\n",
        "            print(f\"Installing {req}...\")\n",
        "            subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', req\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            print(f\"✓ {req} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠ Failed to install {req}: {e}\")\n",
        "\n",
        "def verify_imports():\n",
        "    \"\"\"Verify that key libraries can be imported.\"\"\"\n",
        "    required_libraries = {\n",
        "        'pandas': 'pd',\n",
        "        'numpy': 'np', \n",
        "        'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns',\n",
        "        'scipy': 'scipy',\n",
        "        'pathlib': 'pathlib',\n",
        "        'json': 'json',\n",
        "        'yaml': 'yaml'\n",
        "    }\n",
        "    \n",
        "    print(\"\\nVerifying library imports...\")\n",
        "    failed_imports = []\n",
        "    \n",
        "    for lib, alias in required_libraries.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"✓ {lib} - OK\")\n",
        "        except ImportError as e:\n",
        "            print(f\"✗ {lib} - FAILED: {e}\")\n",
        "            failed_imports.append(lib)\n",
        "    \n",
        "    if failed_imports:\n",
        "        print(f\"\\n⚠ Warning: {len(failed_imports)} libraries failed to import\")\n",
        "        print(\"You may need to restart the kernel after installation\")\n",
        "    else:\n",
        "        print(\"\\n✓ All required libraries verified successfully\")\n",
        "    \n",
        "    return len(failed_imports) == 0\n",
        "\n",
        "# Run installation and verification\n",
        "print(\"=== Analysis Requirements Installation & Verification ===\")\n",
        "install_requirements()\n",
        "verification_success = verify_imports()\n",
        "\n",
        "if verification_success:\n",
        "    print(\"\\n🎉 Setup complete! Ready to proceed with analysis.\")\n",
        "else:\n",
        "    print(\"\\n⚠ Some issues detected. You may need to restart the kernel.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Root Directory Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_project_root():\n",
        "    \"\"\"\n",
        "    Find project root by locating directory containing .gitignore and .gitattributes.\n",
        "    Similar to implementation in 03_pixtral_model.py\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import sys\n",
        "    \n",
        "    try:\n",
        "        # When running as a script, start from script location\n",
        "        start_path = Path(__file__).parent\n",
        "    except NameError:\n",
        "        # When running in a notebook, start from current working directory\n",
        "        start_path = Path.cwd()\n",
        "    \n",
        "    # Walk up the directory tree to find git markers\n",
        "    current_path = start_path\n",
        "    while current_path != current_path.parent:  # Stop at filesystem root\n",
        "        if (current_path / \".gitignore\").exists() and (current_path / \".gitattributes\").exists():\n",
        "            return current_path\n",
        "        current_path = current_path.parent\n",
        "    \n",
        "    raise RuntimeError(\"Could not find project root (directory containing .gitignore and .gitattributes)\")\n",
        "\n",
        "def setup_project_paths():\n",
        "    \"\"\"Set up all project directory paths and verify they exist.\"\"\"\n",
        "    global ROOT_DIR, DELIVERABLES_DIR, DATA_DIR, RESULTS_DIR, ANALYSIS_DIR, CONFIG_DIR\n",
        "    \n",
        "    # Find and set root directory\n",
        "    ROOT_DIR = find_project_root()\n",
        "    print(f\"✓ Found project root: {ROOT_DIR}\")\n",
        "    \n",
        "    # Set up key directories\n",
        "    DELIVERABLES_DIR = ROOT_DIR / \"Deliverables-Code\"\n",
        "    DATA_DIR = DELIVERABLES_DIR / \"data\"\n",
        "    RESULTS_DIR = DELIVERABLES_DIR / \"results\"\n",
        "    ANALYSIS_DIR = DELIVERABLES_DIR / \"analysis\"\n",
        "    CONFIG_DIR = DELIVERABLES_DIR / \"config\"\n",
        "    \n",
        "    # Verify expected directories exist\n",
        "    required_dirs = {\n",
        "        \"Deliverables-Code\": DELIVERABLES_DIR,\n",
        "        \"data\": DATA_DIR,\n",
        "        \"results\": RESULTS_DIR,\n",
        "        \"analysis\": ANALYSIS_DIR,\n",
        "        \"config\": CONFIG_DIR\n",
        "    }\n",
        "    \n",
        "    missing_dirs = []\n",
        "    for name, path in required_dirs.items():\n",
        "        if path.exists():\n",
        "            print(f\"✓ Found {name} directory: {path}\")\n",
        "        else:\n",
        "            print(f\"⚠ Missing {name} directory: {path}\")\n",
        "            missing_dirs.append(name)\n",
        "    \n",
        "    if missing_dirs:\n",
        "        print(f\"\\n⚠ Warning: {len(missing_dirs)} required directories not found\")\n",
        "        print(\"This may indicate the notebook is being run from an unexpected location\")\n",
        "    else:\n",
        "        print(\"\\n✓ All project directories located successfully\")\n",
        "    \n",
        "    # Create analysis directory if it doesn't exist\n",
        "    ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Add project root to Python path for imports\n",
        "    import sys\n",
        "    if str(ROOT_DIR) not in sys.path:\n",
        "        sys.path.append(str(ROOT_DIR))\n",
        "        print(f\"✓ Added project root to Python path\")\n",
        "    \n",
        "    return ROOT_DIR\n",
        "\n",
        "def display_project_structure():\n",
        "    \"\"\"Display relevant project structure for reference.\"\"\"\n",
        "    print(\"\\n=== Project Structure (Key Directories) ===\")\n",
        "    print(f\"ROOT_DIR:         {ROOT_DIR}\")\n",
        "    print(f\"DELIVERABLES_DIR: {DELIVERABLES_DIR}\")\n",
        "    print(f\"DATA_DIR:         {DATA_DIR}\")\n",
        "    print(f\"RESULTS_DIR:      {RESULTS_DIR}\")\n",
        "    print(f\"ANALYSIS_DIR:     {ANALYSIS_DIR}\")\n",
        "    print(f\"CONFIG_DIR:       {CONFIG_DIR}\")\n",
        "    \n",
        "    # Show counts of files in key directories\n",
        "    if RESULTS_DIR.exists():\n",
        "        result_files = list(RESULTS_DIR.glob(\"*.json\"))\n",
        "        print(f\"\\nResult files found: {len(result_files)}\")\n",
        "        \n",
        "    if ANALYSIS_DIR.exists():\n",
        "        analysis_files = list(ANALYSIS_DIR.glob(\"*.json\"))\n",
        "        print(f\"Analysis files found: {len(analysis_files)}\")\n",
        "        \n",
        "    if (DATA_DIR / \"images\" / \"metadata\").exists():\n",
        "        metadata_files = list((DATA_DIR / \"images\" / \"metadata\").glob(\"*.csv\"))\n",
        "        print(f\"Metadata files found: {len(metadata_files)}\")\n",
        "\n",
        "# Run root directory detection and path setup\n",
        "print(\"=== Root Directory Detection & Path Setup ===\")\n",
        "project_root = setup_project_paths()\n",
        "display_project_structure()\n",
        "\n",
        "print(f\"\\n🎯 Ready to proceed with analysis from: {ROOT_DIR.name}\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import standard libraries for data analysis and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind, mannwhitneyu, kruskal\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import itertools\n",
        "\n",
        "# Statistical and machine learning utilities\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure plotting parameters and styles\n",
        "plt.style.use('default')  # Start with clean default style\n",
        "\n",
        "# Set up matplotlib and seaborn styling\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': (12, 8),\n",
        "    'figure.dpi': 100,\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'legend.fontsize': 10,\n",
        "    'legend.title_fontsize': 11,\n",
        "    'axes.grid': True,\n",
        "    'grid.alpha': 0.3,\n",
        "    'lines.linewidth': 2,\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'figure.facecolor': 'white',\n",
        "    'axes.facecolor': 'white'\n",
        "})\n",
        "\n",
        "# Set seaborn style and palette\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Custom color palette for consistent visualization\n",
        "ANALYSIS_COLORS = {\n",
        "    'LMM': '#2E86AB',        # Blue for LMM models\n",
        "    'OCR': '#A23B72',        # Purple for OCR models\n",
        "    'Pixtral': '#2E86AB',    # Blue for Pixtral\n",
        "    'Llama': '#00A6D6',      # Light blue for Llama\n",
        "    'DocTR': '#A23B72',      # Purple for DocTR\n",
        "    'accuracy': '#28A745',    # Green for accuracy metrics\n",
        "    'cer': '#DC3545',        # Red for error metrics\n",
        "    'work_order': '#FD7E14',  # Orange for work order\n",
        "    'total_cost': '#6F42C1',  # Purple for total cost\n",
        "    'baseline': '#6C757D',    # Gray for baseline/reference\n",
        "    'improvement': '#20C997'   # Teal for improvements\n",
        "}\n",
        "\n",
        "# Configure warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Display configuration\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n",
        "print(\"✓ Plotting parameters configured\")\n",
        "print(\"✓ Custom color palette defined\")\n",
        "print(\"✓ Analysis environment ready\")\n",
        "\n",
        "# Show available color palette\n",
        "print(f\"\\n📊 Available analysis colors: {list(ANALYSIS_COLORS.keys())}\")\n",
        "print(\"🎨 Visualization settings optimized for analysis reports\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Logging Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "def setup_analysis_logging():\n",
        "    \"\"\"Configure logging for the analysis process with multiple output destinations.\"\"\"\n",
        "    \n",
        "    # Create logs directory if it doesn't exist\n",
        "    logs_dir = ANALYSIS_DIR / \"logs\"\n",
        "    logs_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Generate timestamped log filename\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_filename = logs_dir / f\"analysis_{timestamp}.log\"\n",
        "    \n",
        "    # Clear any existing handlers\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    \n",
        "    # Configure root logger\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    \n",
        "    # Create file handler for detailed logging\n",
        "    file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "    file_formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'\n",
        "    )\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "    \n",
        "    # Create console handler for important messages\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "    console_formatter = logging.Formatter('%(levelname)s: %(message)s')\n",
        "    console_handler.setFormatter(console_formatter)\n",
        "    \n",
        "    # Add handlers to root logger\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "    \n",
        "    return logger, log_filename\n",
        "\n",
        "def create_section_logger(section_name: str):\n",
        "    \"\"\"Create a logger for a specific analysis section.\"\"\"\n",
        "    return logging.getLogger(f\"analysis.{section_name}\")\n",
        "\n",
        "# Set up logging system\n",
        "analysis_logger, log_file_path = setup_analysis_logging()\n",
        "\n",
        "# Create loggers for different analysis sections\n",
        "setup_logger = create_section_logger(\"setup\")\n",
        "data_logger = create_section_logger(\"data_loading\")\n",
        "viz_logger = create_section_logger(\"visualization\")\n",
        "stats_logger = create_section_logger(\"statistics\")\n",
        "results_logger = create_section_logger(\"results\")\n",
        "\n",
        "# Log initial setup\n",
        "setup_logger.info(\"=== Analysis Framework v2.0 - Session Started ===\")\n",
        "setup_logger.info(f\"Log file: {log_file_path}\")\n",
        "setup_logger.info(f\"Project root: {ROOT_DIR}\")\n",
        "setup_logger.info(f\"Analysis directory: {ANALYSIS_DIR}\")\n",
        "\n",
        "# Configure progress tracking\n",
        "def log_progress(message: str, level: str = \"info\"):\n",
        "    \"\"\"Utility function to log progress with timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    formatted_message = f\"[{timestamp}] {message}\"\n",
        "    \n",
        "    if level.lower() == \"info\":\n",
        "        analysis_logger.info(formatted_message)\n",
        "        print(f\"ℹ️  {formatted_message}\")\n",
        "    elif level.lower() == \"warning\":\n",
        "        analysis_logger.warning(formatted_message)\n",
        "        print(f\"⚠️  {formatted_message}\")\n",
        "    elif level.lower() == \"error\":\n",
        "        analysis_logger.error(formatted_message)\n",
        "        print(f\"❌ {formatted_message}\")\n",
        "    elif level.lower() == \"success\":\n",
        "        analysis_logger.info(formatted_message)\n",
        "        print(f\"✅ {formatted_message}\")\n",
        "\n",
        "# Test logging system\n",
        "setup_logger.info(\"Logging system configured successfully\")\n",
        "data_logger.debug(\"Data logging ready\")\n",
        "viz_logger.debug(\"Visualization logging ready\")\n",
        "stats_logger.debug(\"Statistics logging ready\")\n",
        "results_logger.debug(\"Results logging ready\")\n",
        "\n",
        "print(\"✓ Logging system configured\")\n",
        "print(f\"✓ Log file created: {log_file_path.name}\")\n",
        "print(\"✓ Section loggers initialized\")\n",
        "print(\"✓ Progress tracking ready\")\n",
        "\n",
        "# Display logging configuration\n",
        "print(f\"\\n📝 Logging Details:\")\n",
        "print(f\"   • File log level: DEBUG (detailed)\")\n",
        "print(f\"   • Console log level: INFO (summary)\")\n",
        "print(f\"   • Log location: {logs_dir}\")\n",
        "print(f\"   • Current session: {log_file_path.name}\")\n",
        "\n",
        "log_progress(\"Analysis logging system ready\", \"info\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ground_truth_data(ground_truth_file: str = None) -> pd.DataFrame:\n",
        "    \"\"\"Load and validate ground truth CSV data.\"\"\"\n",
        "    data_logger.info(\"Loading ground truth data\")\n",
        "    \n",
        "    # Set default ground truth file path using ROOT_DIR\n",
        "    if ground_truth_file is None:\n",
        "        ground_truth_file = DATA_DIR / \"images\" / \"metadata\" / \"ground_truth.csv\"\n",
        "    else:\n",
        "        ground_truth_file = Path(ground_truth_file)\n",
        "    \n",
        "    if not ground_truth_file.exists():\n",
        "        raise FileNotFoundError(f\"Ground truth file not found: {ground_truth_file}\")\n",
        "    \n",
        "    try:\n",
        "        # Load with explicit string type for filename column to ensure consistent matching\n",
        "        ground_truth = pd.read_csv(ground_truth_file, dtype={'filename': str})\n",
        "        \n",
        "        # Validate required columns\n",
        "        required_columns = {'filename', 'work_order_number', 'total'}\n",
        "        missing_columns = required_columns - set(ground_truth.columns)\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing required columns in ground truth: {missing_columns}\")\n",
        "        \n",
        "        # Clean and validate data\n",
        "        ground_truth['filename'] = ground_truth['filename'].str.strip()\n",
        "        ground_truth['work_order_number'] = ground_truth['work_order_number'].astype(str).str.strip()\n",
        "        \n",
        "        data_logger.info(f\"Loaded ground truth data: {len(ground_truth)} records\")\n",
        "        return ground_truth\n",
        "        \n",
        "    except Exception as e:\n",
        "        data_logger.error(f\"Error loading ground truth data: {e}\")\n",
        "        raise\n",
        "\n",
        "def discover_results_files() -> Dict[str, List[Path]]:\n",
        "    \"\"\"Discover all results files organized by model type.\"\"\"\n",
        "    data_logger.info(\"Discovering results files\")\n",
        "    \n",
        "    results_files = {\n",
        "        'pixtral': [],\n",
        "        'llama': [],\n",
        "        'doctr': [],\n",
        "        'all': []\n",
        "    }\n",
        "    \n",
        "    # Get all results JSON files\n",
        "    all_files = list(RESULTS_DIR.glob(\"results-*.json\"))\n",
        "    \n",
        "    for file in all_files:\n",
        "        results_files['all'].append(file)\n",
        "        \n",
        "        # Categorize by model type based on filename pattern\n",
        "        if 'pixtral' in file.name:\n",
        "            results_files['pixtral'].append(file)\n",
        "        elif 'llama' in file.name:\n",
        "            results_files['llama'].append(file)\n",
        "        elif 'doctr' in file.name:\n",
        "            results_files['doctr'].append(file)\n",
        "    \n",
        "    # Sort files by modification time (newest first)\n",
        "    for model_type in results_files:\n",
        "        results_files[model_type].sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    \n",
        "    data_logger.info(f\"Found {len(results_files['all'])} total results files\")\n",
        "    for model_type, files in results_files.items():\n",
        "        if model_type != 'all' and files:\n",
        "            data_logger.info(f\"  {model_type}: {len(files)} files\")\n",
        "    \n",
        "    return results_files\n",
        "\n",
        "def discover_analysis_files() -> Dict[str, List[Path]]:\n",
        "    \"\"\"Discover all analysis files organized by model type.\"\"\"\n",
        "    data_logger.info(\"Discovering analysis files\")\n",
        "    \n",
        "    analysis_files = {\n",
        "        'pixtral': [],\n",
        "        'llama': [],\n",
        "        'doctr': [],\n",
        "        'all': []\n",
        "    }\n",
        "    \n",
        "    # Get all analysis JSON files\n",
        "    all_files = list(ANALYSIS_DIR.glob(\"analysis-*.json\"))\n",
        "    \n",
        "    for file in all_files:\n",
        "        analysis_files['all'].append(file)\n",
        "        \n",
        "        # Categorize by model type based on filename pattern\n",
        "        if 'pixtral' in file.name:\n",
        "            analysis_files['pixtral'].append(file)\n",
        "        elif 'llama' in file.name:\n",
        "            analysis_files['llama'].append(file)\n",
        "        elif 'doctr' in file.name:\n",
        "            analysis_files['doctr'].append(file)\n",
        "    \n",
        "    # Sort files by modification time (newest first)\n",
        "    for model_type in analysis_files:\n",
        "        analysis_files[model_type].sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    \n",
        "    data_logger.info(f\"Found {len(analysis_files['all'])} total analysis files\")\n",
        "    for model_type, files in analysis_files.items():\n",
        "        if model_type != 'all' and files:\n",
        "            data_logger.info(f\"  {model_type}: {len(files)} files\")\n",
        "    \n",
        "    return analysis_files\n",
        "\n",
        "def load_results_file(file_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Load and validate a results JSON file.\"\"\"\n",
        "    data_logger.debug(f\"Loading results file: {file_path.name}\")\n",
        "    \n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Validate structure\n",
        "        required_keys = {'metadata', 'results'}\n",
        "        missing_keys = required_keys - set(data.keys())\n",
        "        if missing_keys:\n",
        "            raise ValueError(f\"Missing required keys in results file: {missing_keys}\")\n",
        "        \n",
        "        # Add file metadata\n",
        "        data['file_info'] = {\n",
        "            'filename': file_path.name,\n",
        "            'file_path': str(file_path),\n",
        "            'file_size_mb': round(file_path.stat().st_size / (1024*1024), 2),\n",
        "            'modification_time': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()\n",
        "        }\n",
        "        \n",
        "        data_logger.debug(f\"Loaded results file with {len(data['results'])} results\")\n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        data_logger.error(f\"Error loading results file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_analysis_file(file_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Load and validate an analysis JSON file.\"\"\"\n",
        "    data_logger.debug(f\"Loading analysis file: {file_path.name}\")\n",
        "    \n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Validate structure\n",
        "        required_keys = {'metadata', 'summary', 'extracted_data'}\n",
        "        missing_keys = required_keys - set(data.keys())\n",
        "        if missing_keys:\n",
        "            raise ValueError(f\"Missing required keys in analysis file: {missing_keys}\")\n",
        "        \n",
        "        # Add file metadata\n",
        "        data['file_info'] = {\n",
        "            'filename': file_path.name,\n",
        "            'file_path': str(file_path),\n",
        "            'file_size_mb': round(file_path.stat().st_size / (1024*1024), 2),\n",
        "            'modification_time': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()\n",
        "        }\n",
        "        \n",
        "        data_logger.debug(f\"Loaded analysis file with {len(data['extracted_data'])} analyzed results\")\n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        data_logger.error(f\"Error loading analysis file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_all_results(model_types: List[str] = None) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"Load all results files for specified model types.\"\"\"\n",
        "    data_logger.info(\"Loading all results files\")\n",
        "    \n",
        "    if model_types is None:\n",
        "        model_types = ['pixtral', 'llama', 'doctr']\n",
        "    \n",
        "    results_files = discover_results_files()\n",
        "    all_results = {}\n",
        "    \n",
        "    for model_type in model_types:\n",
        "        if model_type in results_files:\n",
        "            all_results[model_type] = []\n",
        "            for file_path in results_files[model_type]:\n",
        "                try:\n",
        "                    result_data = load_results_file(file_path)\n",
        "                    all_results[model_type].append(result_data)\n",
        "                except Exception as e:\n",
        "                    data_logger.warning(f\"Skipping corrupted results file {file_path}: {e}\")\n",
        "    \n",
        "    total_loaded = sum(len(results) for results in all_results.values())\n",
        "    data_logger.info(f\"Loaded {total_loaded} results files across {len(all_results)} model types\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def load_all_analysis(model_types: List[str] = None) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"Load all analysis files for specified model types.\"\"\"\n",
        "    data_logger.info(\"Loading all analysis files\")\n",
        "    \n",
        "    if model_types is None:\n",
        "        model_types = ['pixtral', 'llama', 'doctr']\n",
        "    \n",
        "    analysis_files = discover_analysis_files()\n",
        "    all_analysis = {}\n",
        "    \n",
        "    for model_type in model_types:\n",
        "        if model_type in analysis_files:\n",
        "            all_analysis[model_type] = []\n",
        "            for file_path in analysis_files[model_type]:\n",
        "                try:\n",
        "                    analysis_data = load_analysis_file(file_path)\n",
        "                    all_analysis[model_type].append(analysis_data)\n",
        "                except Exception as e:\n",
        "                    data_logger.warning(f\"Skipping corrupted analysis file {file_path}: {e}\")\n",
        "    \n",
        "    total_loaded = sum(len(analyses) for analyses in all_analysis.values())\n",
        "    data_logger.info(f\"Loaded {total_loaded} analysis files across {len(all_analysis)} model types\")\n",
        "    \n",
        "    return all_analysis\n",
        "\n",
        "def select_files_interactive(file_type: str = \"results\") -> List[Path]:\n",
        "    \"\"\"Interactive file selection for analysis.\"\"\"\n",
        "    if file_type == \"results\":\n",
        "        files_dict = discover_results_files()\n",
        "        title = \"Available Results Files\"\n",
        "    elif file_type == \"analysis\":\n",
        "        files_dict = discover_analysis_files()\n",
        "        title = \"Available Analysis Files\"\n",
        "    else:\n",
        "        raise ValueError(\"file_type must be 'results' or 'analysis'\")\n",
        "    \n",
        "    all_files = files_dict['all']\n",
        "    if not all_files:\n",
        "        print(f\"No {file_type} files found.\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"\\n{title}:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, file_path in enumerate(all_files, 1):\n",
        "        # Extract model info from filename\n",
        "        model_info = \"\"\n",
        "        if 'pixtral' in file_path.name:\n",
        "            model_info = \" [Pixtral]\"\n",
        "        elif 'llama' in file_path.name:\n",
        "            model_info = \" [Llama]\"\n",
        "        elif 'doctr' in file_path.name:\n",
        "            model_info = \" [DocTR]\"\n",
        "        \n",
        "        # Get file modification time\n",
        "        mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
        "        print(f\"{i:2d}. {file_path.name}{model_info}\")\n",
        "        print(f\"     Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    print(f\"\\n{len(all_files) + 1}. Load all files\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(f\"\\nSelect files (comma-separated numbers, or {len(all_files) + 1} for all): \")\n",
        "            \n",
        "            if choice.strip() == str(len(all_files) + 1):\n",
        "                return all_files\n",
        "            \n",
        "            # Parse comma-separated choices\n",
        "            choices = [int(x.strip()) for x in choice.split(',')]\n",
        "            selected_files = []\n",
        "            \n",
        "            for choice_num in choices:\n",
        "                if 1 <= choice_num <= len(all_files):\n",
        "                    selected_files.append(all_files[choice_num - 1])\n",
        "                else:\n",
        "                    print(f\"Invalid choice: {choice_num}\")\n",
        "                    continue\n",
        "            \n",
        "            if selected_files:\n",
        "                print(f\"\\nSelected {len(selected_files)} file(s):\")\n",
        "                for file_path in selected_files:\n",
        "                    print(f\"  - {file_path.name}\")\n",
        "                return selected_files\n",
        "            else:\n",
        "                print(\"No valid files selected.\")\n",
        "                \n",
        "        except ValueError:\n",
        "            print(\"Please enter valid numbers separated by commas.\")\n",
        "\n",
        "def create_comprehensive_dataset() -> Dict[str, Any]:\n",
        "    \"\"\"Create a comprehensive dataset combining all available data.\"\"\"\n",
        "    data_logger.info(\"Creating comprehensive dataset\")\n",
        "    \n",
        "    # Load ground truth\n",
        "    ground_truth = load_ground_truth_data()\n",
        "    \n",
        "    # Load all analysis files (which contain the processed results)\n",
        "    all_analysis = load_all_analysis()\n",
        "    \n",
        "    # Create comprehensive dataset structure\n",
        "    dataset = {\n",
        "        'ground_truth': ground_truth,\n",
        "        'model_data': {},\n",
        "        'metadata': {\n",
        "            'created_timestamp': datetime.now().isoformat(),\n",
        "            'total_models': 0,\n",
        "            'total_experiments': 0,\n",
        "            'data_sources': {\n",
        "                'ground_truth_file': str(DATA_DIR / \"images\" / \"metadata\" / \"ground_truth.csv\"),\n",
        "                'results_directory': str(RESULTS_DIR),\n",
        "                'analysis_directory': str(ANALYSIS_DIR)\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    total_experiments = 0\n",
        "    for model_type, analyses in all_analysis.items():\n",
        "        if analyses:\n",
        "            dataset['model_data'][model_type] = analyses\n",
        "            total_experiments += len(analyses)\n",
        "            data_logger.info(f\"Added {len(analyses)} experiments for {model_type}\")\n",
        "    \n",
        "    dataset['metadata']['total_models'] = len(dataset['model_data'])\n",
        "    dataset['metadata']['total_experiments'] = total_experiments\n",
        "    \n",
        "    data_logger.info(f\"Comprehensive dataset created with {dataset['metadata']['total_models']} models and {total_experiments} experiments\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Initialize data loading and create comprehensive dataset\n",
        "log_progress(\"Initializing data loading functions\", \"info\")\n",
        "\n",
        "# Verify data directories exist\n",
        "required_dirs = [RESULTS_DIR, ANALYSIS_DIR, DATA_DIR / \"images\" / \"metadata\"]\n",
        "for dir_path in required_dirs:\n",
        "    if not dir_path.exists():\n",
        "        log_progress(f\"Creating missing directory: {dir_path}\", \"warning\")\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Discover available data files\n",
        "available_results = discover_results_files()\n",
        "available_analysis = discover_analysis_files()\n",
        "\n",
        "# Load ground truth data\n",
        "try:\n",
        "    GROUND_TRUTH_DATA = load_ground_truth_data()\n",
        "    log_progress(f\"Ground truth loaded: {len(GROUND_TRUTH_DATA)} records\", \"success\")\n",
        "except Exception as e:\n",
        "    log_progress(f\"Warning: Could not load ground truth data: {e}\", \"warning\")\n",
        "    GROUND_TRUTH_DATA = None\n",
        "\n",
        "# Create comprehensive dataset for analysis\n",
        "try:\n",
        "    COMPREHENSIVE_DATASET = create_comprehensive_dataset()\n",
        "    log_progress(\"Comprehensive dataset created successfully\", \"success\")\n",
        "except Exception as e:\n",
        "    log_progress(f\"Warning: Could not create comprehensive dataset: {e}\", \"warning\")\n",
        "    COMPREHENSIVE_DATASET = None\n",
        "\n",
        "# Display summary of available data\n",
        "print(\"\\n📊 Data Loading Summary:\")\n",
        "print(f\"   • Ground truth records: {len(GROUND_TRUTH_DATA) if GROUND_TRUTH_DATA is not None else 'Not available'}\")\n",
        "print(f\"   • Results files found: {len(available_results['all'])}\")\n",
        "print(f\"   • Analysis files found: {len(available_analysis['all'])}\")\n",
        "\n",
        "if available_results['all']:\n",
        "    print(\"\\n   Results by model type:\")\n",
        "    for model_type, files in available_results.items():\n",
        "        if model_type != 'all' and files:\n",
        "            print(f\"     - {model_type.title()}: {len(files)} files\")\n",
        "\n",
        "if available_analysis['all']:\n",
        "    print(\"\\n   Analysis by model type:\")\n",
        "    for model_type, files in available_analysis.items():\n",
        "        if model_type != 'all' and files:\n",
        "            print(f\"     - {model_type.title()}: {len(files)} files\")\n",
        "\n",
        "print(\"\\n✅ Data loading functions ready for analysis\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 1: Executive Summary\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Experimental Design & Controlled Variables\n",
        "\n",
        "*Placeholder for discussion of controlled experimental design, image quality control, content standardization, and design rationale.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 1.1: Project Context & Key Findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Primary Performance Comparison Bar Chart\n",
        "# Side-by-side comparison of total accuracy for all LMM trials vs all OCR trials\n",
        "# Roll up across all prompts and queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model Type Breakdown Bar Chart\n",
        "# Break down into model types within each category\n",
        "# (LMM-Pixtral, LMM-Llama, OCR with all 7 recognition models)\n",
        "# Group by category and order by performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Complete Model Performance Bar Chart\n",
        "# All models organized by performance, color coded by category (LMM vs OCR only)\n",
        "# Include 85% accuracy reference line for industry automation standards\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for key findings discussion and business case establishment.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 2: Cross-Model Performance Comparison\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 2.1: Comprehensive Model Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Models vs Prompts Heatmap (Accuracy)\n",
        "# Pixtral/Llama (rows) × Prompt types (columns) with accuracy values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Models vs Prompts Heatmap (CER)\n",
        "# Pixtral/Llama (rows) × Prompt types (columns) with CER values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Prompts vs Query Heatmap (Accuracy)\n",
        "# Prompt types (rows) × Query types (Work Order/Total Cost) with accuracy values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LMM Prompts vs Query Heatmap (CER)\n",
        "# Prompt types (rows) × Query types (Work Order/Total Cost) with CER values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create All Models vs Query Heatmap (Accuracy)\n",
        "# All models including OCR (rows) × Query types (columns) with accuracy values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create All Models vs Query Heatmap (CER)\n",
        "# All models including OCR (rows) × Query types (columns) with CER values\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for analysis of LMM model responses to different prompt strategies, optimal prompt-model combinations, and CER pattern relationships.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 2.2: Model Consistency Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Coefficient of Variation Bar Chart\n",
        "# Performance stability across prompts for each model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Min-Max Range Visualization\n",
        "# Performance ranges to identify most/least consistent models\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for evaluation of performance stability across different conditions.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 3: Error Pattern Taxonomy & System Improvement Insights\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 3.1: Systematic Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Error Pattern Examples visualization\n",
        "# Visual examples of each error category with actual vs. expected results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Post-Processing Opportunity Assessment\n",
        "# Estimate potential accuracy improvements for each error type\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for identification of patterns that could be addressed through post-processing.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 3.2: Error Classification System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Error Type Distribution Pie Charts\n",
        "# Separate charts for Work Order vs. Total Cost errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Error Frequency Heatmap\n",
        "# Error types (rows) × Models (columns)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for categorization and quantification of different types of failures.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 3.3: Failure Mode Deep Dive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Failure Severity Distribution\n",
        "# Histogram of error magnitudes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model Robustness Comparison\n",
        "# How models handle edge cases\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for understanding catastrophic vs. graceful degradation patterns.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 4: Prompt Engineering Effectiveness Analysis\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 4.1: Prompt Strategy Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Prompt Performance Matrix\n",
        "# Accuracy gains/losses by prompt type across models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Prompt-Model Interaction Effects\n",
        "# Line graphs showing how each model responds to different prompts\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for quantifying effectiveness of different prompting approaches.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 5: Field-Specific Performance Deep Dive\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 5.1: Work Order vs. Total Cost Performance Differential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Field Performance Comparison\n",
        "# Side-by-side accuracy for each field across all models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Performance Gap Analysis\n",
        "# Difference between Total Cost and Work Order accuracy by model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for understanding why models excel at one field but struggle with another.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 6: Character Error Rate (CER) Deep Analysis\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 6.1: CER Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create CER Distribution Histograms\n",
        "# Separate for Work Order and Total Cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Model CER Comparison Box Plots\n",
        "# Show ranges and outliers\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for understanding the spread and clustering of character-level errors.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 7: Computational Efficiency Analysis\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 7.1: Performance per Resource Unit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Efficiency Frontier Plot\n",
        "# Accuracy vs. computational cost scatter plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Cost-Benefit Analysis\n",
        "# ROI calculations for different model choices\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for comparing accuracy gains vs. computational cost increases.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 8: Statistical Overview & Significance Testing\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 8.1: Statistical Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Performance Distribution Box Plots\n",
        "# Accuracy ranges across all model/prompt combinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Statistical Significance Matrix\n",
        "# P-values for key comparisons\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for high-level statistical summary of all results.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 9: Synthesis & Key Insights\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 9.1: Model Selection Decision Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Multi-Criteria Decision Matrix\n",
        "# Weighted scoring across accuracy, speed, cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Use Case Recommendations\n",
        "# Different models for different deployment scenarios\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for providing clear guidance for model choice based on different criteria.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 9.2: System Improvement Roadmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Improvement Opportunity Matrix\n",
        "# Effort vs. Impact for different enhancement areas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Implementation Timeline\n",
        "# Suggested sequence for system improvements\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for prioritizing enhancement opportunities based on analysis findings.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Cell 9.3: Unexpected Findings & Future Research\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Unexpected Findings Highlight\n",
        "# Key discoveries and their implications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Future Research Opportunities\n",
        "# Areas identified for continued investigation\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "*Placeholder for highlighting discoveries not anticipated in initial research design.*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

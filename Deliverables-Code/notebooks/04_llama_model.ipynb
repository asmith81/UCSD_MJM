{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a334c484",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Llama Vision Model Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the Llama-3.2-11B-Vision model's performance on invoice data extraction.\n",
    "It follows the project's notebook handling rules and functional programming approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tqdm first if not present\n",
    "import subprocess\n",
    "import sys\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tqdm\"])\n",
    "    import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f79993",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup and Configuration\n",
    "### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c5fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from PIL import Image\n",
    "from typing import Union, Dict, Any, List, Literal\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95e70e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Define a Function and Global Variable to Decide and Hold the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d3f2c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Global variable to store selected prompt\n",
    "SELECTED_PROMPT = None\n",
    "\n",
    "def load_prompt_files() -> Dict[str, Dict]:\n",
    "    \"\"\"Load all prompt YAML files from the config/prompts directory.\"\"\"\n",
    "    if not MODEL_CONFIG or \"prompt\" not in MODEL_CONFIG:\n",
    "        raise ValueError(\"Model configuration not loaded or missing prompt configuration\")\n",
    "    \n",
    "    prompts_dir = ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"prompts\"\n",
    "    prompt_files = {\n",
    "        \"basic_extraction\": prompts_dir / \"basic_extraction.yaml\",\n",
    "        \"detailed\": prompts_dir / \"detailed.yaml\",\n",
    "        \"few_shot\": prompts_dir / \"few_shot.yaml\",\n",
    "        \"locational\": prompts_dir / \"locational.yaml\",\n",
    "        \"step_by_step\": prompts_dir / \"step_by_step.yaml\"\n",
    "    }\n",
    "    \n",
    "    loaded_prompts = {}\n",
    "    for name, file_path in prompt_files.items():\n",
    "        with open(file_path, 'r') as f:\n",
    "            prompt_data = yaml.safe_load(f)\n",
    "            # Apply Llama Vision prompt format\n",
    "            for prompt in prompt_data['prompts']:\n",
    "                prompt['text'] = MODEL_CONFIG['prompt']['format'].format(\n",
    "                    prompt_text=prompt['text']\n",
    "                )\n",
    "            loaded_prompts[name] = prompt_data\n",
    "    return loaded_prompts\n",
    "\n",
    "def select_prompt() -> str:\n",
    "    \"\"\"Allow user to select a prompt type and return the prompt text.\"\"\"\n",
    "    global SELECTED_PROMPT\n",
    "    \n",
    "    prompts = load_prompt_files()\n",
    "    print(\"\\nAvailable prompt types:\")\n",
    "    for i, name in enumerate(prompts.keys(), 1):\n",
    "        print(f\"{i}. {name.replace('_', ' ').title()}\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nSelect a prompt type (1-5): \"))\n",
    "            if 1 <= choice <= len(prompts):\n",
    "                selected_name = list(prompts.keys())[choice - 1]\n",
    "                SELECTED_PROMPT = prompts[selected_name]\n",
    "                print(f\"\\nSelected prompt type: {selected_name.replace('_', ' ').title()}\")\n",
    "                print(\"\\nPrompt text:\")\n",
    "                print(\"-\" * 50)\n",
    "                print(SELECTED_PROMPT['prompts'][0]['text'])\n",
    "                print(\"-\" * 50)\n",
    "                return selected_name\n",
    "            else:\n",
    "                print(\"Invalid choice. Please select a number between 1 and 5.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06f5e8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fd7f7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Root Directory Determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c227766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine root directory by finding .gitignore and .gitattributes\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find project root by locating directory containing .gitignore and .gitattributes\"\"\"\n",
    "    try:\n",
    "        # When running as a script, start from script location\n",
    "        start_path = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # When running in a notebook, start from current working directory\n",
    "        start_path = Path.cwd()\n",
    "    \n",
    "    # Walk up the directory tree to find git markers\n",
    "    current_path = start_path\n",
    "    while current_path != current_path.parent:  # Stop at filesystem root\n",
    "        if (current_path / \".gitignore\").exists() and (current_path / \".gitattributes\").exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    \n",
    "    raise RuntimeError(\"Could not find project root (directory containing .gitignore and .gitattributes)\")\n",
    "\n",
    "ROOT_DIR = find_project_root()\n",
    "logger.info(f\"Found project root: {ROOT_DIR}\")\n",
    "\n",
    "# Verify expected files exist in the Deliverables-Code directory\n",
    "deliverables_dir = ROOT_DIR / \"Deliverables-Code\"\n",
    "if not deliverables_dir.exists():\n",
    "    raise RuntimeError(\"Could not find Deliverables-Code directory in project root\")\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Create results directory\n",
    "results_dir = ROOT_DIR / \"Deliverables-Code\" / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f7e31",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    \"\"\"Install required dependencies with progress tracking.\"\"\"\n",
    "    # First install tqdm if not already installed\n",
    "    try:\n",
    "        import tqdm\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tqdm\"])\n",
    "        import tqdm\n",
    "    \n",
    "    # Update pip first\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    \n",
    "    # Check if PyTorch is already installed with correct version\n",
    "    pytorch_step = None\n",
    "    try:\n",
    "        import torch\n",
    "        torch_version = torch.__version__\n",
    "        if torch_version.startswith(\"2.1.0\") and \"cu118\" in torch_version:\n",
    "            logger.info(f\"PyTorch {torch_version} already installed, skipping PyTorch installation\")\n",
    "        else:\n",
    "            pytorch_step = (\"PyTorch\", [\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"torch==2.1.0\",\n",
    "                \"torchvision==0.16.0\",\n",
    "                \"torchaudio==2.1.0\",\n",
    "                \"--index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
    "            ])\n",
    "    except ImportError:\n",
    "        pytorch_step = (\"PyTorch\", [\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "            \"torch==2.1.0\",\n",
    "            \"torchvision==0.16.0\",\n",
    "            \"torchaudio==2.1.0\",\n",
    "            \"--index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
    "        ])\n",
    "    \n",
    "    # Install base requirements\n",
    "    base_requirements = [\n",
    "        (\"Base requirements\", [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(ROOT_DIR / \"Deliverables-Code\" / \"requirements\" / \"requirements_llama.txt\")])\n",
    "    ]\n",
    "    \n",
    "    if pytorch_step:\n",
    "        base_requirements.append(pytorch_step)\n",
    "    \n",
    "    for step_name, command in tqdm.tqdm(base_requirements, desc=\"Installing dependencies\"):\n",
    "        try:\n",
    "            subprocess.check_call(command)\n",
    "            logger.info(f\"Successfully installed {step_name}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.error(f\"Error installing {step_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "# Install dependencies\n",
    "install_dependencies()\n",
    "\n",
    "# Clear CUDA cache to prevent conflicts\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0a16d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Flash Attention Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_flash_attention() -> bool:\n",
    "    \"\"\"\n",
    "    Check if Flash Attention is available and configure it.\n",
    "    Returns True if Flash Attention is enabled, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import flash_attn\n",
    "        \n",
    "        # Check if GPU supports Flash Attention\n",
    "        if not torch.cuda.is_available():\n",
    "            logger.warning(\"Flash Attention requires CUDA GPU. Disabling Flash Attention.\")\n",
    "            return False\n",
    "            \n",
    "        # Get GPU compute capability\n",
    "        major, minor = torch.cuda.get_device_capability()\n",
    "        compute_capability = float(f\"{major}.{minor}\")\n",
    "        \n",
    "        # Flash Attention 2 requires compute capability >= 8.0\n",
    "        if compute_capability >= 8.0:\n",
    "            logger.info(\"Flash Attention 2 enabled - GPU supports compute capability 8.0+\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"GPU compute capability {compute_capability} does not support Flash Attention 2\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        logger.warning(\"Flash Attention not installed. Please install with: pip install flash-attn\")\n",
    "        return False\n",
    "\n",
    "# Configure Flash Attention\n",
    "use_flash_attention = configure_flash_attention()\n",
    "logger.info(f\"Flash Attention Status: {'Enabled' if use_flash_attention else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99ff5b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model Configuration\n",
    "Load and validate the Llama Vision model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_config() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load and validate the Llama Vision model configuration from YAML file.\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the model configuration\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If config file doesn't exist\n",
    "        yaml.YAMLError: If config file is invalid\n",
    "        ValueError: If required configuration sections are missing\n",
    "    \"\"\"\n",
    "    config_path = ROOT_DIR / \"Deliverables-Code\" / \"config\" / \"llama_vision.yaml\"\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    except yaml.YAMLError as e:\n",
    "        raise yaml.YAMLError(f\"Error parsing configuration file: {e}\")\n",
    "    \n",
    "    # Validate required sections\n",
    "    required_sections = [\n",
    "        \"name\", \"repo_id\", \"model_type\", \"processor_type\",\n",
    "        \"hardware\", \"loading\", \"quantization\", \"prompt\",\n",
    "        \"image_preprocessing\", \"inference\"\n",
    "    ]\n",
    "    \n",
    "    missing_sections = [section for section in required_sections if section not in config]\n",
    "    if missing_sections:\n",
    "        raise ValueError(f\"Missing required configuration sections: {missing_sections}\")\n",
    "    \n",
    "    # Validate hardware requirements\n",
    "    if not config[\"hardware\"].get(\"gpu_required\"):\n",
    "        logger.warning(\"GPU is required for optimal performance\")\n",
    "    \n",
    "    # Log configuration summary\n",
    "    logger.info(f\"Loaded configuration for {config['name']}\")\n",
    "    logger.info(f\"Model repository: {config['repo_id']}\")\n",
    "    logger.info(f\"Hardware requirements: {config['hardware']}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load model configuration\n",
    "try:\n",
    "    MODEL_CONFIG = load_model_config()\n",
    "    logger.info(\"Model configuration loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f9ec0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model Settings\n",
    "Configure model settings including quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e7f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_config(selected_option: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get quantization configuration for the selected option.\n",
    "    If no option is selected, prompts the user to choose one.\n",
    "    \n",
    "    Args:\n",
    "        selected_option: Optional pre-selected quantization option\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the quantization configuration\n",
    "    \"\"\"\n",
    "    if not MODEL_CONFIG or \"quantization\" not in MODEL_CONFIG:\n",
    "        raise ValueError(\"Model configuration not loaded or missing quantization options\")\n",
    "    \n",
    "    quantization_options = MODEL_CONFIG[\"quantization\"][\"options\"]\n",
    "    default_option = MODEL_CONFIG[\"quantization\"][\"default\"]\n",
    "    \n",
    "    if not selected_option:\n",
    "        print(\"\\nAvailable quantization options:\")\n",
    "        for i, (option, config) in enumerate(quantization_options.items(), 1):\n",
    "            memory_req = \"16GB\" if option == \"bfloat16\" else \"8GB\" if option == \"int8\" else \"4GB\"\n",
    "            print(f\"{i}. {option.upper()} (Memory: {memory_req})\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                choice = int(input(f\"\\nSelect quantization (1-3) [default: {default_option}]: \") or \"1\")\n",
    "                if 1 <= choice <= len(quantization_options):\n",
    "                    selected_option = list(quantization_options.keys())[choice - 1]\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Invalid choice. Please select a number between 1 and {len(quantization_options)}.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number.\")\n",
    "    \n",
    "    if selected_option not in quantization_options:\n",
    "        raise ValueError(f\"Invalid quantization option: {selected_option}\")\n",
    "    \n",
    "    logger.info(f\"Using {selected_option} quantization\")\n",
    "    return quantization_options[selected_option]\n",
    "\n",
    "# Initialize quantization configuration\n",
    "QUANTIZATION_CONFIG = get_quantization_config()\n",
    "quantization = list(QUANTIZATION_CONFIG.keys())[0]  # Get the selected quantization type\n",
    "logger.info(\"Quantization configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff95b3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Authentication Configuration\n",
    "Configure Hugging Face authentication for accessing the gated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65735595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_token() -> str:\n",
    "    \"\"\"\n",
    "    Get Hugging Face authentication token from environment or user input.\n",
    "    \n",
    "    Returns:\n",
    "        str: Valid Hugging Face token\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If token is invalid or missing required permissions\n",
    "    \"\"\"\n",
    "    # Try to get token from environment\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    if not token:\n",
    "        logger.info(\"HF_TOKEN not found in environment variables\")\n",
    "        token = input(\"\\nPlease enter your Hugging Face token: \").strip()\n",
    "    \n",
    "    if not token:\n",
    "        raise ValueError(\"No Hugging Face token provided\")\n",
    "    \n",
    "    # Validate token format\n",
    "    if not token.startswith(\"hf_\"):\n",
    "        raise ValueError(\"Invalid token format. Token should start with 'hf_'\")\n",
    "    \n",
    "    # Test token with a simple API call\n",
    "    try:\n",
    "        import requests\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        response = requests.get(\n",
    "            \"https://huggingface.co/api/models/meta-llama/Llama-3.2-11B-Vision\",\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 401:\n",
    "            raise ValueError(\"Invalid token: Authentication failed\")\n",
    "        elif response.status_code == 403:\n",
    "            raise ValueError(\"Token does not have access to meta-llama/Llama-3.2-11B-Vision\")\n",
    "        elif response.status_code != 200:\n",
    "            raise ValueError(f\"Token validation failed with status code: {response.status_code}\")\n",
    "            \n",
    "        logger.info(\"Successfully validated Hugging Face token\")\n",
    "        return token\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise ValueError(f\"Failed to validate token: {str(e)}\")\n",
    "\n",
    "def configure_hf_auth():\n",
    "    \"\"\"Configure Hugging Face authentication for the session.\"\"\"\n",
    "    try:\n",
    "        # Get and validate token\n",
    "        token = get_hf_token()\n",
    "        \n",
    "        # Set token in environment for this session\n",
    "        os.environ[\"HF_TOKEN\"] = token\n",
    "        \n",
    "        # Configure huggingface_hub\n",
    "        from huggingface_hub import HfFolder\n",
    "        HfFolder.save_token(token)\n",
    "        \n",
    "        logger.info(\"Hugging Face authentication configured successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to configure Hugging Face authentication: {e}\")\n",
    "        raise\n",
    "\n",
    "# Configure Hugging Face authentication\n",
    "try:\n",
    "    configure_hf_auth()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Authentication configuration failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b333380",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model Loading Configuration\n",
    "Configure model loading parameters and device mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8160ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_device_mapping() -> dict:\n",
    "    \"\"\"\n",
    "    Configure device mapping based on available hardware and model requirements.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Device mapping configuration\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If hardware requirements are not met\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"No GPU available. This model requires a GPU to run.\")\n",
    "    \n",
    "    # Get GPU properties\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    compute_capability = float(f\"{gpu_props.major}.{gpu_props.minor}\")\n",
    "    \n",
    "    # Check compute capability\n",
    "    min_compute = float(MODEL_CONFIG[\"hardware\"][\"minimum_compute_capability\"])\n",
    "    if compute_capability < min_compute:\n",
    "        raise RuntimeError(\n",
    "            f\"GPU compute capability {compute_capability} is below minimum required {min_compute}\"\n",
    "        )\n",
    "    \n",
    "    # Get GPU memory in GB\n",
    "    gpu_memory = gpu_props.total_memory / (1024**3)\n",
    "    logger.info(f\"Available GPU memory: {gpu_memory:.2f}GB\")\n",
    "    \n",
    "    # Configure device mapping based on quantization\n",
    "    if QUANTIZATION_CONFIG.get(\"device_map\") == \"auto\":\n",
    "        # Use automatic device mapping\n",
    "        device_map = \"auto\"\n",
    "        logger.info(\"Using automatic device mapping\")\n",
    "    else:\n",
    "        # Use single GPU setup\n",
    "        device_map = {\"\": 0}\n",
    "        logger.info(\"Using single GPU setup\")\n",
    "    \n",
    "    return device_map\n",
    "\n",
    "def get_model_loading_params() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get model loading parameters based on configuration and selected quantization.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model loading parameters\n",
    "    \"\"\"\n",
    "    # Start with base parameters from config\n",
    "    params = {\n",
    "        \"trust_remote_code\": True,\n",
    "        \"use_auth_token\": True,  # Required for Llama\n",
    "        \"device_map\": configure_device_mapping()\n",
    "    }\n",
    "    \n",
    "    # Add quantization parameters\n",
    "    if \"torch_dtype\" in QUANTIZATION_CONFIG:\n",
    "        params[\"torch_dtype\"] = getattr(torch, QUANTIZATION_CONFIG[\"torch_dtype\"])\n",
    "    \n",
    "    # Add flash attention if configured\n",
    "    if QUANTIZATION_CONFIG.get(\"use_flash_attention_2\", False):\n",
    "        params[\"use_flash_attention_2\"] = True\n",
    "        params[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "    \n",
    "    # Add 4-bit quantization if selected\n",
    "    if QUANTIZATION_CONFIG.get(\"load_in_4bit\", False):\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        params[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "    \n",
    "    # Add 8-bit quantization if selected\n",
    "    if QUANTIZATION_CONFIG.get(\"load_in_8bit\", False):\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        params[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_quant_type=\"fp8\"\n",
    "        )\n",
    "    \n",
    "    logger.info(\"Model loading parameters configured successfully\")\n",
    "    return params\n",
    "\n",
    "# Initialize model loading parameters and device mapping\n",
    "try:\n",
    "    MODEL_LOADING_PARAMS = get_model_loading_params()\n",
    "    device_map = MODEL_LOADING_PARAMS[\"device_map\"]  # Make device_map available globally\n",
    "    logger.info(\"Model loading configuration completed\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to configure model loading parameters: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff9a88",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Prompt and Processing Configuration\n",
    "Configure prompt formatting, image processing, and inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_formatting_params() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get prompt formatting parameters from configuration.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prompt formatting parameters\n",
    "    \"\"\"\n",
    "    if not MODEL_CONFIG or \"prompt\" not in MODEL_CONFIG:\n",
    "        raise ValueError(\"Model configuration not loaded or missing prompt configuration\")\n",
    "    \n",
    "    prompt_config = MODEL_CONFIG[\"prompt\"]\n",
    "    \n",
    "    params = {\n",
    "        \"format\": prompt_config[\"format\"],\n",
    "        \"image_placeholder\": prompt_config[\"image_placeholder\"],\n",
    "        \"system_prompt\": prompt_config[\"system_prompt\"],\n",
    "        \"response_format\": prompt_config[\"response_format\"],\n",
    "        \"field_mapping\": prompt_config[\"field_mapping\"]\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Prompt formatting parameters configured successfully\")\n",
    "    return params\n",
    "\n",
    "def get_image_processing_params() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get image processing parameters from configuration.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Image processing parameters\n",
    "    \"\"\"\n",
    "    if not MODEL_CONFIG or \"image_preprocessing\" not in MODEL_CONFIG:\n",
    "        raise ValueError(\"Model configuration not loaded or missing image preprocessing configuration\")\n",
    "    \n",
    "    img_config = MODEL_CONFIG[\"image_preprocessing\"]\n",
    "    \n",
    "    params = {\n",
    "        \"max_size\": tuple(img_config[\"max_size\"]),\n",
    "        \"convert_to_rgb\": img_config[\"convert_to_rgb\"],\n",
    "        \"normalize\": img_config[\"normalize\"],\n",
    "        \"resize_strategy\": img_config[\"resize_strategy\"]\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Image processing parameters configured successfully\")\n",
    "    return params\n",
    "\n",
    "def get_inference_params() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get inference parameters from configuration.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Inference parameters\n",
    "    \"\"\"\n",
    "    if not MODEL_CONFIG or \"inference\" not in MODEL_CONFIG:\n",
    "        raise ValueError(\"Model configuration not loaded or missing inference configuration\")\n",
    "    \n",
    "    inference_config = MODEL_CONFIG[\"inference\"]\n",
    "    \n",
    "    params = {\n",
    "        \"max_new_tokens\": inference_config[\"max_new_tokens\"],\n",
    "        \"do_sample\": inference_config[\"do_sample\"],\n",
    "        \"temperature\": inference_config[\"temperature\"],\n",
    "        \"top_k\": inference_config[\"top_k\"],\n",
    "        \"top_p\": inference_config[\"top_p\"],\n",
    "        \"batch_size\": inference_config[\"batch_size\"],\n",
    "        \"max_batch_memory_gb\": inference_config[\"max_batch_memory_gb\"]\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Inference parameters configured successfully\")\n",
    "    return params\n",
    "\n",
    "def format_prompt(prompt_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Format the prompt using the Llama Vision template.\n",
    "    \n",
    "    Args:\n",
    "        prompt_text: The base prompt text\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted prompt with image token and begin/end of text tokens\n",
    "    \"\"\"\n",
    "    # Remove any existing [INST] or [IMG] tags\n",
    "    prompt_text = prompt_text.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").replace(\"[IMG]\", \"\").replace(\"[/IMG]\", \"\")\n",
    "    \n",
    "    # Add the Llama Vision tokens with clearer separation and explicit end token\n",
    "    return f\"<|image|>\\n<|begin_of_text|>\\n{prompt_text}\\n<|end_of_text|>\\n<|response|>\\n<|end_of_response|>\"\n",
    "\n",
    "def process_image(image: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Process the image according to Llama Vision specifications.\n",
    "    \n",
    "    Args:\n",
    "        image: Input PIL Image\n",
    "        \n",
    "    Returns:\n",
    "        Image.Image: Processed image\n",
    "    \"\"\"\n",
    "    img_params = get_image_processing_params()\n",
    "    \n",
    "    # Convert to RGB if needed\n",
    "    if img_params[\"convert_to_rgb\"] and image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    # Resize if needed\n",
    "    if image.size[0] > img_params[\"max_size\"][0] or image.size[1] > img_params[\"max_size\"][1]:\n",
    "        if img_params[\"resize_strategy\"] == \"maintain_aspect_ratio\":\n",
    "            image.thumbnail(img_params[\"max_size\"], Image.Resampling.LANCZOS)\n",
    "        else:\n",
    "            image = image.resize(img_params[\"max_size\"], Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Initialize processing parameters\n",
    "try:\n",
    "    PROMPT_PARAMS = get_prompt_formatting_params()\n",
    "    IMAGE_PARAMS = get_image_processing_params()\n",
    "    INFERENCE_PARAMS = get_inference_params()\n",
    "    logger.info(\"Processing parameters configured successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to configure processing parameters: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40bf844",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e93073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_llama_model(model_id: str = \"meta-llama/Llama-3.2-11B-Vision\", \n",
    "                        max_retries: int = 2,\n",
    "                        retry_delay: int = 5) -> tuple:\n",
    "    \"\"\"\n",
    "    Download the Llama Vision model with retry logic and memory monitoring.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID\n",
    "        max_retries: Maximum number of download attempts\n",
    "        retry_delay: Delay between retries in seconds\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, processor) if successful\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If download fails after max retries\n",
    "    \"\"\"\n",
    "    from transformers import MllamaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "    import time\n",
    "    import psutil\n",
    "    \n",
    "    def log_memory_usage(stage: str):\n",
    "        \"\"\"Log current memory usage\"\"\"\n",
    "        gpu_mem = torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0\n",
    "        ram = psutil.virtual_memory().used / (1024**3)\n",
    "        logger.info(f\"Memory usage at {stage}: GPU={gpu_mem:.2f}GB, RAM={ram:.2f}GB\")\n",
    "    \n",
    "    # Log initial memory usage\n",
    "    log_memory_usage(\"start\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Download attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            # Configure model loading based on selected quantization\n",
    "            model_kwargs = {\n",
    "                \"device_map\": device_map,\n",
    "                \"trust_remote_code\": True,\n",
    "                \"use_auth_token\": True  # Required for Llama\n",
    "            }\n",
    "            \n",
    "            if quantization == \"bfloat16\":\n",
    "                model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
    "            elif quantization == \"int8\":\n",
    "                # Simplified 8-bit config that works more reliably\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_threshold=6.0,\n",
    "                    llm_int8_enable_fp32_cpu_offload=False\n",
    "                )\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "            elif quantization == \"int4\":\n",
    "                # Simplified 4-bit config\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\n",
    "                    bnb_4bit_use_double_quant=False,\n",
    "                    bnb_4bit_quant_type=\"nf4\"\n",
    "                )\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "            \n",
    "            # Download model and processor\n",
    "            model = MllamaForConditionalGeneration.from_pretrained(model_id, **model_kwargs)\n",
    "            processor = AutoProcessor.from_pretrained(model_id)\n",
    "            \n",
    "            # Log final memory usage\n",
    "            log_memory_usage(\"complete\")\n",
    "            \n",
    "            logger.info(\"Model and processor downloaded successfully\")\n",
    "            return model, processor\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Download attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Failed to download model after {max_retries} attempts: {str(e)}\")\n",
    "\n",
    "# Download model and processor\n",
    "model, processor = download_llama_model()\n",
    "\n",
    "# Clear CUDA cache after model loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "logger.info(\"Model and processor ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35efa2c7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Prompt Selection\n",
    "Select a prompt type for the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1925ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prompt selection\n",
    "selected_prompt_type = select_prompt()\n",
    "logger.info(f\"Selected prompt type: {selected_prompt_type}\")\n",
    "\n",
    "# The selected prompt is now stored in the global variable SELECTED_PROMPT\n",
    "# This can be accessed in subsequent cells for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f034354",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Single Image Test\n",
    "Run the model on a single image using the selected prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc313a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_image_test():\n",
    "    \"\"\"Run the model on a single image with the selected prompt.\"\"\"\n",
    "    # Get the first .jpg file from data/images\n",
    "    image_dir = ROOT_DIR / \"Deliverables-Code\" / \"data\" / \"images\" / \"1_curated\"\n",
    "    image_files = list(image_dir.glob(\"*.jpg\"))\n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(\"No .jpg files found in data/images/1_curated directory\")\n",
    "    \n",
    "    image_path = str(image_files[0])\n",
    "    \n",
    "    # Load and process image\n",
    "    image = process_image(Image.open(image_path))\n",
    "    \n",
    "    # Create a display version of the image with a max size of 800x800\n",
    "    display_image = image.copy()\n",
    "    max_display_size = (800, 800)\n",
    "    display_image.thumbnail(max_display_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Format the prompt\n",
    "    prompt_text = SELECTED_PROMPT['prompts'][0]['text']\n",
    "    formatted_prompt = format_prompt(prompt_text)\n",
    "    \n",
    "    # Display the image\n",
    "    print(\"\\nInput Image (resized for display):\")\n",
    "    display(display_image)\n",
    "    \n",
    "    # Display the prompt\n",
    "    print(\"\\nFormatted Prompt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(formatted_prompt)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Prepare model inputs with dtype handling\n",
    "    inputs = processor(image, formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Convert pixel_values to match model dtype for quantized models\n",
    "    if 'pixel_values' in inputs:\n",
    "        # Get the actual dtype of the vision model's first layer\n",
    "        try:\n",
    "            vision_dtype = next(model.vision_model.parameters()).dtype\n",
    "            if vision_dtype != torch.float32:\n",
    "                inputs['pixel_values'] = inputs['pixel_values'].to(vision_dtype)\n",
    "        except:\n",
    "            # Fallback for quantized models\n",
    "            if quantization in [\"int8\", \"int4\"]:\n",
    "                inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generation_params = {\n",
    "            \"max_new_tokens\": INFERENCE_PARAMS[\"max_new_tokens\"],\n",
    "            \"do_sample\": INFERENCE_PARAMS[\"do_sample\"],\n",
    "            \"temperature\": INFERENCE_PARAMS[\"temperature\"],\n",
    "            \"top_k\": INFERENCE_PARAMS[\"top_k\"],\n",
    "            \"top_p\": INFERENCE_PARAMS[\"top_p\"]\n",
    "        }\n",
    "        \n",
    "        # Generate response\n",
    "        generated_ids = model.generate(**inputs, **generation_params)\n",
    "    \n",
    "    # Decode and display response\n",
    "    response = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\nModel Response:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Run the single image test\n",
    "try:\n",
    "    run_single_image_test()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during single image test: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450606c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Batch Test\n",
    "Run the model on all images and save raw results only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ada2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_filename(model_name: str, quantization_level: str, results_dir: Path) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Generate a results filename with auto-incrementing counter.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model (e.g., \"pixtral\", \"llama\", \"doctr\")\n",
    "        quantization_level: Quantization level (e.g., \"bfloat16\", \"int8\", \"int4\", \"none\")\n",
    "        results_dir: Directory where results are stored\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (filename_without_extension, full_filepath)\n",
    "    \"\"\"\n",
    "    # Find existing files with the same model and quantization pattern\n",
    "    pattern = f\"results-{model_name}-{quantization_level}-*.json\"\n",
    "    existing_files = list(results_dir.glob(pattern))\n",
    "    \n",
    "    # Extract counter numbers from existing files\n",
    "    counter_numbers = []\n",
    "    for file in existing_files:\n",
    "        try:\n",
    "            # Extract number from filename like \"results-llama-bfloat16-3.json\"\n",
    "            parts = file.stem.split('-')\n",
    "            if len(parts) >= 4:\n",
    "                counter_numbers.append(int(parts[-1]))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Get next counter number\n",
    "    next_counter = max(counter_numbers, default=0) + 1\n",
    "    \n",
    "    # Generate filename\n",
    "    filename_base = f\"results-{model_name}-{quantization_level}-{next_counter}\"\n",
    "    full_filepath = results_dir / f\"{filename_base}.json\"\n",
    "    \n",
    "    return filename_base, str(full_filepath)\n",
    "\n",
    "def collect_test_metadata(test_id: str) -> dict:\n",
    "    \"\"\"Collect metadata about the current test configuration.\"\"\"\n",
    "    # Get GPU information\n",
    "    gpu_props = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None\n",
    "    \n",
    "    return {\n",
    "        \"test_id\": test_id,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_info\": {\n",
    "            \"name\": MODEL_CONFIG[\"name\"],\n",
    "            \"version\": \"1.0\",\n",
    "            \"model_id\": MODEL_CONFIG[\"repo_id\"],\n",
    "            \"model_type\": \"vision_language_model\",\n",
    "            \"quantization\": {\n",
    "                \"type\": quantization,\n",
    "                \"config\": QUANTIZATION_CONFIG\n",
    "            },\n",
    "            \"device_info\": {\n",
    "                \"device_map\": device_map,\n",
    "                \"use_flash_attention\": use_flash_attention,\n",
    "                \"gpu_memory_gb\": round(gpu_props.total_memory / (1024**3), 2) if gpu_props else None,\n",
    "                \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\" if gpu_props else None\n",
    "            }\n",
    "        },\n",
    "        \"prompt_info\": {\n",
    "            \"prompt_type\": selected_prompt_type,\n",
    "            \"raw_text\": SELECTED_PROMPT['prompts'][0]['text'],\n",
    "            \"formatted_text\": format_prompt(SELECTED_PROMPT['prompts'][0]['text']),\n",
    "            \"special_tokens\": [\"<|image|>\", \"<|begin_of_text|>\", \"<|end_of_text|>\", \"<|response|>\", \"<|end_of_response|>\"]\n",
    "        },\n",
    "        \"processing_config\": {\n",
    "            \"inference_params\": INFERENCE_PARAMS,\n",
    "            \"image_preprocessing\": IMAGE_PARAMS\n",
    "        }\n",
    "    }\n",
    "\n",
    "def save_incremental_results(results_file: Path, results: list, metadata: dict):\n",
    "    \"\"\"Save results incrementally to avoid losing progress.\"\"\"\n",
    "    complete_results = {\n",
    "        \"metadata\": metadata,\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(complete_results, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Saved incremental results to {results_file}\")\n",
    "\n",
    "def process_batch(test_id: str) -> list:\n",
    "    \"\"\"Process all images in the data/images directory and collect raw responses only.\"\"\"\n",
    "    results = []\n",
    "    image_dir = ROOT_DIR / \"Deliverables-Code\" / \"data\" / \"images\" / \"1_curated\"\n",
    "    image_files = list(image_dir.glob(\"*.jpg\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(\"No .jpg files found in data/images/1_curated directory\")\n",
    "    \n",
    "    # Collect metadata\n",
    "    metadata = collect_test_metadata(test_id)\n",
    "    \n",
    "    results_file = results_dir / f\"{test_id}.json\"\n",
    "    \n",
    "    logger.info(f\"Starting batch processing of {len(image_files)} images\")\n",
    "    logger.info(f\"Raw results will be saved to: {results_file}\")\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        try:\n",
    "            # Load and process image\n",
    "            image = process_image(Image.open(str(image_path)))\n",
    "            \n",
    "            # Format the prompt\n",
    "            prompt_text = SELECTED_PROMPT['prompts'][0]['text']\n",
    "            formatted_prompt = format_prompt(prompt_text)\n",
    "            \n",
    "            # Prepare model inputs with dtype handling\n",
    "            inputs = processor(image, formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            # Convert pixel_values to match model dtype for quantized models\n",
    "            if 'pixel_values' in inputs:\n",
    "                # Get the actual dtype of the vision model's first layer\n",
    "                try:\n",
    "                    vision_dtype = next(model.vision_model.parameters()).dtype\n",
    "                    if vision_dtype != torch.float32:\n",
    "                        inputs['pixel_values'] = inputs['pixel_values'].to(vision_dtype)\n",
    "                except:\n",
    "                    # Fallback for quantized models\n",
    "                    if quantization in [\"int8\", \"int4\"]:\n",
    "                        inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16)\n",
    "            \n",
    "            # Time the inference\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                generation_params = {\n",
    "                    \"max_new_tokens\": INFERENCE_PARAMS[\"max_new_tokens\"],\n",
    "                    \"do_sample\": INFERENCE_PARAMS[\"do_sample\"],\n",
    "                    \"temperature\": INFERENCE_PARAMS[\"temperature\"],\n",
    "                    \"top_k\": INFERENCE_PARAMS[\"top_k\"],\n",
    "                    \"top_p\": INFERENCE_PARAMS[\"top_p\"]\n",
    "                }\n",
    "                \n",
    "                generated_ids = model.generate(**inputs, **generation_params)\n",
    "            \n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # Decode response\n",
    "            response = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Create result entry with raw output only\n",
    "            result = {\n",
    "                \"image_name\": image_path.name,\n",
    "                \"status\": \"completed\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": round(processing_time, 2),\n",
    "                \"raw_output\": {\n",
    "                    \"model_response\": response,\n",
    "                    \"model_tokens_used\": len(generated_ids[0]),\n",
    "                    \"generation_parameters_used\": generation_params\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add to results\n",
    "            results.append(result)\n",
    "            \n",
    "            # Save incremental results\n",
    "            save_incremental_results(results_file, results, metadata)\n",
    "            \n",
    "            logger.info(f\"Processed image: {image_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing image {image_path.name}: {str(e)}\")\n",
    "            result = {\n",
    "                \"image_name\": image_path.name,\n",
    "                \"status\": \"error\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": {\n",
    "                    \"type\": \"processing_error\",\n",
    "                    \"message\": str(e),\n",
    "                    \"stage\": \"inference\"\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "            # Save incremental results even on error\n",
    "            save_incremental_results(results_file, results, metadata)\n",
    "    \n",
    "    logger.info(f\"Batch processing completed. Processed {len(results)} images\")\n",
    "    return results\n",
    "\n",
    "def run_batch_test():\n",
    "    \"\"\"Run the model on all images and save raw results only.\"\"\"\n",
    "    try:\n",
    "        # Generate filename with new naming convention\n",
    "        test_id, results_file_path = generate_results_filename(\"llama\", quantization, results_dir)\n",
    "        results_file = Path(results_file_path)\n",
    "        \n",
    "        logger.info(f\"Starting Llama batch test with {quantization} quantization\")\n",
    "        logger.info(f\"Results will be saved to: {results_file}\")\n",
    "        \n",
    "        # Process all images with incremental saving\n",
    "        results = process_batch(test_id)\n",
    "        \n",
    "        logger.info(f\"Batch test completed. Raw results saved to: {results_file}\")\n",
    "        return str(results_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during batch test: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run the batch test\n",
    "try:\n",
    "    results_file = run_batch_test()\n",
    "    logger.info(f\"Batch test completed successfully. Results saved to: {results_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Batch test failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d768b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Analysis Functions - Data Processing Phase\n",
    "Functions for analyzing raw model outputs and generating structured analysis reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec642940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_response(raw_response: str) -> dict:\n",
    "    \"\"\"Extract JSON data from the raw response string.\"\"\"\n",
    "    try:\n",
    "        # Find all JSON objects in the response\n",
    "        json_objects = []\n",
    "        start_idx = 0\n",
    "        while True:\n",
    "            start_idx = raw_response.find('{', start_idx)\n",
    "            if start_idx == -1:\n",
    "                break\n",
    "            end_idx = raw_response.find('}', start_idx) + 1\n",
    "            if end_idx == 0:\n",
    "                break\n",
    "            json_str = raw_response[start_idx:end_idx]\n",
    "            try:\n",
    "                json_obj = json.loads(json_str)\n",
    "                json_objects.append(json_obj)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        # The second JSON object should be the actual results\n",
    "        # (first one is the prompt template)\n",
    "        if len(json_objects) >= 2:\n",
    "            return json_objects[1]\n",
    "        elif len(json_objects) == 1:\n",
    "            # If only one JSON object found, use it\n",
    "            return json_objects[0]\n",
    "        else:\n",
    "            logger.warning(\"No valid JSON objects found in response\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to parse JSON from response: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_total_cost(cost_str: str) -> float:\n",
    "    \"\"\"Convert a cost string to a float by removing currency symbols and commas.\"\"\"\n",
    "    if not cost_str:\n",
    "        return None\n",
    "    # If already a float, return as is\n",
    "    if isinstance(cost_str, (int, float)):\n",
    "        return float(cost_str)\n",
    "    # Remove $ and commas, then convert to float\n",
    "    try:\n",
    "        return float(cost_str.replace('$', '').replace(',', '').strip())\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def categorize_work_order_error(predicted: str, ground_truth: str) -> str:\n",
    "    \"\"\"Categorize the type of error in work order number prediction.\"\"\"\n",
    "    if not predicted or not ground_truth:\n",
    "        return \"No Extraction\"\n",
    "    if predicted == ground_truth:\n",
    "        return \"Exact Match\"\n",
    "    # Check if prediction looks like a date (contains - or /)\n",
    "    if '-' in predicted or '/' in predicted:\n",
    "        return \"Date Confusion\"\n",
    "    # Check for partial match (some digits match)\n",
    "    if any(digit in ground_truth for digit in predicted):\n",
    "        return \"Partial Match\"\n",
    "    return \"Completely Wrong\"\n",
    "\n",
    "def categorize_total_cost_error(predicted: float, ground_truth: float) -> str:\n",
    "    \"\"\"Categorize the type of error in total cost prediction.\"\"\"\n",
    "    if predicted is None or ground_truth is None:\n",
    "        return \"No Extraction\"\n",
    "    if predicted == ground_truth:\n",
    "        return \"Numeric Match\"\n",
    "    \n",
    "    # Convert to strings for digit comparison\n",
    "    pred_str = str(int(predicted))\n",
    "    truth_str = str(int(ground_truth))\n",
    "    \n",
    "    # Check for digit reversal\n",
    "    if pred_str[::-1] == truth_str:\n",
    "        return \"Digit Reversal\"\n",
    "    \n",
    "    # Check for missing digit\n",
    "    if len(pred_str) == len(truth_str) - 1 and all(d in truth_str for d in pred_str):\n",
    "        return \"Missing Digit\"\n",
    "    \n",
    "    # Check for extra digit\n",
    "    if len(pred_str) == len(truth_str) + 1 and all(d in pred_str for d in truth_str):\n",
    "        return \"Extra Digit\"\n",
    "    \n",
    "    return \"Completely Wrong\"\n",
    "\n",
    "def calculate_cer(str1: str, str2: str) -> float:\n",
    "    \"\"\"Calculate Character Error Rate between two strings.\"\"\"\n",
    "    if not str1 or not str2:\n",
    "        return 1.0  # Return maximum error if either string is empty\n",
    "    \n",
    "    # Convert to strings and remove whitespace\n",
    "    str1 = str(str1).strip()\n",
    "    str2 = str(str2).strip()\n",
    "    \n",
    "    # Calculate Levenshtein distance\n",
    "    if len(str1) < len(str2):\n",
    "        str1, str2 = str2, str1\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    previous_row = range(len(str2) + 1)\n",
    "    for i, c1 in enumerate(str1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(str2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    # Return CER as distance divided by length of longer string\n",
    "    return previous_row[-1] / len(str1)\n",
    "\n",
    "def analyze_raw_results(results_file: str, ground_truth_file: str = None) -> dict:\n",
    "    \"\"\"Analyze raw model results and generate analysis report.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Set default ground truth file path\n",
    "    if ground_truth_file is None:\n",
    "        ground_truth_file = str(ROOT_DIR / \"Deliverables-Code\" / \"data\" / \"images\" / \"metadata\" / \"ground_truth.csv\")\n",
    "    \n",
    "    # Load results and ground truth\n",
    "    with open(results_file, 'r') as f:\n",
    "        raw_results = json.load(f)\n",
    "    \n",
    "    # Read ground truth with explicit string type for filename column\n",
    "    ground_truth = pd.read_csv(ground_truth_file, dtype={'filename': str})\n",
    "    \n",
    "    # Initialize analysis structure\n",
    "    analysis = {\n",
    "        \"source_results\": results_file,\n",
    "        \"extraction_method\": \"json_parsing_v2\",\n",
    "        \"ground_truth_file\": ground_truth_file,\n",
    "        \"metadata\": raw_results[\"metadata\"],\n",
    "        \"summary\": {\n",
    "            \"total_images\": len(raw_results[\"results\"]),\n",
    "            \"completed\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"json_extraction_successful\": 0,\n",
    "            \"work_order_accuracy\": 0,\n",
    "            \"total_cost_accuracy\": 0,\n",
    "            \"average_cer\": 0\n",
    "        },\n",
    "        \"error_categories\": {\n",
    "            \"work_order\": {},\n",
    "            \"total_cost\": {}\n",
    "        },\n",
    "        \"extracted_data\": [],\n",
    "        \"performance_metrics\": {}\n",
    "    }\n",
    "    \n",
    "    # Process each result\n",
    "    total_cer = 0\n",
    "    work_order_matches = 0\n",
    "    total_cost_matches = 0\n",
    "    json_successful = 0\n",
    "    \n",
    "    for result in raw_results[\"results\"]:\n",
    "        # Get ground truth for this image - use filename directly for matching\n",
    "        image_filename = result[\"image_name\"]\n",
    "        \n",
    "        gt_row = ground_truth[ground_truth[\"filename\"] == image_filename]\n",
    "        \n",
    "        if gt_row.empty:\n",
    "            logger.warning(f\"No ground truth found for image {image_filename}\")\n",
    "            continue\n",
    "            \n",
    "        gt_work_order = str(gt_row[\"work_order_number\"].iloc[0]).strip()\n",
    "        gt_total_cost = normalize_total_cost(str(gt_row[\"total\"].iloc[0]))\n",
    "        \n",
    "        # Initialize extraction entry\n",
    "        extraction_entry = {\n",
    "            \"image_name\": result[\"image_name\"],\n",
    "            \"status\": result[\"status\"],\n",
    "            \"raw_response\": result.get(\"raw_output\", {}).get(\"model_response\", \"\"),\n",
    "            \"ground_truth\": {\n",
    "                \"work_order_number\": gt_work_order,\n",
    "                \"total_cost\": gt_total_cost\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if result[\"status\"] == \"completed\":\n",
    "            analysis[\"summary\"][\"completed\"] += 1\n",
    "            \n",
    "            # Extract JSON data from raw response\n",
    "            raw_response = result[\"raw_output\"][\"model_response\"]\n",
    "            extracted_data = extract_json_from_response(raw_response)\n",
    "            \n",
    "            if extracted_data:\n",
    "                json_successful += 1\n",
    "                \n",
    "                # Analyze work order\n",
    "                pred_work_order = extracted_data.get(\"work_order_number\", \"\")\n",
    "                work_order_error = categorize_work_order_error(pred_work_order, gt_work_order)\n",
    "                work_order_cer = calculate_cer(pred_work_order, gt_work_order)\n",
    "                \n",
    "                if work_order_error == \"Exact Match\":\n",
    "                    work_order_matches += 1\n",
    "                \n",
    "                # Analyze total cost\n",
    "                pred_total_cost = normalize_total_cost(extracted_data.get(\"total_cost\", \"\"))\n",
    "                total_cost_error = categorize_total_cost_error(pred_total_cost, gt_total_cost)\n",
    "                \n",
    "                if total_cost_error == \"Numeric Match\":\n",
    "                    total_cost_matches += 1\n",
    "                \n",
    "                # Update extraction entry\n",
    "                extraction_entry.update({\n",
    "                    \"extracted_data\": {\n",
    "                        \"work_order_number\": pred_work_order,\n",
    "                        \"total_cost\": pred_total_cost\n",
    "                    },\n",
    "                    \"extraction_confidence\": {\n",
    "                        \"json_extraction_successful\": True,\n",
    "                        \"parsing_method\": \"json_extraction\",\n",
    "                        \"work_order_found\": bool(pred_work_order),\n",
    "                        \"total_cost_found\": bool(pred_total_cost),\n",
    "                        \"overall_confidence\": 1.0 - work_order_cer\n",
    "                    },\n",
    "                    \"performance\": {\n",
    "                        \"work_order_error_category\": work_order_error,\n",
    "                        \"total_cost_error_category\": total_cost_error,\n",
    "                        \"work_order_cer\": work_order_cer,\n",
    "                        \"work_order_correct\": work_order_error == \"Exact Match\",\n",
    "                        \"total_cost_correct\": total_cost_error == \"Numeric Match\"\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                # Update error categories\n",
    "                analysis[\"error_categories\"][\"work_order\"][work_order_error] = \\\n",
    "                    analysis[\"error_categories\"][\"work_order\"].get(work_order_error, 0) + 1\n",
    "                analysis[\"error_categories\"][\"total_cost\"][total_cost_error] = \\\n",
    "                    analysis[\"error_categories\"][\"total_cost\"].get(total_cost_error, 0) + 1\n",
    "                \n",
    "                total_cer += work_order_cer\n",
    "            else:\n",
    "                extraction_entry.update({\n",
    "                    \"extraction_error\": \"JSON extraction failed\",\n",
    "                    \"extraction_confidence\": {\n",
    "                        \"json_extraction_successful\": False,\n",
    "                        \"parsing_method\": \"json_extraction\",\n",
    "                        \"work_order_found\": False,\n",
    "                        \"total_cost_found\": False,\n",
    "                        \"overall_confidence\": 0.0\n",
    "                    }\n",
    "                })\n",
    "        else:\n",
    "            analysis[\"summary\"][\"errors\"] += 1\n",
    "            extraction_entry[\"processing_error\"] = result.get(\"error\", {})\n",
    "        \n",
    "        analysis[\"extracted_data\"].append(extraction_entry)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    completed = analysis[\"summary\"][\"completed\"]\n",
    "    if completed > 0:\n",
    "        analysis[\"summary\"][\"json_extraction_successful\"] = json_successful\n",
    "        analysis[\"summary\"][\"work_order_accuracy\"] = work_order_matches / completed\n",
    "        analysis[\"summary\"][\"total_cost_accuracy\"] = total_cost_matches / completed\n",
    "        analysis[\"summary\"][\"average_cer\"] = total_cer / completed\n",
    "        \n",
    "        # Performance metrics\n",
    "        analysis[\"performance_metrics\"] = {\n",
    "            \"json_extraction_rate\": json_successful / completed,\n",
    "            \"work_order_extraction_rate\": work_order_matches / completed,\n",
    "            \"total_cost_extraction_rate\": total_cost_matches / completed,\n",
    "            \"average_processing_time\": sum(\n",
    "                r.get(\"processing_time_seconds\", 0) \n",
    "                for r in raw_results[\"results\"] \n",
    "                if r[\"status\"] == \"completed\"\n",
    "            ) / completed\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def select_test_results_file() -> Path:\n",
    "    \"\"\"Allow user to select a test results file for analysis.\"\"\"\n",
    "    # Get all test result files\n",
    "    results_dir_path = ROOT_DIR / \"Deliverables-Code\" / \"results\"\n",
    "    result_files = list(results_dir_path.glob(\"results-*.json\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        raise FileNotFoundError(\"No test result files found in results directory\")\n",
    "    \n",
    "    # Sort files by modification time (newest first)\n",
    "    result_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(\"\\nAvailable test result files:\")\n",
    "    for i, file in enumerate(result_files, 1):\n",
    "        # Get file modification time\n",
    "        mod_time = datetime.fromtimestamp(file.stat().st_mtime)\n",
    "        print(f\"{i}. {file.name} (Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nSelect a test result file (1-{}): \".format(len(result_files))))\n",
    "            if 1 <= choice <= len(result_files):\n",
    "                selected_file = result_files[choice - 1]\n",
    "                print(f\"\\nSelected file: {selected_file.name}\")\n",
    "                return selected_file\n",
    "            else:\n",
    "                print(f\"Invalid choice. Please select a number between 1 and {len(result_files)}.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "def run_analysis():\n",
    "    \"\"\"Run analysis on raw results and generate comprehensive performance report.\"\"\"\n",
    "    try:\n",
    "        # Get test results file\n",
    "        results_file = select_test_results_file()\n",
    "        \n",
    "        # Generate analysis\n",
    "        analysis = analyze_raw_results(str(results_file))\n",
    "        \n",
    "        # Create analysis directory if it doesn't exist\n",
    "        analysis_dir = ROOT_DIR / \"Deliverables-Code\" / \"analysis\"\n",
    "        analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Generate analysis filename with same convention as results\n",
    "        model_name = \"llama\"\n",
    "        quantization_level = analysis['metadata']['model_info']['quantization']['type']\n",
    "        \n",
    "        # Find existing analysis files with the same model and quantization pattern\n",
    "        pattern = f\"analysis-{model_name}-{quantization_level}-*.json\"\n",
    "        existing_files = list(analysis_dir.glob(pattern))\n",
    "        \n",
    "        # Extract counter numbers from existing files\n",
    "        counter_numbers = []\n",
    "        for file in existing_files:\n",
    "            try:\n",
    "                # Extract number from filename like \"analysis-llama-bfloat16-3.json\"\n",
    "                parts = file.stem.split('-')\n",
    "                if len(parts) >= 4:\n",
    "                    counter_numbers.append(int(parts[-1]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Get next counter number\n",
    "        next_counter = max(counter_numbers, default=0) + 1\n",
    "        \n",
    "        # Generate analysis filename\n",
    "        analysis_filename = f\"analysis-{model_name}-{quantization_level}-{next_counter}.json\"\n",
    "        analysis_file = analysis_dir / analysis_filename\n",
    "        \n",
    "        with open(analysis_file, 'w') as f:\n",
    "            json.dump(analysis, f, indent=2)\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\nLlama Vision Model Analysis Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total Images: {analysis['summary']['total_images']}\")\n",
    "        print(f\"Completed: {analysis['summary']['completed']}\")\n",
    "        print(f\"Errors: {analysis['summary']['errors']}\")\n",
    "        print(f\"JSON Extraction Successful: {analysis['summary']['json_extraction_successful']}\")\n",
    "        print(f\"Work Order Accuracy: {analysis['summary']['work_order_accuracy']:.2%}\")\n",
    "        print(f\"Total Cost Accuracy: {analysis['summary']['total_cost_accuracy']:.2%}\")\n",
    "        print(f\"Average CER: {analysis['summary']['average_cer']:.3f}\")\n",
    "        \n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        for metric, value in analysis['performance_metrics'].items():\n",
    "            if 'rate' in metric:\n",
    "                print(f\"- {metric.replace('_', ' ').title()}: {value:.2%}\")\n",
    "            else:\n",
    "                print(f\"- {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "        \n",
    "        print(\"\\nWork Order Error Categories:\")\n",
    "        for category, count in analysis['error_categories']['work_order'].items():\n",
    "            print(f\"- {category}: {count}\")\n",
    "        \n",
    "        print(\"\\nTotal Cost Error Categories:\")\n",
    "        for category, count in analysis['error_categories']['total_cost'].items():\n",
    "            print(f\"- {category}: {count}\")\n",
    "        \n",
    "        print(f\"\\nDetailed analysis saved to: {analysis_file}\")\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run the analysis\n",
    "analysis_results = run_analysis()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
